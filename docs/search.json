[{"path":"index.html","id":"regressions-and-causal-inference","chapter":"“Regressions and Causal Inference”","heading":"“Regressions and Causal Inference”","text":" Welcome site course PS9591: “Regressions Causal Inference” Western University, taught Sebastián Vallejo Vera. week, find lecture slides, lecture code, lab exercises, lab code corresponding topic.class divided lectures tutorials. go lectures tutorials simultaneously. Thus, arranged website way shows suggested order lecture tutorials carried .start, don’t forget read Syllabus check Perusall readings course. site corrected/updated throughout semester(s).","code":""},{"path":"index.html","id":"about-tutorials","chapter":"“Regressions and Causal Inference”","heading":"0.1 About Tutorials","text":"tutorials interactive R documents can also run computer. allows practice concepts experiment different approaches pace. ’s get started.","code":""},{"path":"index.html","id":"prerequisites","chapter":"“Regressions and Causal Inference”","heading":"0.1.1 Prerequisites","text":"running tutorials, make sure :R installed computer (version 4.0.0 higher)RStudio installed (recent version)following R packages installed. can install running commands R:","code":"\n# Install required packages\ninstall.packages(\"learnr\")\ninstall.packages(\"wooldridge\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidylog\")\ninstall.packages(sjPlot) # to plot some models\ninstall.packages(readstata13) # to load .dta files"},{"path":"index.html","id":"running-a-tutorial","chapter":"“Regressions and Causal Inference”","heading":"0.1.2 Running a Tutorial","text":"two ways run tutorial locally:","code":""},{"path":"index.html","id":"method-1-using-rstudio","chapter":"“Regressions and Causal Inference”","heading":"0.1.2.1 Method 1: Using RStudio","text":"Download tutorial file (.Rmd extension)Open RStudioClick “Run Document” button top editorThe tutorial open new window","code":""},{"path":"index.html","id":"method-2-using-r-console","chapter":"“Regressions and Causal Inference”","heading":"0.1.2.2 Method 2: Using R Console","text":"tutorial file working directory, can run:Replace “filename” name tutorial file (without .Rmd extension).","code":"\nrmarkdown::run_tutorial(\"filename\", package = \"learnr\")"},{"path":"index.html","id":"tips-for-success","chapter":"“Regressions and Causal Inference”","heading":"0.1.3 Tips for Success","text":"working tutorials locally:Make sure required packages loaded tutorial’s setup chunkIf modify tutorial code, save file runningTo clear tutorial cache start fresh, just click “Start ” button bottom left corner.","code":""},{"path":"index.html","id":"troubleshooting-common-issues","chapter":"“Regressions and Causal Inference”","heading":"0.1.4 Troubleshooting Common Issues","text":"encounter problems:Tutorial won’t knit: Check required packages installedExercise chunks don’t run: Verify learnr properly loadedPrevious answers persist: Clear cache using code provided abovePackage found: Run install.packages() missing package","code":""},{"path":"index.html","id":"getting-help","chapter":"“Regressions and Causal Inference”","heading":"0.1.5 Getting Help","text":"need assistance:Check tutorial error messages specific package requirementsReview setup chunk missing dependenciesConsult learnr documentationAsk questions office hours send e-mailAsk ChatGPT (?)","code":""},{"path":"index.html","id":"next-steps","chapter":"“Regressions and Causal Inference”","heading":"0.1.6 Next Steps","text":"getting tutorials running locally, can:Experiment modifying codeCreate practice exercisesTry different approaches analysis tasksSave work future referenceRemember tutorials learning tools. Feel free experiment try different approaches – ’s learn best!","code":""},{"path":"index.html","id":"assignments","chapter":"“Regressions and Causal Inference”","heading":"0.2 Assignments","text":"list assignments course. assignments must handed pdf documents using R Markdown.","code":""},{"path":"index.html","id":"final-exam","chapter":"“Regressions and Causal Inference”","heading":"0.3 Final Exam","text":"final exam require students replicate findings papers, interpret results. Final Exam, post required datasets analyses replication exercises : https://github.com/svallejovera/regression_ci/tree/main/Sample_data.","code":""},{"path":"index.html","id":"acknowledgments","chapter":"“Regressions and Causal Inference”","heading":"0.4 Acknowledgments","text":"organization course based great textbook ‘Effect: Introduction Research Design Causality’ Nick Huntington-Klein, freely available . code used throughout main lectures patchwork code, code borrows heavily internet (’s true code). try best give credit original authors code (possible). code labs created revised two amazing doctoral students1 Western University, Hugo Machado John Santos (posted permission).","code":""},{"path":"lecture-1-what-is-causal-inference.html","id":"lecture-1-what-is-causal-inference","chapter":"1 Lecture 1: What is Causal Inference?","heading":"1 Lecture 1: What is Causal Inference?","text":"","code":""},{"path":"lecture-1-what-is-causal-inference.html","id":"slides","chapter":"1 Lecture 1: What is Causal Inference?","heading":"Slides","text":"2 Causal Inference? (link)","code":""},{"path":"lecture-1-what-is-causal-inference.html","id":"introduction","chapter":"1 Lecture 1: What is Causal Inference?","heading":"1.1 Introduction","text":"first week, introduce ‘causal inference’ concept interest, main problems determining causality observational data. fancy code week.lecture slide displayed full :\nFigure 1.1: Slides 2 Causal Inference?.\n","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","text":"tutorial created Hugo Machado John Santos (minor adaptations ).tutorial guide setting R RStudio, installing essential packages, learning basics RMarkdown, suggesting best practices help keep work organized find solutions common problems (definitely encounter).","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"installing-r-and-rstudio","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.1 Installing R and RStudio","text":"R programming language ’ll using course. optimized statistics data analysis. Additionally, open source, easily customizable, popular academia, gives edge proprietary (e.g., Stata, SPSS) general purpose frameworks (e.g., Python) best language learn social scientists working data.Go R Project website: https://mirror.csclub.uwaterloo.ca/CRAN/Go R Project website: https://mirror.csclub.uwaterloo.ca/CRAN/link send CRAN mirror hosted University Waterloo.link send CRAN mirror hosted University Waterloo.Select appropriate link operating system (Windows, macOS, Linux).Select appropriate link operating system (Windows, macOS, Linux).Follow instructions download install R.\nmacOS users:\nMac Apple Silicon chip (M1, M2, M3, etc.), choose arm64 version.\nMac Intel chip, choose x86_64 version.\n\nNote: updating R, ’ll need reinstall packages. ’s good idea time right deadline.\nFollow instructions download install R.macOS users:\nMac Apple Silicon chip (M1, M2, M3, etc.), choose arm64 version.\nMac Intel chip, choose x86_64 version.\nmacOS users:Mac Apple Silicon chip (M1, M2, M3, etc.), choose arm64 version.Mac Intel chip, choose x86_64 version.Note: updating R, ’ll need reinstall packages. ’s good idea time right deadline.Note: updating R, ’ll need reinstall packages. ’s good idea time right deadline.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"rstudio","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.1.1 RStudio","text":"RStudio Integrated Development Environment (IDE) makes working R much easier user-friendly.Go RStudio Desktop download page: https://posit.co/download/rstudio-desktop/Download free version RStudio Desktop operating system.Follow instructions install RStudio.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"a-quick-tour-of-the-rstudio-interface","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.2 A Quick Tour of the RStudio Interface","text":"open RStudio, ’ll see four main panes:Source (top-left): write edit R code RMarkdown documents.Console (bottom-left): can run R code interactively see output.Environment/History (top-right): Environment tab shows objects (data, variables, functions) created. History tab shows commands run.Files/Plots/Packages/Help (bottom-right): pane several tabs:\nFiles: Allows browse manage files computer.\nPlots: Displays plots create.\nPackages: Shows installed R packages allows load/unload .\nHelp: Displays R documentation.\nFiles: Allows browse manage files computer.Plots: Displays plots create.Packages: Shows installed R packages allows load/unload .Help: Displays R documentation.can customize layout panes Tools > Global Options > Pane Layout.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"r-working-directory","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.3 R Working Directory","text":"working directory folder R look files save output default. Issues conflicting working directories common, especially multiple folders different projects. best practices deal .Using setwd(): can use setwd() function set working directory. example, setwd(\"~/Documents/R Projects/MyProject\") sets working directory “MyProject” folder within “R Projects” folder “Documents” directory. can also open new script (tab top left), right-click tab, tell R “Set working directory” wherever script located. working external data files (e.g., .dta database, example), data need folder script working .Using RStudio Interface: can also set working directory RStudio menu: “Session” > “Set Working Directory” > “Choose Directory…”.R Projects: better organization, consider creating R Projects assignments. R Project special type working directory makes easier manage code, data, output. can create new project going “File” > “New Project…”. RStudio automatically set working directory project folder. information working projects, consult guide: https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"introduction-to-rmarkdown","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.4 Introduction to RMarkdown","text":"RMarkdown file format allows combine R code, output (e.g., tables, plots), text single document. ’s powerful tool creating reproducible reports assignments. can also use write HTML render PDFs. However, can little finicky intuitive write , compared common word processors like MSWord Google Docs, takes practice (patience) get place feel comfortable writing .","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"why-use-rmarkdown","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.4.1 Why Use RMarkdown?","text":"Reproducibility: code, output, text one place, making easy reproduce analysis.Clarity: can interweave code explanations narrative, making work easier understand.Efficiency: can generate different output formats (HTML, PDF, Word) RMarkdown file. outputs also look pretty nice.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"basic-rmarkdown-syntax","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.4.2 Basic RMarkdown Syntax","text":"Code Chunks: R code enclosed “chunks” start ```{r} end ```.\n\n# R code chunk\nx <- 10\ny <- 20\nx + yCode Chunks: R code enclosed “chunks” start ```{r} end ```.Headers: can create headers using #, ##, ###, etc. number # determines level header.Headers: can create headers using #, ##, ###, etc. number # determines level header.Text Formatting: can format text using Markdown syntax. example:\nItalic: *italic*\nBold: **bold**\nCode: `code`\nLinks: [Link Text](URL)\nText Formatting: can format text using Markdown syntax. example:Italic: *italic*Bold: **bold**Code: `code`Links: [Link Text](URL)","code":"\n# This is an R code chunk\nx <- 10\ny <- 20\nx + y"},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"creating-an-rmarkdown-file","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.4.3 Creating an RMarkdown File","text":"RStudio, go “File” > “New File” > “R Markdown…”.Choose title author name.Select desired output format (e.g., HTML, PDF).Click “OK”.RStudio create new RMarkdown file example content. can edit file, add code text, “knit” generate output document.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"knitting","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.4.4 Knitting","text":"“Knitting” process converting RMarkdown file output document. knit document, click “Knit” button top Source pane. can choose output format dropdown menu next “Knit” button.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"rmarkdown-cheatsheet","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.4.5 RMarkdown cheatsheet:","text":"useful quick guide can use help formatting related tasks: https://rstudio.github.io/cheatsheets/html/rmarkdown.html","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"installing-r-packages","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.5 Installing R Packages","text":"R packages extend functionality base R. progress course, ’ll use several packages help perform different tasks related statistical analysis. need install packages , need load new session.basic packages use go installing .Experienced: experience R, might interested exploring pacman package. provides convenient way manage packages, allowing load, install, update packages using single function, p_load(). example, instead repeated install.packages operations , pacman installed use one line like: pacman::p_load(devtools, remotes, foreign, readstata13, rio, labelled, sjlabelled, tidyverse, fixest, wooldridge, modelsummary, stargazer, ggplot2, knitr, kableExtra, markdown, car, carData, lmtest, sandwich, survey, srvyr). Using pacman may also automatically install packages CRAN, Bioconductor GitHub, according latest version . ’re interested, can learn pacman CRAN page. However, course, using standard install.packages() method sufficient.","code":"\n# Install packages for data import/export:\ninstall.packages(\"foreign\")\ninstall.packages(\"rio\")\n\n# Install tidyverse, a collection of packages for data science:\ninstall.packages(\"tidyverse\")\n\n# Install wooldridge, which contains datasets you will use for the assignments:\ninstall.packages(\"wooldridge\")\n\n# Install knitr and markdown for improved RMarkdown functionality:\ninstall.packages(\"knitr\")\ninstall.packages(\"markdown\")"},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"using-ai-tools-to-assist-with-r-programming","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.6 Using AI Tools to Assist with R Programming","text":"Large Language Models (LLMs) related AI tools, ChatGPT, Copilot Gemini, can valuable assistants learning using R. used correctly, can save lot time, especially debugging finding ways performing specific tasks ’re really familiar . However, ’s crucial use responsibly ethically. general tips using tools.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"how-ai-can-help","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.6.1 How AI Can Help","text":"Code Suggestions Auto-Completion: AI tools like GitHub Copilot can suggest code snippets type, helping write code faster fewer errors. also hallucinate times, mindful suggesting. ’m big fan auto-complete, ’s usually better either ask something specific ask edits base code ’ve already written.Debugging Assistance: encounter error, AI can help explain error message means suggest potential solutions. Just copy paste error message (relevant excerpt), tell wanted , ask causing error.Understanding Functions Packages: AI can provide explanations R functions work, arguments take, use effectively. Check documentation make sure LLMs telling something mark.Learning New Concepts: can ask AI explain statistical concepts R programming topics way ’s easy understand. Documentation references StackOverflow can bit cryptic.Finding Relevant Documentation: AI can help locate relevant documentation R packages functions.Generating Code Specific Tasks: can describe task want accomplish, AI can generate R code perform task. better informed prompt , better output get. models typically better contained, well-defined, tasks broad tasks underspecified.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"limitations-and-ethical-considerations","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.6.2 Limitations and Ethical Considerations","text":"Critical Thinking Essential: AI tool, replacement understanding. Always critically evaluate code generated AI. Make sure understand works using .Don’t Blindly Copy Paste: Avoid copying pasting code without understanding . can lead errors lack comprehension.Avoid Plagiarism: transparent use AI. Properly cite AI-generated code ideas assignments. instructors need know work came AI assistance.AI Makes Mistakes: AI perfect. can generate incorrect inefficient code. Always test code thoroughly.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"prompt-engineering-tips","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.6.3 Prompt Engineering Tips","text":"Specific Clear: specific prompts, better AI understand request.Provide Context: ’re asking specific piece code, include relevant code prompt.Break Complex Tasks: complex task, break smaller, manageable steps.Iterate Refine: AI doesn’t give answer ’re looking , try rephrasing prompt providing information.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"example","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.6.4 Example","text":"’s example use AI tool help generate plot:Prompt: “data frame called mydata columns x y. can create scatterplot y x using ggplot2, blue points red trend line?”AI-Generated Code (example):Remember: still need load ggplot2 package (library(ggplot2)) data frame mydata loaded R environment code work.","code":"\nggplot(mydata, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")"},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"quick-example-importing-and-checking-data-from-wooldridge","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.7 Quick Example: Importing and Checking Data from wooldridge","text":"Let’s load dataset wooldridge package take quick look . ’ll use wage1 dataset, contains information wages individual characteristics.Explanation:library(wooldridge): Loads wooldridge package, making datasets available.data(\"wage1\"): Loads wage1 dataset R environment.head(wage1): Displays first six rows dataset, allowing see variable names sample data.\nExercise: Try changing number inside parentheses head() function display different number rows.\nExercise: Try changing number inside parentheses head() function display different number rows.summary(wage1): Provides descriptive statistics variable dataset (e.g., mean, median, min, max, quartiles).str(wage1): Shows structure dataset, including data type variable (e.g., numeric, integer, factor).","code":"\n# Load the wooldridge package\nlibrary(wooldridge)\n\n# Load the wage1 dataset\ndata(\"wage1\")\n\n# Display the first few rows of the dataset\nhead(wage1)##   wage educ exper tenure nonwhite female married numdep smsa northcen south west\n## 1 3.10   11     2      0        0      1       0      2    1        0     0    1\n## 2 3.24   12    22      2        0      1       1      3    1        0     0    1\n## 3 3.00   11     2      0        0      0       0      2    0        0     0    1\n## 4 6.00    8    44     28        0      0       1      0    1        0     0    1\n## 5 5.30   12     7      2        0      0       1      1    0        0     0    1\n## 6 8.75   16     9      8        0      0       1      0    1        0     0    1\n##   construc ndurman trcommpu trade services profserv profocc clerocc servocc\n## 1        0       0        0     0        0        0       0       0       0\n## 2        0       0        0     0        1        0       0       0       1\n## 3        0       0        0     1        0        0       0       0       0\n## 4        0       0        0     0        0        0       0       1       0\n## 5        0       0        0     0        0        0       0       0       0\n## 6        0       0        0     0        0        1       1       0       0\n##      lwage expersq tenursq\n## 1 1.131402       4       0\n## 2 1.175573     484       4\n## 3 1.098612       4       0\n## 4 1.791759    1936     784\n## 5 1.667707      49       4\n## 6 2.169054      81      64\n# Get a summary of the dataset\nsummary(wage1)##       wage             educ           exper           tenure      \n##  Min.   : 0.530   Min.   : 0.00   Min.   : 1.00   Min.   : 0.000  \n##  1st Qu.: 3.330   1st Qu.:12.00   1st Qu.: 5.00   1st Qu.: 0.000  \n##  Median : 4.650   Median :12.00   Median :13.50   Median : 2.000  \n##  Mean   : 5.896   Mean   :12.56   Mean   :17.02   Mean   : 5.105  \n##  3rd Qu.: 6.880   3rd Qu.:14.00   3rd Qu.:26.00   3rd Qu.: 7.000  \n##  Max.   :24.980   Max.   :18.00   Max.   :51.00   Max.   :44.000  \n##     nonwhite          female          married           numdep     \n##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n##  Median :0.0000   Median :0.0000   Median :1.0000   Median :1.000  \n##  Mean   :0.1027   Mean   :0.4791   Mean   :0.6084   Mean   :1.044  \n##  3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:2.000  \n##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :6.000  \n##       smsa           northcen         south             west       \n##  Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n##  Median :1.0000   Median :0.000   Median :0.0000   Median :0.0000  \n##  Mean   :0.7224   Mean   :0.251   Mean   :0.3555   Mean   :0.1692  \n##  3rd Qu.:1.0000   3rd Qu.:0.750   3rd Qu.:1.0000   3rd Qu.:0.0000  \n##  Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n##     construc          ndurman          trcommpu           trade       \n##  Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n##  1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n##  Median :0.00000   Median :0.0000   Median :0.00000   Median :0.0000  \n##  Mean   :0.04563   Mean   :0.1141   Mean   :0.04373   Mean   :0.2871  \n##  3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n##  Max.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n##     services         profserv         profocc          clerocc      \n##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n##  Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n##  Mean   :0.1008   Mean   :0.2586   Mean   :0.3669   Mean   :0.1673  \n##  3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n##     servocc           lwage            expersq          tenursq       \n##  Min.   :0.0000   Min.   :-0.6349   Min.   :   1.0   Min.   :   0.00  \n##  1st Qu.:0.0000   1st Qu.: 1.2030   1st Qu.:  25.0   1st Qu.:   0.00  \n##  Median :0.0000   Median : 1.5369   Median : 182.5   Median :   4.00  \n##  Mean   :0.1407   Mean   : 1.6233   Mean   : 473.4   Mean   :  78.15  \n##  3rd Qu.:0.0000   3rd Qu.: 1.9286   3rd Qu.: 676.0   3rd Qu.:  49.00  \n##  Max.   :1.0000   Max.   : 3.2181   Max.   :2601.0   Max.   :1936.00\n# Get the structure of the dataset\nstr(wage1)## 'data.frame':    526 obs. of  24 variables:\n##  $ wage    : num  3.1 3.24 3 6 5.3 ...\n##  $ educ    : int  11 12 11 8 12 16 18 12 12 17 ...\n##  $ exper   : int  2 22 2 44 7 9 15 5 26 22 ...\n##  $ tenure  : int  0 2 0 28 2 8 7 3 4 21 ...\n##  $ nonwhite: int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ female  : int  1 1 0 0 0 0 0 1 1 0 ...\n##  $ married : int  0 1 0 1 1 1 0 0 0 1 ...\n##  $ numdep  : int  2 3 2 0 1 0 0 0 2 0 ...\n##  $ smsa    : int  1 1 0 1 0 1 1 1 1 1 ...\n##  $ northcen: int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ south   : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ west    : int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ construc: int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ ndurman : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ trcommpu: int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ trade   : int  0 0 1 0 0 0 1 0 1 0 ...\n##  $ services: int  0 1 0 0 0 0 0 0 0 0 ...\n##  $ profserv: int  0 0 0 0 0 1 0 0 0 0 ...\n##  $ profocc : int  0 0 0 0 0 1 1 1 1 1 ...\n##  $ clerocc : int  0 0 0 1 0 0 0 0 0 0 ...\n##  $ servocc : int  0 1 0 0 0 0 0 0 0 0 ...\n##  $ lwage   : num  1.13 1.18 1.1 1.79 1.67 ...\n##  $ expersq : int  4 484 4 1936 49 81 225 25 676 484 ...\n##  $ tenursq : int  0 4 0 784 4 64 49 9 16 441 ...\n##  - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"getting-help-in-r","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.8 Getting Help in R","text":"several ways get help R:? help(): access documentation specific function, type ? followed function name (e.g., ?mean). can also use help(mean).?? help.search(): search help topics related keyword, use ?? followed keyword (e.g., ??regression). can also use help.search(\"regression\").Online Resources: R community active online. Websites like Stack Overflow (https://stackoverflow.com/questions/tagged/r) great places find answers common R questions.Package Vignettes: Many R packages include vignettes, longer, tutorial-style documents demonstrate use package. can access using browseVignettes() function.Package Websites: packages dedicated websites additional resources, tutorials, articles, examples. can typically found searching package name followed “R package” search engine.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"style-guides-in-r","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.9 Style Guides in R","text":"Coding style guides sets conventions prescribe code formatted written. Following style guide can make code readable, maintainable, consistent.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"the-tidyverse-style-guide","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.9.1 The tidyverse Style Guide","text":"tidyverse style guide, developed Hadley Wickham RStudio team, widely used style guide R. covers various aspects coding style, including:File namesObject namesSyntaxSpacingControl flowCommentsYou can find tidyverse style guide : https://style.tidyverse.org/. Note: try follow guide, incredibly good .","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"quick-examples","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.9.2 Quick Examples","text":"examples tidyverse style guidelines:File Names: Use .R extension R script files .Rmd R Markdown files. File names meaningful use lowercase letters, numbers, underscores (e.g., process_data.R, analysis_report.Rmd).Object Names: Use lowercase underscores separate words (e.g., my_variable, data_frame). descriptive concise.Spacing: Place spaces around operators (e.g., x + y, x+y) commas (e.g., mean(x, na.rm = TRUE), mean(x,na.rm=TRUE)).Indentation: Use two spaces indentation. use tabs.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"other-style-guides","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.9.3 Other Style Guides","text":"tidyverse style guide popular, style guides might encounter choose follow, :Google’s R Style Guide: https://google.github.io/styleguide/Rguide.htmlThe Bioconductor Style Guide: https://contributions.bioconductor.org/Ultimately, important thing consistent style, regardless guide choose follow. style guides agree basic principles writing clear readable code.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"protips-for-r-success","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.10 Protips for R Success","text":"","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"enable-helpful-rstudio-options","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.10.1 Enable Helpful RStudio Options","text":"options can make coding experience pleasant efficient:Highlight R function calls: Go “Tools” > “Global Options” > “Code” > “Display” check “Highlight R function calls.” visually distinguish function names code.Rainbow parentheses: “Display” settings, check “Rainbow parentheses.” color-code matching parentheses, making easier track nested functions.Use dark theme: find staring screen long periods, dark theme can help reduce eye strain. Go “Tools” > “Global Options” > “Appearance” choose dark theme “Editor Theme” options.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"good-coding-practices","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.10.2 Good Coding Practices","text":"Organization: Use clear directory structure projects. Keep data, code, output separate folders.Comments: Explain code using comments (lines starting #). Focus explaining behind code, just .Readability: Use consistent spacing indentation make code easy read. RStudio can help automatically indenting code.Section Headings: Use # create section headings organize R scripts. number # determines level heading (e.g., # top-level heading, ## subheading, etc.).Don’t Overwrite: Avoid overwriting original variables. Create new variables need modify data.Backups: Regularly back work. Save reusable code snippets future use. larger projects, consider using separate R scripts different tasks (e.g., data cleaning, analysis, reporting).Coding Language: Learning code like learning new language. takes time practice. patient .Precision: Approach analysis care attention detail. Avoid rushing, especially tired.","code":""},{"path":"tutorial-1-getting-started-with-r-rstudio-and-rmarkdown.html","id":"solutions-to-common-problems","chapter":"2 Tutorial 1: Getting Started with R, RStudio and RMarkdown","heading":"2.10.3 Solutions to Common Problems","text":"Knitting Issues:\nRestart RStudio try knitting .\nCheck special characters (e.g., Greek letters) might causing problems.\nMake sure PDF file (’re knitting PDF) open another program.\nDelete temporary .md .tex files created knitting process.\nConsider disabling “use tinytex compiling .tex files” “Tools” > “Global Options” > “Sweave” ’re issues TinyTeX.\nRestart RStudio try knitting .Check special characters (e.g., Greek letters) might causing problems.Make sure PDF file (’re knitting PDF) open another program.Delete temporary .md .tex files created knitting process.Consider disabling “use tinytex compiling .tex files” “Tools” > “Global Options” > “Sweave” ’re issues TinyTeX.Error Messages:\nCarefully check function arguments. enter correctly right order?\nPay attention capitalization use quotes.\nencounter naming conflicts (e.g., dplyr::recode() vs. car::recode()), specify package want use (e.g., dplyr::recode()).\nCarefully check function arguments. enter correctly right order?Pay attention capitalization use quotes.encounter naming conflicts (e.g., dplyr::recode() vs. car::recode()), specify package want use (e.g., dplyr::recode()).Model Problems:\nDouble-check ’re using correct variables model.\nReview recoding steps performed. make mistakes might affecting results?\nDouble-check ’re using correct variables model.Review recoding steps performed. make mistakes might affecting results?","code":""},{"path":"lecture-2-introduction-to-causal-inference.html","id":"lecture-2-introduction-to-causal-inference","chapter":"3 Lecture 2: Introduction to Causal Inference","heading":"3 Lecture 2: Introduction to Causal Inference","text":"","code":""},{"path":"lecture-2-introduction-to-causal-inference.html","id":"slides-1","chapter":"3 Lecture 2: Introduction to Causal Inference","heading":"Slides","text":"3 Introduction Causal Inference (link)","code":""},{"path":"lecture-2-introduction-to-causal-inference.html","id":"introduction-1","chapter":"3 Lecture 2: Introduction to Causal Inference","heading":"3.1 Introduction","text":"now dive deeper causal inference counterfactual problem. show randomized trails solve counterfactual problem, also counterfactual problem still problem using observational data.lecture slide displayed full :\nFigure 3.1: Slides 3 Introduction Causal Inference.\n","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling"},{"path":"lecture-2-introduction-to-causal-inference.html","id":"vignette-2.1","chapter":"3 Lecture 2: Introduction to Causal Inference","heading":"3.2 Vignette 2.1","text":"Usually, know data generation process, , gods. Let’s create world taking treatment (e.g., taking pill) positively affect Y (e.g., health) one unit. Let’s run experiment.Now can create counterfactual:Let’s look counterfactual:Now let’s give individual treatment (either pill placebo):can see average effect pill treated group (remember lecture effect , essence, difference receive treatment, ):can plot :","code":"\ndf <- data.frame(health_no_pill= rnorm(5000),\n                 # Randomly assign a treatment\n                 pill=sample(c(0,1),5000,replace=T))\nhist(df$health_no_pill)\nknitr::kable(table(df$pill), format=\"markdown\")\ndf <- df %>%\n  mutate(health_w_pill = health_no_pill + 1) # Our Y when A=1 aka our counterfactual## mutate: new variable 'health_w_pill' (double) with 5,000 unique values and 0% NA\nhealth_w_pill <- cbind.data.frame(df$health_w_pill,\"with Pill\")\ncolnames(health_w_pill) <- c(\"health\",\"treatment\")\nhealth_no_pill <- cbind.data.frame(df$health_no_pill,\"without Pill\")\ncolnames(health_no_pill) <- c(\"health\",\"treatment\")\ncomparison_y <- rbind.data.frame(health_w_pill,health_no_pill)\n\ncomparison_y %>%\n  group_by(treatment) %>%\n  mutate(mean_health = mean(health)) %>%\n  ungroup() %>%\n  ggplot(aes(x=health,fill = treatment,color = treatment)) +\n  geom_density(alpha = .5) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +\n  geom_vline(aes(xintercept = mean_health, color = treatment ),\n             linetype = \"dashed\")## group_by: one grouping variable (treatment)\n## mutate (grouped): new variable 'mean_health' (double) with 2 unique values and 0% NA\n## ungroup: no grouping variables remain\ndf <- df %>%\n  mutate(health_obs = ifelse(pill==1,health_w_pill,health_no_pill))## mutate: new variable 'health_obs' (double) with 5,000 unique values and 0% NA\nhead(df,10)##    health_no_pill pill health_w_pill health_obs\n## 1       0.8423212    0     1.8423212  0.8423212\n## 2      -0.5428214    1     0.4571786  0.4571786\n## 3      -1.5839233    1    -0.5839233 -0.5839233\n## 4      -1.6744286    0    -0.6744286 -1.6744286\n## 5       0.5431052    1     1.5431052  1.5431052\n## 6      -0.6689065    1     0.3310935  0.3310935\n## 7      -0.2207760    0     0.7792240 -0.2207760\n## 8       0.4573494    1     1.4573494  1.4573494\n## 9       0.4404777    0     1.4404777  0.4404777\n## 10      0.0932991    1     1.0932991  1.0932991\ndf %>%\n  group_by(pill) %>%\n  summarize(health = mean(health_obs))## group_by: one grouping variable (pill)\n## summarize: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##    pill   health\n##   <dbl>    <dbl>\n## 1     0 -0.00903\n## 2     1  1.01\ndf %>%\n  group_by(pill) %>%\n  mutate(mean_health_obs = mean(health_obs)) %>%\n  ungroup() %>%\n  ggplot(aes(x=health_obs,fill = factor(pill),color = factor(pill))) +\n  geom_density(alpha = .5) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +\n  geom_vline(aes(xintercept = mean_health_obs, color = factor(pill) ),\n             linetype = \"dashed\")## group_by: one grouping variable (pill)\n## mutate (grouped): new variable 'mean_health_obs' (double) with 2 unique values and 0% NA\n## ungroup: no grouping variables remain"},{"path":"lecture-2-introduction-to-causal-inference.html","id":"vignette-2.2","chapter":"3 Lecture 2: Introduction to Causal Inference","heading":"3.3 Vignette 2.2","text":"Ok… happens randomize? observational data, …Let’s see happens now estimated mean average ‘effect’ (remember lecture effect , essence, difference receive treatment, ):Oh ! actual effect pill, know 1 since created . However, properly model (RDD!), (remember lecture effect , essence, difference receive treatment, ):","code":"\ndf <- data.frame(income = runif(10000)) %>%\n  # In this case, your health is determined randomly AND by your levels of income\n  mutate(health_no_pill = rnorm(10000) + income,\n         health_w_pill = health_no_pill + 1) %>%\n  # Now we give the pill only to people that have money\n  mutate(pill = income > .7,\n         health_obs = ifelse(pill==1,health_w_pill,health_no_pill))## mutate: new variable 'health_no_pill' (double) with 10,000 unique values and 0% NA\n##         new variable 'health_w_pill' (double) with 10,000 unique values and 0% NA\n## mutate: new variable 'pill' (logical) with 2 unique values and 0% NA\n##         new variable 'health_obs' (double) with 10,000 unique values and 0% NA\nhead(df,10)##        income health_no_pill health_w_pill  pill health_obs\n## 1  0.15681246    -0.47126247     0.5287375 FALSE -0.4712625\n## 2  0.99598697    -0.80685390     0.1931461  TRUE  0.1931461\n## 3  0.11365139     0.60829196     1.6082920 FALSE  0.6082920\n## 4  0.04471854    -0.19103219     0.8089678 FALSE -0.1910322\n## 5  0.08604455    -1.84413255    -0.8441325 FALSE -1.8441325\n## 6  0.80914912     0.02072352     1.0207235  TRUE  1.0207235\n## 7  0.59190844     0.73166084     1.7316608 FALSE  0.7316608\n## 8  0.90610009     0.51558137     1.5155814  TRUE  1.5155814\n## 9  0.92152681     3.51082541     4.5108254  TRUE  4.5108254\n## 10 0.12657253     0.57418145     1.5741814 FALSE  0.5741814\ndf %>%\n  group_by(pill) %>%\n  summarize(health = mean(health_obs))## group_by: one grouping variable (pill)\n## summarize: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##   pill  health\n##   <lgl>  <dbl>\n## 1 FALSE  0.342\n## 2 TRUE   1.87\ndf %>%\n  filter(abs(income-.7)<.01) %>%\n  group_by(pill) %>%\n  summarize(health = mean(health_obs)) ## BOOM!!## filter: removed 9,769 rows (98%), 231 rows remaining\n## group_by: one grouping variable (pill)\n## summarize: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##   pill  health\n##   <lgl>  <dbl>\n## 1 FALSE  0.636\n## 2 TRUE   1.85"},{"path":"tutorial-2-key-concepts.html","id":"tutorial-2-key-concepts","chapter":"4 Tutorial 2: Key Concepts","heading":"4 Tutorial 2: Key Concepts","text":"","code":""},{"path":"tutorial-2-key-concepts.html","id":"what-is-this-tutorial-about","chapter":"4 Tutorial 2: Key Concepts","heading":"What is this tutorial about?","text":"can access tutorial clicking . tutorial quiz key concepts Chapter 1 “Causal Inference: ” Hernán Robins well Chapter 3 (“Describing Variables”) “Effect” Huntington-Klein.","code":""},{"path":"lecture-2-exercises.html","id":"lecture-2-exercises","chapter":"5 Lecture 2 Exercises","heading":"5 Lecture 2 Exercises","text":"tutorial created John Santos (minor adaptations ).","code":""},{"path":"lecture-2-exercises.html","id":"main-exercise","chapter":"5 Lecture 2 Exercises","heading":"5.1 Main Exercise","text":"data ’fertil2’ collected women living Republic Botswana 1988. variable children refers number living children. variable electric binary indicator equal one woman’s home electricity, zero . Using “fertil2” data {wooldridge}…Find smallest largest values children sample. average children?percentage women electricity home?Compute average children without electricity electricity.part (iii), can infer electricity “causes” women fewer children?() Find smallest largest values children sample. average children?Using Base R…Using describe() function psych package…(ii) percentage women electricity home?Using Base…Using tidyverse conventions…14% women electricity.(iii) Compute average children without electricity electricity.Using Base manually calculate averages subsets…code , translated plain English, something like: “Calculate mean fertil2$children cases fertil2$electric equals 0, removing cases NAs.”code calculates compliment code . plain English, code says, “Calculate mean fertil2$children cases fertil2$electric equals 1, removing cases NAs.”Mean number children among women without electricity = 2.33.Mean number children among women electricity = 1.90.also use t.test() command Base R:Mean difference = 0.43, \\(p\\leq0.001\\), 95% CI = 0.27 0.59.average, women electricity 0.43 fewer children women without electricity, difference statistically significant.(iv) part (iii), can infer electricity “causes” women fewer children?women electricity, average, fewer children women without electricity, relationship statistically significant, necessarily infer electricity “causes” women fewer children. need mechanism link electricity children conclude electricity cause.Perhaps, electricity spurious common cause SES.","code":"\nlibrary(wooldridge)\nlibrary(tidyverse)\nlibrary(psych)## \n## Attaching package: 'psych'## The following objects are masked from 'package:ggplot2':\n## \n##     %+%, alpha\ndata(\"fertil2\")\nmin(fertil2$children)## [1] 0\nmax(fertil2$children)## [1] 13\nmean(fertil2$children)## [1] 2.267828\nsummary(fertil2$children)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.000   0.000   2.000   2.268   4.000  13.000\ndescribe(fertil2$children)##    vars    n mean   sd median trimmed  mad min max range skew kurtosis   se\n## X1    1 4361 2.27 2.22      2    1.95 2.97   0  13    13 1.07     0.75 0.03\nprop.table(table(fertil2$electric))## \n##         0         1 \n## 0.8597981 0.1402019\nfertil2%>%\n  select(electric)%>%\n  table()/nrow(fertil2)## select: dropped 26 variables (mnthborn, yearborn, age, radio, tv, …)## electric\n##         0         1 \n## 0.8592066 0.1401055\nmean(fertil2$children[fertil2$electric==0], na.rm = TRUE)## [1] 2.327729\nmean(fertil2$children[fertil2$electric==1], na.rm = TRUE)## [1] 1.898527\nt.test(fertil2$children ~ fertil2$electric)## \n##  Welch Two Sample t-test\n## \n## data:  fertil2$children by fertil2$electric\n## t = 5.2409, df = 958, p-value = 1.965e-07\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  0.2684895 0.5899142\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        2.327729        1.898527\n# dplyr\nlibrary(dplyr)\nfertil2 %>%\n  group_by(electric) %>%\n  summarise(mean = mean(children),\n            sd = sd(children))## group_by: one grouping variable (electric)\n## summarise: now 3 rows and 3 columns, ungrouped## # A tibble: 3 × 3\n##   electric  mean    sd\n##      <int> <dbl> <dbl>\n## 1        0  2.33  2.28\n## 2        1  1.90  1.80\n## 3       NA  2.67  2.89"},{"path":"lecture-2-exercises.html","id":"additional-exercises","chapter":"5 Lecture 2 Exercises","heading":"5.2 Additional Exercises:","text":"Use {ces.Rda} data found .Overall ratings TrudeauThe variables feel_trudeau feeling thermometer ratings Liberal Leader Justin Trudeau. average, Canadians rate ? ’s lowest rating? ’s highest rating?Trudeau ratings groupsDo Trudeau’s ratings vary across groups population? Specifically, look gender (gender), age (agegrp), education (educ).variable (leftrightgrp) measures whether individual places left (0-4), centre (5), right (6-10) political spectrum. ratings Trudeau vary across self-placed ideological categories?","code":""},{"path":"lecture-2-exercises.html","id":"stop","chapter":"5 Lecture 2 Exercises","heading":"5.2.1 STOP!!","text":"continue, try solving exercises . ’s way learn. , come back page see well .","code":""},{"path":"lecture-2-exercises.html","id":"continue","chapter":"5 Lecture 2 Exercises","heading":"5.2.2 Continue","text":"","code":"\nload(\"Sample_data/ces.Rda\")"},{"path":"lecture-2-exercises.html","id":"overall-ratings-of-trudeau","chapter":"5 Lecture 2 Exercises","heading":"5.2.3 Overall ratings of Trudeau","text":"variable feel_trudeau feeling thermometer ratings Liberal Leader Justin Trudeau. average, Canadians rate ? ’s lowest rating? ’s highest rating?Using base R.D’oh! didn’t work NAs.Let’s remove using option na.rm = TRUE.Alternatively, can use summary() command.can using dplyr. can use method calculate summary statistics time.also calculate statistics…","code":"\nmean(ces$feel_trudeau)## [1] NA\nmin(ces$feel_trudeau)## [1] NA\nmax(ces$feel_trudeau)## [1] NA\nmean(ces$feel_trudeau, na.rm = TRUE)## [1] 44.84804\nmin(ces$feel_trudeau, na.rm = TRUE)## [1] 0\nmax(ces$feel_trudeau, na.rm = TRUE)## [1] 100\nsummary(ces$feel_trudeau)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00    7.00   50.00   44.85   75.00  100.00    2409\nlibrary(dplyr)\nces %>%\n  summarise(mean = mean(feel_trudeau, na.rm = TRUE),\n            min = min(feel_trudeau, na.rm = TRUE),\n            max = max(feel_trudeau, na.rm = TRUE))## summarise: now one row and 3 columns, ungrouped##       mean min max\n## 1 44.84804   0 100\nces %>%\n  summarise(mean = mean(feel_trudeau, na.rm = TRUE),\n            median = median(feel_trudeau, na.rm = TRUE),\n            min = min(feel_trudeau, na.rm = TRUE),\n            max = max(feel_trudeau, na.rm = TRUE),\n            sd = sd(feel_trudeau, na.rm = TRUE),\n            se = sd(feel_trudeau, na.rm = TRUE) / sqrt(sum(!is.na(feel_trudeau))),\n            lower95 = mean - (1.96*se),\n            upper95 = mean + (1.96*se)) ## summarise: now one row and 8 columns, ungrouped##       mean median min max       sd        se  lower95 upper95\n## 1 44.84804     50   0 100 34.54668 0.1838109 44.48777 45.2083"},{"path":"lecture-2-exercises.html","id":"trudeau-ratings-by-groups","chapter":"5 Lecture 2 Exercises","heading":"5.2.4 Trudeau ratings by groups","text":"Trudeau’s ratings vary across groups population? look gender (gender), age (agegrp), education (educ). variable (leftrightgrp) measures whether individual places left (0-4), centre (5), right (6-10) political spectrum. ratings Trudeau vary across self-placed ideological categories?","code":""},{"path":"lecture-2-exercises.html","id":"gender","chapter":"5 Lecture 2 Exercises","heading":"5.2.5 Gender","text":"Using dplyr…can also use base R command t.test().option somewhat limited works comparing across two categories. However, test significance difference, useful.","code":"\nmean(ces$feel_trudeau[ces$gender==\"Man\"], na.rm=T)## [1] 43.35218\nmean(ces$feel_trudeau[ces$gender==\"Woman\"], na.rm=T)## [1] 45.93724\nt.test(ces$feel_trudeau ~ ces$gender)## \n##  Welch Two Sample t-test\n## \n## data:  ces$feel_trudeau by ces$gender\n## t = -6.9009, df = 31483, p-value = 5.266e-12\n## alternative hypothesis: true difference in means between group Man and group Woman is not equal to 0\n## 95 percent confidence interval:\n##  -3.319280 -1.850828\n## sample estimates:\n##   mean in group Man mean in group Woman \n##            43.35218            45.93724\nces %>%\n  group_by(gender) %>%\n  summarise(avg = mean(feel_trudeau, na.rm = TRUE))## group_by: one grouping variable (gender)\n## summarise: now 3 rows and 2 columns, ungrouped## # A tibble: 3 × 2\n##   gender   avg\n##   <fct>  <dbl>\n## 1 Man     43.4\n## 2 Woman   45.9\n## 3 <NA>    44.8\nt.test(ces$feel_trudeau ~ ces$gender, na.rm = TRUE)## \n##  Welch Two Sample t-test\n## \n## data:  ces$feel_trudeau by ces$gender\n## t = -6.9009, df = 31483, p-value = 5.266e-12\n## alternative hypothesis: true difference in means between group Man and group Woman is not equal to 0\n## 95 percent confidence interval:\n##  -3.319280 -1.850828\n## sample estimates:\n##   mean in group Man mean in group Woman \n##            43.35218            45.93724"},{"path":"lecture-2-exercises.html","id":"age","chapter":"5 Lecture 2 Exercises","heading":"5.2.5.1 Age","text":"","code":"\nces %>%\n  group_by(agegrp) %>%\n  summarise(avg = mean(feel_trudeau, na.rm = TRUE))## group_by: one grouping variable (agegrp)\n## summarise: now 3 rows and 2 columns, ungrouped## # A tibble: 3 × 2\n##   agegrp   avg\n##   <fct>  <dbl>\n## 1 18-34   49.1\n## 2 35-54   43.2\n## 3 55+     43.7"},{"path":"lecture-2-exercises.html","id":"education","chapter":"5 Lecture 2 Exercises","heading":"5.2.5.2 Education","text":"","code":"\nces %>%\n  group_by(educ) %>%\n  summarise(avg = mean(feel_trudeau, na.rm = TRUE))## group_by: one grouping variable (educ)\n## summarise: now 5 rows and 2 columns, ungrouped## # A tibble: 5 × 2\n##   educ         avg\n##   <fct>      <dbl>\n## 1 HS or less  38.0\n## 2 Some PSE    42.3\n## 3 Bachelors   51.3\n## 4 Postgrad    52.1\n## 5 <NA>        38.1"},{"path":"lecture-2-exercises.html","id":"ideology","chapter":"5 Lecture 2 Exercises","heading":"5.2.5.3 Ideology","text":"","code":"\nces %>%\n  group_by(leftrightgrp) %>%\n  summarise(avg = mean(feel_trudeau, na.rm = TRUE))## group_by: one grouping variable (leftrightgrp)\n## summarise: now 4 rows and 2 columns, ungrouped## # A tibble: 4 × 2\n##   leftrightgrp   avg\n##   <fct>        <dbl>\n## 1 Left          59.4\n## 2 Centre        42.9\n## 3 Right         37.8\n## 4 <NA>          43.6"},{"path":"lecture-3-core-concepts-of-experimental-design.html","id":"lecture-3-core-concepts-of-experimental-design","chapter":"6 Lecture 3: Core Concepts of Experimental Design","heading":"6 Lecture 3: Core Concepts of Experimental Design","text":"","code":""},{"path":"lecture-3-core-concepts-of-experimental-design.html","id":"slides-2","chapter":"6 Lecture 3: Core Concepts of Experimental Design","heading":"Slides","text":"4 Core Concepts Experimental Design (link)","code":""},{"path":"lecture-3-core-concepts-of-experimental-design.html","id":"introduction-2","chapter":"6 Lecture 3: Core Concepts of Experimental Design","heading":"6.1 Introduction","text":"lecture look (slightly) technical understanding selection bias (understand problem observational data) potential outcomes approach (understand random assignment solves problem).lecture slide displayed full :\nFigure 6.1: Slides 4 Core Concepts Experimental Design.\n","code":""},{"path":"lecture-3-core-concepts-of-experimental-design.html","id":"vignette-3.1","chapter":"6 Lecture 3: Core Concepts of Experimental Design","heading":"6.2 Vignette 3.1","text":"Liebman, Jeffrey B., Erzo FP Luttmer. “people behave differently better understood social security? Evidence field experiment.” American Economic Journal: Economic Policy 7.1 (2015): 275-99.variable interest (DV, Y) paid_work_yes: whether person worked . treatment variable treat: whether got pamphlet . experiment, can compare averages treatment group control group estimate causal effect.effect reported authors!! can also estimate differences gender (?):Treatment effect women… ?averages look great can sure effects mere coincidence product randomization? Let’s add confidence intervals:now gender:Cool…","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(sjPlot) # to plot somse models\nlibrary(readstata13) # to load .dta files\nexp_data <- read.dta13(\"Sample_data/Data_WithAgeInconsistency.dta\")\nexp_data %>%\n  group_by(treat) %>% \n  summarise(ATE = mean(paid_work_yes, na.rm = T))## group_by: one grouping variable (treat)\n## summarise: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##   treat   ATE\n##   <dbl> <dbl>\n## 1     0 0.744\n## 2     1 0.785\nexp_data %>%\n  group_by(female,treat) %>% \n  summarise(ATE = mean(paid_work_yes, na.rm = T))## group_by: 2 grouping variables (female, treat)\n## summarise: now 4 rows and 3 columns, one group variable remaining (female)## # A tibble: 4 × 3\n## # Groups:   female [2]\n##   female treat   ATE\n##    <int> <dbl> <dbl>\n## 1      0     0 0.784\n## 2      0     1 0.787\n## 3      1     0 0.713\n## 4      1     1 0.782\nexp_data$treat <- as.factor(exp_data$treat)\nexp_data$female <- as.factor(exp_data$female)\n\n# Full model\nmodel_1 <- lm(paid_work_yes ~ treat, data = exp_data)\nsummary(model_1)## \n## Call:\n## lm(formula = paid_work_yes ~ treat, data = exp_data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.7845  0.2155  0.2155  0.2555  0.2555 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.74446    0.01530  48.666   <2e-16 ***\n## treat1       0.04004    0.02124   1.885   0.0596 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4237 on 1591 degrees of freedom\n##   (890 observations deleted due to missingness)\n## Multiple R-squared:  0.002228,   Adjusted R-squared:  0.001601 \n## F-statistic: 3.553 on 1 and 1591 DF,  p-value: 0.05961\nplot_model(model_1, type = \"pred\", terms = \"treat\") +\n  theme_minimal() +\n  labs(x=\"Treatment\", y=\"Worked +1 Year\",\n       title=\"Predicted Effect of Treatment\")\n# Model by gender\nmodel_2 <- lm(paid_work_yes ~ female + treat + female*treat, data = exp_data)\nsummary(model_2)## \n## Call:\n## lm(formula = paid_work_yes ~ female + treat + female * treat, \n##     data = exp_data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.7871  0.2129  0.2164  0.2176  0.2871 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     0.783626   0.022885  34.242   <2e-16 ***\n## female1        -0.070685   0.030743  -2.299   0.0216 *  \n## treat1          0.003436   0.031725   0.108   0.9138    \n## female1:treat1  0.066040   0.042680   1.547   0.1220    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4232 on 1589 degrees of freedom\n##   (890 observations deleted due to missingness)\n## Multiple R-squared:  0.005552,   Adjusted R-squared:  0.003675 \n## F-statistic: 2.957 on 3 and 1589 DF,  p-value: 0.03136\nplot_model(model_2, type = \"int\") +\n  theme_minimal() +\n  labs(x=\"Women\", y=\"Worked +1 Year\", color = \"Treatment\",\n       title=\"Predicted Effect of Treatment by Gender\")"},{"path":"tutorial-3-more-key-concepts.html","id":"tutorial-3-more-key-concepts","chapter":"7 Tutorial 3: More Key Concepts","heading":"7 Tutorial 3: More Key Concepts","text":"","code":""},{"path":"tutorial-3-more-key-concepts.html","id":"what-is-this-tutorial-about-1","chapter":"7 Tutorial 3: More Key Concepts","heading":"What is this tutorial about?","text":"can access tutorial clicking . tutorial test understanding key concepts Chapter 2 “Causal Inference: ” Hernán Robins, focusing randomized experiments. ’ll engage theoretical concepts practical data analysis.","code":""},{"path":"lecture-3-exercises.html","id":"lecture-3-exercises","chapter":"8 Lecture 3 Exercises","heading":"8 Lecture 3 Exercises","text":"tutorial created John Santos (minor adaptations ).start exercises, take look dplyr pipping.","code":""},{"path":"lecture-3-exercises.html","id":"dplyr-primer","chapter":"8 Lecture 3 Exercises","heading":"8.1 {dplyr} primer","text":"dplyr R package part tidyverse family packages. tidyverse provides unified “grammar” coding, opposed idiosyncratic conventions Base R.dplyr focuses data management/transformation.purposes, used dplyr functions include:select(): pick column (.e. variable) multiple columns data frame.mutate(): create new column.summarise(): create new column calculated existing column.group_by(): precedes summarise() command tells R column want summarise.filter(): pick rows (.e. cases) according criterion/criteria.nifty thing dplyr (tidyverse generally) use “pipelines,” created using pipe operator, %>%. means need identify data frame working “pipe ” “pipeline operations”, saves time reduces chance coding errors.(Note: RStudio also “native pipe operator”, |>, works newer versions R yet universally adopted.)","code":""},{"path":"lecture-3-exercises.html","id":"the-select-command-in-action","chapter":"8 Lecture 3 Exercises","heading":"8.1.1 The select() command in action","text":"Say wanted pick feeling thermometer questions sample CES data set.column names:Let’s pull columns using Base R put new data frame called “feelings_base”. ’m feeling lazy, ’m just going parties.also pull columns using index numbers, isn’t recommended index numbers change depending version data set previous operations performed .dplyr way efficient:get even fancier use starts_with() command select every column whose column name starts character string “feel_”:","code":"\nload(\"Sample_data/ces.rda\")\nnames(ces)##  [1] \"weight\"        \"age\"           \"agegrp\"        \"age_18to34\"   \n##  [5] \"age_35to54\"    \"age_55plus\"    \"gender\"        \"woman\"        \n##  [9] \"lang\"          \"province\"      \"region\"        \"reg_on\"       \n## [13] \"reg_qc\"        \"reg_atl\"       \"reg_west\"      \"educ\"         \n## [17] \"univ\"          \"relig\"         \"rel_catholic\"  \"rel_christian\"\n## [21] \"rel_other\"     \"rel_none\"      \"income\"        \"incomegrp\"    \n## [25] \"mostimp\"       \"turnout\"       \"leftright\"     \"leftrightgrp\" \n## [29] \"lr_left\"       \"lr_centre\"     \"lr_right\"      \"feel_lib\"     \n## [33] \"feel_cpc\"      \"feel_ndp\"      \"feel_bloc\"     \"feel_green\"   \n## [37] \"feel_ppc\"      \"feel_trudeau\"  \"feel_scheer\"   \"feel_singh\"   \n## [41] \"feel_blanchet\" \"feel_may\"      \"feel_bernier\"  \"polinterest\"  \n## [45] \"marketlib\"     \"moraltrad\"     \"helpgroups\"\nfeelings_base <- data.frame(\n  feel_lib = ces$feel_lib,\n  feel_cpc = ces$feel_cpc,\n  feel_ndp = ces$feel_ndp,\n  feel_bloc = ces$feel_bloc,\n  feel_green = ces$feel_green,\n  feel_ppc = ces$feel_ppc\n)\nhead(feelings_base)##   feel_lib feel_cpc feel_ndp feel_bloc feel_green feel_ppc\n## 1       52        0       64        NA         97        0\n## 2       51        0       64        40         88        0\n## 3       52       49       NA        NA         NA       NA\n## 4       30       85        0         0          0       30\n## 5       20       62       51        20         50       20\n## 6       81       31       79        NA         85        0\nfeelings_numsel <- ces[32:43]\nhead(feelings_numsel)##   feel_lib feel_cpc feel_ndp feel_bloc feel_green feel_ppc feel_trudeau\n## 1       52        0       64        NA         97        0           55\n## 2       51        0       64        40         88        0           53\n## 3       52       49       NA        NA         NA       NA           74\n## 4       30       85        0         0          0       30            0\n## 5       20       62       51        20         50       20           30\n## 6       81       31       79        NA         85        0           81\n##   feel_scheer feel_singh feel_blanchet feel_may feel_bernier\n## 1           0         71            15       80            0\n## 2           0         NA            32       59            0\n## 3          24         63            NA       56           20\n## 4          80          0             0        0           10\n## 5          60         NA            50       60           30\n## 6          35         70            NA       74            3\nlibrary(dplyr)\nfeelings_dplyr <- select(ces, c(feel_lib:feel_blanchet))## select: dropped 37 variables (weight, age, agegrp, age_18to34, age_35to54, …)\nhead(feelings_dplyr)##   feel_lib feel_cpc feel_ndp feel_bloc feel_green feel_ppc feel_trudeau\n## 1       52        0       64        NA         97        0           55\n## 2       51        0       64        40         88        0           53\n## 3       52       49       NA        NA         NA       NA           74\n## 4       30       85        0         0          0       30            0\n## 5       20       62       51        20         50       20           30\n## 6       81       31       79        NA         85        0           81\n##   feel_scheer feel_singh feel_blanchet\n## 1           0         71            15\n## 2           0         NA            32\n## 3          24         63            NA\n## 4          80          0             0\n## 5          60         NA            50\n## 6          35         70            NA\nfeelings_dplyr2 <- select(ces, starts_with(\"feel_\"))## select: dropped 35 variables (weight, age, agegrp, age_18to34, age_35to54, …)\nhead(feelings_dplyr2)##   feel_lib feel_cpc feel_ndp feel_bloc feel_green feel_ppc feel_trudeau\n## 1       52        0       64        NA         97        0           55\n## 2       51        0       64        40         88        0           53\n## 3       52       49       NA        NA         NA       NA           74\n## 4       30       85        0         0          0       30            0\n## 5       20       62       51        20         50       20           30\n## 6       81       31       79        NA         85        0           81\n##   feel_scheer feel_singh feel_blanchet feel_may feel_bernier\n## 1           0         71            15       80            0\n## 2           0         NA            32       59            0\n## 3          24         63            NA       56           20\n## 4          80          0             0        0           10\n## 5          60         NA            50       60           30\n## 6          35         70            NA       74            3"},{"path":"lecture-3-exercises.html","id":"using-summarise-to-calculate-summary-statistics","chapter":"8 Lecture 3 Exercises","heading":"8.1.2 Using {summarise} to calculate summary statistics","text":"Let’s say want calculate sample mean feeling thermometer score. Base, might something like following, separate line code calculation. (, ’m lazy, ’ve done three, 12 lines total.)dplyr way efficient, thanks %>% operator, allows us write name data frame “pipe ” entire chain commands.","code":"\nmean(ces$feel_lib, na.rm = TRUE)## [1] 48.34366\nmean(ces$feel_cpc, na.rm = TRUE)## [1] 43.61424\nmean(ces$feel_ndp, na.rm = TRUE)## [1] 50.03684\nces %>%\n  select(starts_with(\"feel_\")) %>%\n  summarise(across(everything(), \n                   list(mean), \n                   na.rm = TRUE))## select: dropped 35 variables (weight, age, agegrp, age_18to34, age_35to54, …)\n## summarise: now one row and 12 columns, ungrouped##   feel_lib_1 feel_cpc_1 feel_ndp_1 feel_bloc_1 feel_green_1 feel_ppc_1\n## 1   48.34366   43.61424   50.03684    23.33321     47.45785   25.81046\n##   feel_trudeau_1 feel_scheer_1 feel_singh_1 feel_blanchet_1 feel_may_1\n## 1       44.84804      39.51979     50.36898        27.79471   46.85782\n##   feel_bernier_1\n## 1       26.14414"},{"path":"lecture-3-exercises.html","id":"the-group_by-and-summarise-combo","chapter":"8 Lecture 3 Exercises","heading":"8.1.3 The {group_by} and {summarise} combo","text":"Calculating group-wise summary statistics Base bit clunky. Recall last week, Base R uses square brackets selection subsetting. want calculate average rating PPC among Western Canadians versus everyone else, something like :dplyr way :first, might think dplyr way longer three instead two lines code. However, efficient less duplication code.dplyr really starts show advantage performan many calculations (variables, groupings, /summary statistics).want calculate dispersion distributions (SD) uncertainty associated estimated means (SE), looks like Base R:dplyr way:","code":"\nmean(ces$feel_ppc[ces$reg_west==1], na.rm = TRUE)## [1] 27.04223\nmean(ces$feel_ppc[ces$reg_west==0], na.rm = TRUE)## [1] 25.25404\nces %>%\n  group_by(reg_west) %>% \n  summarise(mean_ppc = mean(feel_ppc, na.rm = TRUE))## group_by: one grouping variable (reg_west)\n## summarise: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##   reg_west mean_ppc\n##      <dbl>    <dbl>\n## 1        0     25.3\n## 2        1     27.0\nsd(ces$feel_ppc[ces$reg_west==1], na.rm = TRUE)## [1] 26.75502\nsd(ces$feel_ppc[ces$reg_west==0], na.rm = TRUE)## [1] 26.43218\nsd(ces$feel_ppc[ces$reg_west==1], na.rm = TRUE) / \n  sqrt(sum(!is.na(ces$feel_ppc[ces$reg_west==1])))## [1] 0.2654482\nsd(ces$feel_ppc[ces$reg_west==0], na.rm = TRUE) / \n  sqrt(sum(!is.na(ces$feel_ppc[ces$reg_west==0])))## [1] 0.1762576\nces %>%\n  group_by(reg_west) %>% \n  summarise(mean_ppc = mean(feel_ppc, na.rm = TRUE),\n            sd_ppc = sd(feel_ppc, na.rm = TRUE),\n            n_ppc = sum(!is.na(feel_ppc)),\n            semean_ppc = sd_ppc / sqrt(n_ppc)\n            )## group_by: one grouping variable (reg_west)\n## summarise: now 2 rows and 5 columns, ungrouped## # A tibble: 2 × 5\n##   reg_west mean_ppc sd_ppc n_ppc semean_ppc\n##      <dbl>    <dbl>  <dbl> <int>      <dbl>\n## 1        0     25.3   26.4 22489      0.176\n## 2        1     27.0   26.8 10159      0.265"},{"path":"lecture-3-exercises.html","id":"lecture-3-exercises-1","chapter":"8 Lecture 3 Exercises","heading":"8.2 Lecture 3 Exercises","text":"Using ’fertil2’ dataset ’wooldridge’ women living Republic Botswana 1988,() Calculate means mean differences without electricity following two characteristics:education (educ)age first child born (agefbrth)’ll start loading dataset.loading dataset, let’s look summary statistics (also known descriptive statistics simply descriptives) variables ’re analyze. looking , keep mine units analysis whether missing values deal analyze data.Now can calculate summary statistics group two variables.Difference education attainment without (0) (1) electricity.Without (0) = 5.382 yearsWith (1) = 8.763 yearsDifference = 8.763 - 5.382 == 3.381 years schooling(Note: isn’t important now, average among NAs group often indicates missingness bias results. later methods courses, ’ll learn ways test deal .)Difference age first child born without (0) (1) electricity.Without (0) = 18.825 years oldWith (1) = 20.162 years oldDifference = 20.162 - 18.825 == 1.337 years old(ii) Evaluate mean differences statistically significant 0.01 0.05 levels.education, difference means = 3.381 years, p = 2.2*10^-16 (less 0.001), difference significant 0.05 0.01 levels. fact, even significant 0.001 level.Using confidence interval approach, also say 95% confidence interval difference -3.720 -3.041, include zero. indicates difference statistically significant 0.05 level.code uses “formula notation”, quantitative variable listed first, tilde (“~”), categorical variable.use formula notation incorrect order, won’t work, following case: t.test(fertil2$electric ~ fertil2$agefbrth).Specifying “vector notation” specifying two complementary subsets (using square brackets, []) also works. vector notation approach works regardless order specify two vectors, can seen example :Regardless approach use, age birth first child, difference means = 1.337 years (p = 1.432*10^-13). , difference significant 0.05 0.01 levels. fact, even significant 0.001 level.Using confidence interval approach, also say 95% confidence interval difference -1.683 -0.990, include zero. indicates difference statistically significant 0.05 level.note Stata users: default, t.test() uses unequal variances (proposed Welch). possible perform “vanilla” Student’s t-test assumes equal variances specifying option var.equal = TRUE. However, practice, reason . Welch’s t-test robust Student’s t-test assumptions test violated, performs just well (rare) cases assumptions met. compare results R Stata, t-tests usually match Stata, default, assumes equal variances. can get Stata assume unequal variances specifying option (e.g. ttest agefbrth, (electric) unequal welch).(iii) Interpret results.Women access electricity , average, 3.4 years education 1.3 years younger women access electricity. mean differences significant 0.01 0.05 levels.(iv) analysis, say comparisons women without electricity apples--apples apples--oranges?apples--oranges comparisons.(v) affect ability conclude anything relationship access electricity number children?previous exercise, found women access electricity , average, fewer children women access electricity (mean difference = 0.43 children, p<0.001). Women electricity systematically different women without electricity terms children, children later, years schooling. Presumably, children, children, years schooling also related. , sure true link access electricity number children woman without controlling factors.","code":"\nlibrary(wooldridge)\ncupcakes <- get(data('fertil2'))\nhead(cupcakes)##   mnthborn yearborn age electric radio tv bicycle educ ceb agefbrth children\n## 1        5       64  24        1     1  1       1   12   0       NA        0\n## 2        1       56  32        1     1  1       1   13   3       25        3\n## 3        7       58  30        1     0  0       0    5   1       27        1\n## 4       11       45  42        1     0  1       0    4   3       17        2\n## 5        5       45  43        1     1  1       1   11   2       24        2\n## 6        8       52  36        1     0  0       0    7   1       26        1\n##   knowmeth usemeth monthfm yearfm agefm idlnchld heduc agesq urban urb_educ spirit\n## 1        1       0      NA     NA    NA        2    NA   576     1       12      0\n## 2        1       1      11     80    24        3    12  1024     1       13      0\n## 3        1       0       6     83    24        5     7   900     1        5      1\n## 4        1       0       1     61    15        3    11  1764     1        4      0\n## 5        1       1       3     66    20        2    14  1849     1       11      0\n## 6        1       1      11     76    24        4     9  1296     1        7      0\n##   protest catholic frsthalf educ0 evermarr\n## 1       0        0        1     0        0\n## 2       0        0        1     0        1\n## 3       0        0        0     0        1\n## 4       0        0        0     0        1\n## 5       1        0        1     0        1\n## 6       0        0        0     0        1\ndata(\"fertil2\")\nhead(fertil2)##   mnthborn yearborn age electric radio tv bicycle educ ceb agefbrth children\n## 1        5       64  24        1     1  1       1   12   0       NA        0\n## 2        1       56  32        1     1  1       1   13   3       25        3\n## 3        7       58  30        1     0  0       0    5   1       27        1\n## 4       11       45  42        1     0  1       0    4   3       17        2\n## 5        5       45  43        1     1  1       1   11   2       24        2\n## 6        8       52  36        1     0  0       0    7   1       26        1\n##   knowmeth usemeth monthfm yearfm agefm idlnchld heduc agesq urban urb_educ spirit\n## 1        1       0      NA     NA    NA        2    NA   576     1       12      0\n## 2        1       1      11     80    24        3    12  1024     1       13      0\n## 3        1       0       6     83    24        5     7   900     1        5      1\n## 4        1       0       1     61    15        3    11  1764     1        4      0\n## 5        1       1       3     66    20        2    14  1849     1       11      0\n## 6        1       1      11     76    24        4     9  1296     1        7      0\n##   protest catholic frsthalf educ0 evermarr\n## 1       0        0        1     0        0\n## 2       0        0        1     0        1\n## 3       0        0        0     0        1\n## 4       0        0        0     0        1\n## 5       1        0        1     0        1\n## 6       0        0        0     0        1\nsummary(fertil2$electric)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##  0.0000  0.0000  0.0000  0.1402  0.0000  1.0000       3\nsummary(fertil2$educ)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.000   3.000   7.000   5.856   8.000  20.000\nsummary(fertil2$agefbrth)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##   10.00   17.00   19.00   19.01   20.00   38.00    1088\nlibrary(dplyr)\nfertil2 %>%\n  group_by(electric) %>%\n  summarise(mean_educ = mean(educ, na.rm = TRUE))## group_by: one grouping variable (electric)\n## summarise: now 3 rows and 2 columns, ungrouped## # A tibble: 3 × 2\n##   electric mean_educ\n##      <int>     <dbl>\n## 1        0      5.38\n## 2        1      8.76\n## 3       NA      5.67\nfertil2 %>%\n  group_by(electric) %>%\n  summarise(mean_agefbrth = mean(agefbrth, na.rm = TRUE))## group_by: one grouping variable (electric)\n## summarise: now 3 rows and 2 columns, ungrouped## # A tibble: 3 × 2\n##   electric mean_agefbrth\n##      <int>         <dbl>\n## 1        0          18.8\n## 2        1          20.2\n## 3       NA          18\nt.test(fertil2$educ ~ fertil2$electric)## \n##  Welch Two Sample t-test\n## \n## data:  fertil2$educ by fertil2$electric\n## t = -19.569, df = 790.18, p-value < 2.2e-16\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -3.719608 -3.041416\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        5.382172        8.762684\nt.test(fertil2$agefbrth ~ fertil2$electric)## \n##  Welch Two Sample t-test\n## \n## data:  fertil2$agefbrth by fertil2$electric\n## t = -7.5801, df = 562.66, p-value = 1.432e-13\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -1.6827861 -0.9901586\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        18.82545        20.16193\nt.test(fertil2$agefbrth[fertil2$electric==1],fertil2$agefbrth[fertil2$electric==0])## \n##  Welch Two Sample t-test\n## \n## data:  fertil2$agefbrth[fertil2$electric == 1] and fertil2$agefbrth[fertil2$electric == 0]\n## t = 7.5801, df = 562.66, p-value = 1.432e-13\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  0.9901586 1.6827861\n## sample estimates:\n## mean of x mean of y \n##  20.16193  18.82545\nt.test(fertil2$agefbrth[fertil2$electric==0],fertil2$agefbrth[fertil2$electric==1])## \n##  Welch Two Sample t-test\n## \n## data:  fertil2$agefbrth[fertil2$electric == 0] and fertil2$agefbrth[fertil2$electric == 1]\n## t = -7.5801, df = 562.66, p-value = 1.432e-13\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -1.6827861 -0.9901586\n## sample estimates:\n## mean of x mean of y \n##  18.82545  20.16193\nfertil2 %>%\n  select(c(electric, educ, agefbrth)) %>%\n  filter(!is.na(electric)) %>%\n  group_by(electric) %>%\n  summarise(mean_agefbrth = mean(agefbrth, na.rm = TRUE),\n            mean_educ = mean(educ, na.rm = TRUE)) %>%\n  mutate(electric = factor(electric,\n                           levels = c(0,1),\n                           labels = c(\"No electricity\",\n                                      \"Electricity\")))## select: dropped 24 variables (mnthborn, yearborn, age, radio, tv, …)\n## filter: removed 3 rows (<1%), 4,358 rows remaining\n## group_by: one grouping variable (electric)\n## summarise: now 2 rows and 3 columns, ungrouped\n## mutate: converted 'electric' from integer to factor (0 new NA)## # A tibble: 2 × 3\n##   electric       mean_agefbrth mean_educ\n##   <fct>                  <dbl>     <dbl>\n## 1 No electricity          18.8      5.38\n## 2 Electricity             20.2      8.76"},{"path":"lecture-3-exercises.html","id":"additional-exercises-1","chapter":"8 Lecture 3 Exercises","heading":"8.3 Additional Exercises:","text":"Using ‘ces’ dataset, calculate means mean differences support market liberalism (marketlib) across whether someone lives Western Canada (reg_west) men women (gender);evaluate mean differences statistically significant 0.01 0.05 levels;interpret results way practice exercises lecture;","code":""},{"path":"lecture-3-exercises.html","id":"stop-1","chapter":"8 Lecture 3 Exercises","heading":"8.3.1 STOP!!","text":"continue, try solving exercises . ’s way learn. , come back page see well .","code":""},{"path":"lecture-3-exercises.html","id":"continue-1","chapter":"8 Lecture 3 Exercises","heading":"8.3.2 Continue","text":"’ll start loading dataset taking look univariate summaries variable ’ll analyzing.Market liberalism measured scale 0 16 points, average 8.59.Region West gender binary variables.NAs marketlib gender.","code":"\nload(\"Sample_data/ces.rda\")\nsummary(ces$marketlib)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00    6.00    9.00    8.59   11.00   16.00   32684\nsummary(ces$reg_west)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  0.0000  0.0000  0.0000  0.3145  1.0000  1.0000\nsummary(ces$gender)##   Man Woman  NA's \n## 15517 21928   288"},{"path":"lecture-3-exercises.html","id":"group-wise-means-mean-differences-and-statistical-significance","chapter":"8 Lecture 3 Exercises","heading":"8.3.3 Group-wise means, mean differences, and statistical significance","text":"’m going use t.test() get significance testing. dplyr version follows, want see .Among live Western Canada, average market liberalism score 8.748. Among live Western Canada, score 8.498. means , average, living Western Canada 0.25 points 16 supportive market liberalism living elsewhere country. difference significant 0.05 level (p=0.018).Among men, average market liberalism score 9.079. Among women, score 8.156. means , average, men living Western Canada 0.923 points 16 supportive market liberalism women. difference significant 0.001 level (p=<0.001).dplyr version:","code":"\nt.test(ces$marketlib ~ ces$reg_west)## \n##  Welch Two Sample t-test\n## \n## data:  ces$marketlib by ces$reg_west\n## t = -2.3712, df = 3428.5, p-value = 0.01779\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -0.4571754 -0.0433279\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        8.498180        8.748431\nt.test(ces$marketlib ~ ces$gender)## \n##  Welch Two Sample t-test\n## \n## data:  ces$marketlib by ces$gender\n## t = 9.4086, df = 5012.2, p-value < 2.2e-16\n## alternative hypothesis: true difference in means between group Man and group Woman is not equal to 0\n## 95 percent confidence interval:\n##  0.7309037 1.1156689\n## sample estimates:\n##   mean in group Man mean in group Woman \n##            9.079404            8.156118\nces %>%\n  group_by(reg_west) %>% \n  summarise(mean_marketlib = mean(marketlib, na.rm = TRUE))## group_by: one grouping variable (reg_west)\n## summarise: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##   reg_west mean_marketlib\n##      <dbl>          <dbl>\n## 1        0           8.50\n## 2        1           8.75\nces %>%\n  group_by(gender) %>% \n  summarise(mean_marketlib = mean(marketlib, na.rm = TRUE))## group_by: one grouping variable (gender)\n## summarise: now 3 rows and 2 columns, ungrouped## # A tibble: 3 × 2\n##   gender mean_marketlib\n##   <fct>           <dbl>\n## 1 Man              9.08\n## 2 Woman            8.16\n## 3 <NA>             5.38"},{"path":"lecture-3-exercises.html","id":"interpretation-of-results","chapter":"8 Lecture 3 Exercises","heading":"8.3.4 Interpretation of results","text":"Among sets comparisons statistically significant differences support principle market liberalism. However, gender region, necessarily conclude ’ve solved reason Westerners supportive free market Canadians men supportive free market women. Moreover, region, magnitude difference quarter point 16 scale, dubious substantive significance.variables usually referred demographic characteristics, personal attributes individual. tend “far back” processes lead outcomes interest political science (like policy preferences vote choice) often exert effect intervening variables, things come -personal characteristics outcomes.","code":""},{"path":"lecture-4-the-simple-regression-model-i.html","id":"lecture-4-the-simple-regression-model-i","chapter":"9 Lecture 4: The Simple Regression Model I","heading":"9 Lecture 4: The Simple Regression Model I","text":"","code":""},{"path":"lecture-4-the-simple-regression-model-i.html","id":"slides-3","chapter":"9 Lecture 4: The Simple Regression Model I","heading":"Slides","text":"5 Simple Regression Model (link)","code":""},{"path":"lecture-4-the-simple-regression-model-i.html","id":"introduction-3","chapter":"9 Lecture 4: The Simple Regression Model I","heading":"9.1 Introduction","text":"previous lecture, explored can understand causal inference importance random assignment. Experiments allow us randomize treatment create believable counterfactuals. solely rely experiments estimate causal relations. Thus, study regression…\nlecture slide displayed full :\nFigure 9.1: Slides 4 Simple Regression Model .\n","code":""},{"path":"tutorial-4-the-simple-regression-model-i.html","id":"tutorial-4-the-simple-regression-model-i","chapter":"10 Tutorial 4: The Simple Regression Model I","heading":"10 Tutorial 4: The Simple Regression Model I","text":"","code":""},{"path":"tutorial-4-the-simple-regression-model-i.html","id":"what-is-this-tutorial-about-2","chapter":"10 Tutorial 4: The Simple Regression Model I","heading":"What is this tutorial about?","text":"can access tutorial clicking . tutorial test understanding Simple Regression Model (SRM), building concepts Wooldridge’s Introductory Econometrics Huntington-Klein’s Effect. ’ll engage theoretical foundations practical applications simple regression analysis related measures relationship variables.","code":""},{"path":"lecture-4-exercises.html","id":"lecture-4-exercises","chapter":"11 Lecture 4 Exercises","heading":"11 Lecture 4 Exercises","text":"tutorial created John Santos (minor adaptations ).","code":""},{"path":"lecture-4-exercises.html","id":"practice-exercises","chapter":"11 Lecture 4 Exercises","heading":"11.1 Practice exercises","text":"lecture:Using “fertil2” dataset “wooldridge” women living Republic Botswana 1988,produce scatterplot number children (children) y axis education (educ) x axis;two variables appear related?;estimate regression equation number children education (note: say regress y x);interpret \\(\\hat{\\beta_0}\\) \\(\\hat{\\beta_1}\\) ;plot regression line scatterplot;calculate \\(SST\\), \\(SSE\\) \\(SSR\\). Verify \\(SST = SSE + SSR\\);using \\(SST\\) , \\(SSE\\) \\(SSR\\), calculate R 2 regression. Verify R 2 reported summary regression output;interpret \\(R^2\\) regression.tutorial:Using sample 2019 CES dataset provided John, look feeling thermometer ratings Justin Trudeau (feel_trudeau) vary individual’s support free market principles (marketlib).Produce scatterplot two variables regression line visualized;Describe relationship (.e. positive negative);Run regression report coefficients mean;Calculate predicted rating Justin Trudeau following levels market liberalism: lower tertile (33rd percentile), mean, upper quartile (75 percentile), maximum;Explain well market liberalism explains someone rates Justin Trudeau; andExplain might u term model.","code":""},{"path":"lecture-4-exercises.html","id":"writing-math-in-rstudio","chapter":"11 Lecture 4 Exercises","heading":"11.2 Writing math in RStudio","text":"write math RStudio enclosing equations within dollar signs ($).\\(y = mx + b\\)can centre equation new line enclosing within two dollar signs.\\[y = mx + b\\]Subscript denoted underscore (_) superscript caret (^).\\[y = B_0 + B_1x_1 + u\\]Special symbols (greek letters, hats, etc.) prefaced backslash (\\) sometimes enclosed curly braces ({}).\\[ y = \\beta_0 + \\beta_1x_1 + u\\]\\[ \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta}_1x_1\\]","code":""},{"path":"lecture-4-exercises.html","id":"stop-2","chapter":"11 Lecture 4 Exercises","heading":"11.3 STOP!!","text":"continue, try solving exercises . ’s way learn. , come back page see well .","code":""},{"path":"lecture-4-exercises.html","id":"continue-2","chapter":"11 Lecture 4 Exercises","heading":"11.4 Continue","text":"","code":""},{"path":"lecture-4-exercises.html","id":"lecture-exercises-solutions","chapter":"11 Lecture 4 Exercises","heading":"11.5 Lecture exercises (Solutions)","text":"","code":"\nlibrary(wooldridge)\ndata('fertil2')"},{"path":"lecture-4-exercises.html","id":"q1.-produce-a-scatterplot-with-number-of-children-children-on-the-y-axis-and-education-educ-on-the-x-axis","chapter":"11 Lecture 4 Exercises","heading":"11.5.1 Q1. Produce a scatterplot with number of children (children) on the y axis and education (educ) on the \\(x\\) axis","text":", use ggplot2 make scatterplot. also use Base R graphics, find ggplot2 easier (especially ’s party tidyverse follows conventions).Note, ggplot2, can command ggplot() command span several lines, connected using + sign. makes facilitates building graph layer--layer, easier, readable, easier troubleshoot.add random noise (.k.. jitter), include argument position = \"jitter\" within geom_point() function call.","code":"\nlibrary(ggplot2)\nggplot(fertil2) +\n  aes(x = educ, y = children) +\n  geom_point() + \n  labs(x = \"Education (years)\", \n       y = \"Number of children\",\n       title = \"Number of children by years of education\")\nggplot(fertil2) +\n  aes(x = educ, y = children) +\n  geom_point(position = \"jitter\") + \n  labs(x = \"Education (years)\", \n       y = \"Number of children\",\n       title = \"Number of children by years of education (with jitter)\")"},{"path":"lecture-4-exercises.html","id":"q2.-how-do-the-two-variables-appear-to-be-related","chapter":"11 Lecture 4 Exercises","heading":"11.5.2 Q2. How do the two variables appear to be related?;","text":"appears negative relationship amount education number children. number years school attended increases, number children women tends decrease.","code":""},{"path":"lecture-4-exercises.html","id":"q3.-estimate-the-regression-equation-of-the-number-of-children-on-education-note-we-say-to-regress-y-on-x","chapter":"11 Lecture 4 Exercises","heading":"11.5.3 Q3. Estimate the regression equation of the number of children on education (note: we say “to regress \\(y\\) on \\(x\\)”);","text":"Remember, equation population model :\\[children = \\beta_0 + \\beta_1educ + \\mu \\]regression equation :\\[\\hat{children} = 3.496 - .210educ\\]\\[n = 4361, R^2 = 0.137\\]","code":"\nlibrary(dplyr)\ndat <- fertil2 %>%\n  select(c(children, educ)) %>%\n  na.omit()## select: dropped 25 variables (mnthborn, yearborn, age, electric, radio, …)\nmodel1 <- lm(children ~ educ, data = dat)\nsummary(model1)## \n## Call:\n## lm(formula = children ~ educ, data = dat)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.495 -1.496 -0.399  1.182  9.505 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.49554    0.05612   62.28   <2e-16 ***\n## educ        -0.20965    0.00796  -26.34   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.064 on 4359 degrees of freedom\n## Multiple R-squared:  0.1373, Adjusted R-squared:  0.1371 \n## F-statistic: 693.7 on 1 and 4359 DF,  p-value: < 2.2e-16"},{"path":"lecture-4-exercises.html","id":"q4.-interpret-hatbeta_0-and-hatbeta_1","chapter":"11 Lecture 4 Exercises","heading":"11.5.4 Q4. Interpret \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\);","text":"\\(\\hat{\\beta_0} = 3.5\\). intercept (constant), value \\(\\hat{y}\\) coefficients equal 0. means woman education predicted 3.5 children.\\(\\hat{\\beta_1} = -0.2\\). coefficient educ. means , average, every additional year schooling women , predict 0.2 fewer children.","code":""},{"path":"lecture-4-exercises.html","id":"q5.-plot-the-regression-line-on-the-scatterplot","chapter":"11 Lecture 4 Exercises","heading":"11.5.5 Q5. Plot the regression line on the scatterplot;","text":"ways . First, generate predictions, save data set, plot .Running lm() function automatically calculates predictions (also know fitted values y-hats), can just extract model object. stored slot $fitted.values., take fitted values store new column called “yhat.”Another way retrieve fitted values command fitted.values(). produces exactly results $fitted.values, can seen .plot just fitted values x-axis, ’d get graph looks like :predict() function. function used calculate predicted values, given combination values input \\(x\\)’s model.Theoretically, use get y-hats specifying new values. However, mechanics function works, doesn’t produce exactly values using $fitted.values fitted.values() (though comes close).also calculate predictions manually using coefficients. practice, wouldn’t actually ; just show math R behind--scenes.Finally, avoid make calculations entering numbers manually (.e. retrieving model object). put rounding numbers end, lest throw subsequent calculations.example, numbers aren’t different (see 4th column ). , doesn’t mean ’ll always able get away .Regardless calculate predictions, can add original ggplot() statement geom_line().simple (bivariate) regression, simplest way add regression line make use command geom_smooth(), built ggplot2.example, ’ve also added addition geom_point() command aesthetics mapped mean values predictor response variables. show point coordinates \\([\\bar{educ},\\bar{children}]\\) (.e. \\([5.9, 2.3]\\) always regression line.","code":"\ndat$yhat <- model1$fitted.values\ndat$yhat2 <- fitted.values(model1)\ndat$yhat[1:10] == dat$yhat2[1:10]##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nlibrary(ggplot2)\nggplot(dat) +\n  aes(x = educ, y = yhat) +\n  geom_point() + \n  labs(x = \"Education (years)\", \n       y = \"Predicted number of children\",\n       title = \"Predicted number of children by years of education\")\ndat$yhat3 <- predict(model1)\ndat$yhat[1:10] == dat$yhat3[1:10]##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\ndat$calcpreds <- model1$coefficients[[1]] + (model1$coefficients[[2]] * dat$educ)\ntibble(\n  \"fitted.values()\" = dat$yhat,\n  \"predict()\" = dat$yhat3,\n  \"Manual calculation\" = dat$calcpreds) %>% \n  head()## # A tibble: 6 × 3\n##   `fitted.values()` `predict()` `Manual calculation`\n##               <dbl>       <dbl>                <dbl>\n## 1             0.980       0.980                0.980\n## 2             0.770       0.770                0.770\n## 3             2.45        2.45                 2.45 \n## 4             2.66        2.66                 2.66 \n## 5             1.19        1.19                 1.19 \n## 6             2.03        2.03                 2.03\ndat$roundpreds <- 3.496 + (-0.210 * dat$educ)\ntibble(\n  \"fitted.values()\" = dat$yhat,\n  \"predict()\" = dat$yhat3,\n  \"Manual calculation\" = dat$calcpreds,\n  \"Rounded calculation\" = dat$roundpreds) %>% \n  head()## # A tibble: 6 × 4\n##   `fitted.values()` `predict()` `Manual calculation` `Rounded calculation`\n##               <dbl>       <dbl>                <dbl>                 <dbl>\n## 1             0.980       0.980                0.980                 0.976\n## 2             0.770       0.770                0.770                 0.766\n## 3             2.45        2.45                 2.45                  2.45 \n## 4             2.66        2.66                 2.66                  2.66 \n## 5             1.19        1.19                 1.19                  1.19 \n## 6             2.03        2.03                 2.03                  2.03\nggplot(dat) +\n  geom_point(aes(x = educ, y = children), position = \"jitter\") + \n  geom_line(aes(x = educ, y = yhat), color = \"red\", linewidth = 1) +\n  labs(x = \"Education (years)\", \n       y = \"Number of children\",\n       title = \"Number of children by years of education\")\nggplot(dat, aes(x = educ, y = children)) +\n  geom_point(position = \"jitter\") + \n  geom_smooth(method = \"lm\", fullrange = FALSE) +\n  labs(x = \"Education (years)\", \n       y = \"Number of children\",\n       title = \"Number of children by years of education\") +\n  geom_point(aes(x = mean(dat$educ), y = mean(dat$children)), color = \"red\", size = 4)## `geom_smooth()` using formula = 'y ~ x'"},{"path":"lecture-4-exercises.html","id":"q6.-calculate-sst-sse-and-ssr.-verify-that-sst-sse-ssr","chapter":"11 Lecture 4 Exercises","heading":"11.5.6 Q6. Calculate SST, SSE and SSR. Verify that \\(SST = SSE + SSR\\);","text":"First, ’ll generate predictions.Calculate \\(SST\\) (sum squares TOTAL).formula, words, interpreted , total sum squares equal sum squared differences observed Y average Y.\\[SST = \\Sigma{ ^n_{=1} ( y_i - \\bar{y} )^2 }\\]Calculate \\(SSE\\) (sum squares EXPLAINED).words, formula interpreted , explained sum squares equal sum squared differences predicted Y average Y.\\[SSE = \\Sigma{ ^n_{=1} ( \\hat{y}_i - \\bar{y} )^2 }\\]Calculate \\(SSR\\) (sum squares RESIDUAL).\\[SST = \\Sigma{ ^n_{=1} \\hat{u_i}^2 }\\]model’s residuals stored slot $residuals, can use inputs calculation. can also retrieve using resid() function.manual calculations mostly pedagogical purposes. practice, pre-built functions calculate quantities us.can use anova() function display ANOVA table regression, shows various sum--squares statistics.Note total sum squares reported ANOVA table, easily calculated.","code":"\ndat$childrenhat <- model1$fitted.values\nmodel1_sst <- sum( (dat$children - mean(dat$children)) ^ 2 )\nmodel1_sst## [1] 21527.18\nmodel1_sse <- sum( (dat$yhat - mean(dat$children)) ^ 2 )\nmodel1_sse## [1] 2955.401\nmodel1_ssr <- model1_sst - model1_sse\nmodel1_ssr## [1] 18571.78\nsum(model1$residuals^2)## [1] 18571.78\nsum(resid(model1)^2)## [1] 18571.78\nanova(model1)## Analysis of Variance Table\n## \n## Response: children\n##             Df  Sum Sq Mean Sq F value    Pr(>F)    \n## educ         1  2955.4 2955.40  693.67 < 2.2e-16 ***\n## Residuals 4359 18571.8    4.26                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(model1)[1,2] + anova(model1)[2,2]## [1] 21527.18"},{"path":"lecture-4-exercises.html","id":"q7.-using-sst-sse-and-ssr-calculate-the-r2-of-the-regression.-verify-it-is-the-same-as-the-r2-reported-in-the-summary-of-your-regression-output","chapter":"11 Lecture 4 Exercises","heading":"11.5.7 Q7. Using \\(SST\\), \\(SSE\\) and \\(SSR\\), calculate the \\(R^2\\) of the regression. Verify it is the same as the \\(R^2\\) reported in the summary of your regression output;","text":"proportion variance Y explained X simple ratio \\(SSE/SST\\).can check model reports…Note residual standard error 2.064 4359 degrees freedom. equal standard deviation residuals (\\({\\sqrt{\\hat{\\sigma^2}}}\\)) (makes sense use model’s SDs approximations population’s SEs).","code":"\nmodel1_sse / model1_sst## [1] 0.137287\nsummary(model1)## \n## Call:\n## lm(formula = children ~ educ, data = dat)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.495 -1.496 -0.399  1.182  9.505 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.49554    0.05612   62.28   <2e-16 ***\n## educ        -0.20965    0.00796  -26.34   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.064 on 4359 degrees of freedom\n## Multiple R-squared:  0.1373, Adjusted R-squared:  0.1371 \n## F-statistic: 693.7 on 1 and 4359 DF,  p-value: < 2.2e-16\nsd(model1$residuals)## [1] 2.063875"},{"path":"lecture-4-exercises.html","id":"q8.-interpret-the-r2-of-the-regression.","chapter":"11 Lecture 4 Exercises","heading":"11.5.8 Q8. Interpret the R^2 of the regression.","text":"\\(R^2 = 0.137\\). means education explains 14% variation number children.","code":""},{"path":"lecture-4-exercises.html","id":"additional-practice-exercises","chapter":"11 Lecture 4 Exercises","heading":"11.6 Additional practice exercises","text":"Using 2019 CES dataset, look feeling thermometer ratings Justin Trudeau (feel_trudeau) vary individual’s support free market principles (marketlib).analysis…Produce scatterplot two variables regression line visualized;Describe relationship (.e. positive negative);Run regression report coefficients mean;Calculate predicted rating Justin Trudeau following levels market liberalism: lower tertile (33rd percentile), mean, upper quartile (75 percentile), maximum;Explain well market liberalism explains someone rates Justin Trudeau; andExplain might \\(\\mu\\) term model.","code":""},{"path":"lecture-4-exercises.html","id":"stop-3","chapter":"11 Lecture 4 Exercises","heading":"11.7 STOP!!","text":"continue, try solving exercises . ’s way learn. , come back page see well .","code":""},{"path":"lecture-4-exercises.html","id":"continue-3","chapter":"11 Lecture 4 Exercises","heading":"11.8 Continue","text":"answer questions, ’m going start looking summary statistics variables.average, Canadians rate Trudeau 45 points 100 feeling thermometer scale (SD = 34.5), score 8.6 market liberalism scale (SD = 3.5), ranges 0 16.","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nload(\"Sample_data/ces.rda\")\nsummary(ces$feel_trudeau)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00    7.00   50.00   44.85   75.00  100.00    2409\nsd(ces$feel_trudeau, na.rm = TRUE)## [1] 34.54668\nsummary(ces$marketlib)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00    6.00    9.00    8.59   11.00   16.00   32684\nsd(ces$marketlib, na.rm = TRUE)## [1] 3.519915"},{"path":"lecture-4-exercises.html","id":"q1.-produce-a-scatterplot-of-the-two-variables-with-a-regression-line-visualized","chapter":"11 Lecture 4 Exercises","heading":"11.8.1 Q1. Produce a scatterplot of the two variables with a regression line visualized","text":"","code":"\nggplot(ces, aes(x = marketlib, y = feel_trudeau)) +\n  geom_point(position = \"jitter\") + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Market liberalism\", \n       y = \"Feeling thermometer ratings of Trudeau\",\n       title = \"Trudeau ratings by market liberalism\") +\n  theme_minimal()## `geom_smooth()` using formula = 'y ~ x'"},{"path":"lecture-4-exercises.html","id":"q2.-describe-the-relationship-i.e.-is-it-positive-or-negative","chapter":"11 Lecture 4 Exercises","heading":"11.8.2 Q2. Describe the relationship (i.e. is it positive or negative)","text":"appears negative relationship market liberalism ratings Trudeau–.e. support free market principles increases, positive feelings towards Trudeau decrease.can difficult see without regression line variables upper lower bounds (.e. go onto infinity).situations, look diagonal pairs corners denser opposite diagonal. , top left bottom right corners denser bottom left top right corners.","code":""},{"path":"lecture-4-exercises.html","id":"q3.-run-the-regression-and-report-the-coefficients-and-what-they-mean","chapter":"11 Lecture 4 Exercises","heading":"11.8.3 Q3. Run the regression and report the coefficients and what they mean","text":"Regressing feel_trudeau marketlib gives us following regression equation:\\[\\hat{feel\\_trudeau} = 68.399 + -2.852marketlib\\]\n\\[n = 4797, R^2 = 0.085\\]results indicate , every one-point increase market liberalism (scale 0 16), rating Trudeau decreases, average, 2.9 points. Moving halfway (8 points) across scale (e.g. difference far left centre centre far right) result shift 23 points average (2.9 * 8 = 23.2).intercept tells us average rating Justin Trudeau market liberalism 0 (.e. left-position) 68.399.","code":"\njtmod <- lm(feel_trudeau ~ marketlib, data = ces)\nsummary(jtmod)## \n## Call:\n## lm(formula = feel_trudeau ~ marketlib, data = ces)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -68.399 -31.323   0.121  28.677  74.382 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  68.3989     1.2546   54.52   <2e-16 ***\n## marketlib    -2.8520     0.1352  -21.10   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 33.11 on 4795 degrees of freedom\n##   (32936 observations deleted due to missingness)\n## Multiple R-squared:  0.08496,    Adjusted R-squared:  0.08477 \n## F-statistic: 445.2 on 1 and 4795 DF,  p-value: < 2.2e-16"},{"path":"lecture-4-exercises.html","id":"q4.-calculate-the-predicted-rating-for-justin-trudeau-at-the-following-levels-of-market-liberalism-the-lower-tertile-33rd-percentile-the-mean-the-upper-quartile-75-percentile-and-the-maximum","chapter":"11 Lecture 4 Exercises","heading":"11.8.4 Q4. Calculate the predicted rating for Justin Trudeau at the following levels of market liberalism: the lower tertile (33rd percentile), the mean, the upper quartile (75 percentile), and the maximum","text":"First, let’s find ratings Trudeau quantiles interest.quantile() function gives us values 0th (min), 25th (lower quartile), 50th (.e. median), 75th (upper quartile), 100th (max) percentiles. Note, data set missing values, use option na.rm = TRUE, otherwise function work.can specify specific value quantile() function return get 33rd 75th percentile.ways can calculate predicted probabilities.First, math manually plug value market liberalism regression equation. Let’s 33rd percentile market liberalism (\\(marketlib=7\\)).avoid rounding errors, ’s better use actual coefficients returned model, can retrieve using coefs() $coefficients. ’s looks like example .Finally, can also use predict() function specify values coefficient using argument newdata=. ’s example format.Let’s continue using predict() function calculate predicted values marketlib mean (8.59)., predict feel_trudeau marketlib 75th percentile.predict() max value marketlib.","code":"\nquantile(ces$marketlib, na.rm = TRUE)##   0%  25%  50%  75% 100% \n##    0    6    9   11   16\ntibble(\n  \"qtile33\" = quantile(ces$marketlib, .33, na.rm = TRUE),\n  \"mean\" = mean(ces$marketlib, na.rm = TRUE),\n  \"qtile75\" = quantile(ces$marketlib, .75, na.rm = TRUE),\n  \"max\" = max(ces$marketlib, na.rm = TRUE))## # A tibble: 1 × 4\n##   qtile33  mean qtile75   max\n##     <dbl> <dbl>   <dbl> <dbl>\n## 1       7  8.59      11    16\n68.399 + (-2.852 * 7)## [1] 48.435\njtmod$coefficients[[1]] + \n  (jtmod$coefficients[[2]] *  quantile(ces$marketlib, .33, na.rm = TRUE))##      33% \n## 48.43472\npredict(jtmod, \n        newdata = data.frame(\n          marketlib = quantile(ces$marketlib, .33, na.rm = TRUE)\n          )\n        ) %>% \n  unname()## [1] 48.43472\npredict(jtmod, \n        newdata = data.frame(marketlib = mean(ces$marketlib, na.rm = TRUE))) %>% \n  unname()## [1] 43.91406\npredict(jtmod, \n        newdata = data.frame((marketlib = quantile(ces$marketlib, .75, na.rm = TRUE)))) %>% \n  unname()## [1] 37.0266\npredict(jtmod, \n        newdata = data.frame((marketlib = max(ces$marketlib, na.rm = TRUE)))) %>% \n  unname()## [1] 22.76645"},{"path":"lecture-4-exercises.html","id":"q5.-explain-how-well-market-liberalism-explains-ratings-of-justin-trudeau","chapter":"11 Lecture 4 Exercises","heading":"11.8.5 Q5. Explain how well market liberalism explains ratings of Justin Trudeau","text":"simple regression model, \\(R^2=0.085\\). means market liberalism explains 8.5% variation feeling thermometer ratings Justin Trudeau. Another way think knowing someone’s score market liberalism scale improves ability predict 8.5% guess based knowing average market liberalism Canadians whole.doesn’t seem surprising, given expect self-reported ideology relationship ratings party leader, many factors.","code":""},{"path":"lecture-4-exercises.html","id":"q6.-explain-what-might-be-in-the-u-term-of-the-model","chapter":"11 Lecture 4 Exercises","heading":"11.8.6 Q6. Explain what might be in the u term of the model","text":"Obviously, many things besides market liberalism go someone rates Justin Trudeau. factors likely correlated market liberalism ratings Justin Trudeau—, , likely source omitted variable bias—include one’s stand social moral issues (often called moral traditionalism), one’s party identification, evaluations economic conditions, personal characteristics (often called “socio-demographics”).","code":""},{"path":"lecture-4-exercises.html","id":"bonus-content","chapter":"11 Lecture 4 Exercises","heading":"11.9 Bonus content","text":"","code":""},{"path":"lecture-4-exercises.html","id":"looking-inside-model-objects","chapter":"11 Lecture 4 Exercises","heading":"11.9.1 Looking inside model objects…","text":"Model objects created lm() list contain various quantities interest. Let’s take look inside. can using command View(model1) (note capital “V” “View”).can also look names item model1 object using command names(model1).Notice can see coefficients, ’re missing things like standard errors, t-statistics, p-values. ’s actually relatively easy calculate math variance-covariance matrix, ’re point yet.easiest way get quantities getting model summary table.’re used printing using summary() command. However, can also assign output command object, get values want item $coefficients.Note, assign mysummary$coefficients object, ’ll find ’s actually array numbers, makes bit tricky extract ’s contents.can get around telling R treat object like ’s data frame using .data.frame() command.Now can work like data frames extract columns using $ operator.Note, space special characters columns, need use backticks (.e. ` character, “lower-case tilde”) around variable names.","code":"\n# View(model1)\nnames(model1)##  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n##  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n##  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\nmysummary <- summary(model1)\nnames(mysummary)##  [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n##  [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n##  [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"\nmysummary$coefficients##               Estimate  Std. Error   t value      Pr(>|t|)\n## (Intercept)  3.4955406 0.056123844  62.28263  0.000000e+00\n## educ        -0.2096504 0.007960142 -26.33752 5.413817e-142\nmycoefs <- mysummary$coefficients\nmycoefs##               Estimate  Std. Error   t value      Pr(>|t|)\n## (Intercept)  3.4955406 0.056123844  62.28263  0.000000e+00\n## educ        -0.2096504 0.007960142 -26.33752 5.413817e-142\nclass(mycoefs)## [1] \"matrix\" \"array\"\nmycoefs1 <- as.data.frame(mysummary$coefficients)\nclass(mycoefs1)## [1] \"data.frame\"\nmycoefs1##               Estimate  Std. Error   t value      Pr(>|t|)\n## (Intercept)  3.4955406 0.056123844  62.28263  0.000000e+00\n## educ        -0.2096504 0.007960142 -26.33752 5.413817e-142\nmycoefs1$Estimate## [1]  3.4955406 -0.2096504\nmycoefs1$`Std. Error`## [1] 0.056123844 0.007960142"},{"path":"lecture-4-exercises.html","id":"how-se-t-and-p-are-calculated","chapter":"11 Lecture 4 Exercises","heading":"11.9.2 How SE, t, and p are calculated","text":"’re curious calculate Standard Errors, t-values, p-values coefficients, chunk shows equations.SEs square root diagonal variance-covariance matrix (won’t know course).t-values (t-statistics) quotients coefficients divided SEs (.e. \\(\\frac{b_i}{SE_i}\\)).p-values integrals t-distribution evaluated absolute values t-values, given residual degrees freedom model (course, ’ll using statistical table, knowing R comes handy).","code":"\ndata.frame(\n  b = model1$coefficients,\n  se = sqrt(diag(vcov(model1))),\n  t = model1$coefficients / sqrt(diag(vcov(model1))),\n  p = 2 * pt(abs(model1$coefficients / sqrt(diag(vcov(model1)))), \n             model1$df.residual, lower.tail = FALSE))##                      b          se         t             p\n## (Intercept)  3.4955406 0.056123844  62.28263  0.000000e+00\n## educ        -0.2096504 0.007960142 -26.33752 5.413817e-142"},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"lecture-5-the-simple-regression-model-ii","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12 Lecture 5: The Simple Regression Model II","text":"","code":""},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"slides-4","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"Slides","text":"5 Simple Regression Model (link)","code":""},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"introduction-4","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12.1 Introduction","text":"continue studying simple regression model.\nFigure 12.1: Slides 4 Simple Regression Model II.\n","code":""},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"vignette-5.1","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12.2 Vignette 5.1","text":"Let’s create data:","code":"\n## This is my population regression:\ndf <- tibble(x=rnorm(1000000)) %>%\n  mutate(y = 1 + 2*x + rnorm(1000000))## mutate: new variable 'y' (double) with 1,000,000 unique values and 0% NA\ndf %>%\n  ggplot(aes(x=x,y=y)) +\n  geom_point(color=\"royalblue3\",alpha = .05) + \n  theme_minimal() \nmodel_pop <- lm(y~x,data = df)\nsummary(model_pop)## \n## Call:\n## lm(formula = y ~ x, data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.0930 -0.6738 -0.0003  0.6733  5.0326 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.9996     0.0010   999.6   <2e-16 ***\n## x             1.9996     0.0010  1999.6   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1 on 999998 degrees of freedom\n## Multiple R-squared:  0.7999, Adjusted R-squared:  0.7999 \n## F-statistic: 3.998e+06 on 1 and 999998 DF,  p-value: < 2.2e-16"},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"theorem-slr.1-unbiasedness-of-ols","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12.2.1 Theorem SLR.1: Unbiasedness of OLS:","text":"Let’s say take n RANDOM samples size N population:Let’s see mean betas:","code":"\nn_samples <- 200\nN_size <- 1000\n\nbeta_coefs <- NULL\n\nfor(i in 1:n_samples){\n  df_sample <- df %>%\n    slice_sample(n=N_size)\n  model_sam <- lm(y~x,data = df_sample)\n  \n  beta_coefs <- cbind(beta_coefs,coef(model_sam)[2])\n}## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\n## slice_sample: removed 999,000 rows (>99%), 1,000 rows remaining\nmean(beta_coefs)## [1] 2.00274\nhist(beta_coefs)"},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"vignette-5.2","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12.3 Vignette 5.2","text":"","code":""},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"a.-increased-variation-in-mu","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12.3.1 a. Increased variation in \\(\\mu\\)","text":"","code":"\n## This is my population regression a:\ndf_a <- tibble(x=rnorm(100000)) %>%\n  mutate(y = 1 + 2*x + rnorm(100000,sd=1))## mutate: new variable 'y' (double) with 100,000 unique values and 0% NA\n### Let's say that I take n RANDOM samples of size N from the population:\nn_samples <- 200\nN_size <- 1000\n\nbeta_coefs_a <- NULL\n\nfor(i in 1:n_samples){\n  df_sample <- df_a %>%\n    slice_sample(n=N_size)\n  model_sam <- lm(y~x,data = df_sample)\n  \n  beta_coefs_a <- rbind.data.frame(beta_coefs_a,coef(model_sam)[2])\n}## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\ncolnames(beta_coefs_a) <- \"betas\"\nbeta_coefs_a$model <- \"df_a\"\n\n## This is my population regression b:\ndf_b <- tibble(x=rnorm(100000)) %>%\n  mutate(y = 1 + 2*x + rnorm(100000,sd=5))## mutate: new variable 'y' (double) with 100,000 unique values and 0% NA\n### Let's say that I take n RANDOM samples of size N from the population:\nn_samples <- 200\nN_size <- 1000\n\nbeta_coefs_b <- NULL\n\nfor(i in 1:n_samples){\n  df_sample <- df_b %>%\n    slice_sample(n=N_size)\n  model_sam <- lm(y~x,data = df_sample)\n  \n  beta_coefs_b <- rbind.data.frame(beta_coefs_b,coef(model_sam)[2])\n}## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\ncolnames(beta_coefs_b) <- \"betas\"\nbeta_coefs_b$model <- \"df_b\"\n\n## Now we compare:\nbeta_coefs_comp <- rbind.data.frame(beta_coefs_a,beta_coefs_b)\n\nbeta_coefs_comp %>%\n  group_by(model) %>%\n  mutate(mean_beta=mean(betas)) %>%\n  ungroup() %>%\n  ggplot(aes(x=betas,fill=model,color=model)) +\n  geom_density(alpha=0.5) + \n  geom_vline(aes(xintercept=mean_beta,color=model),linetype=\"dashed\") +\n  theme_minimal() ## group_by: one grouping variable (model)\n## mutate (grouped): new variable 'mean_beta' (double) with 2 unique values and 0% NA\n## ungroup: no grouping variables remain"},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"b.-increased-variation-in-x-is-good","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12.3.2 b. Increased variation in x is good:","text":"","code":"\n## This is my population regression a:\ndf_a <- tibble(x=rnorm(100000,sd=1)) %>%\n  mutate(y = 1 + 2*x + rnorm(100000))## mutate: new variable 'y' (double) with 100,000 unique values and 0% NA\n### Let's say that I take n RANDOM samples of size N from the population:\nn_samples <- 200\nN_size <- 1000\n\nbeta_coefs_a <- NULL\n\nfor(i in 1:n_samples){\n  df_sample <- df_a %>%\n    slice_sample(n=N_size)\n  model_sam <- lm(y~x,data = df_sample)\n  \n  beta_coefs_a <- rbind.data.frame(beta_coefs_a,coef(model_sam)[2])\n}## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\ncolnames(beta_coefs_a) <- \"betas\"\nbeta_coefs_a$model <- \"df_a\"\n\n## This is my population regression b:\ndf_b <- tibble(x=rnorm(100000,sd=5)) %>%\n  mutate(y = 1 + 2*x + rnorm(100000))## mutate: new variable 'y' (double) with 100,000 unique values and 0% NA\n### Let's say that I take n RANDOM samples of size N from the population:\nn_samples <- 200\nN_size <- 1000\n\nbeta_coefs_b <- NULL\n\nfor(i in 1:n_samples){\n  df_sample <- df_b %>%\n    slice_sample(n=N_size)\n  model_sam <- lm(y~x,data = df_sample)\n  \n  beta_coefs_b <- rbind.data.frame(beta_coefs_b,coef(model_sam)[2])\n}## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\n## slice_sample: removed 99,000 rows (99%), 1,000 rows remaining\ncolnames(beta_coefs_b) <- \"betas\"\nbeta_coefs_b$model <- \"df_b\"\n\n## Now we compare:\nbeta_coefs_comp <- rbind.data.frame(beta_coefs_a,beta_coefs_b)\n\nbeta_coefs_comp %>%\n  group_by(model) %>%\n  mutate(mean_beta=mean(betas)) %>%\n  ungroup() %>%\n  ggplot(aes(x=betas,fill=model,color=model)) +\n  geom_density(alpha=0.5) + \n  geom_vline(aes(xintercept=mean_beta,color=model),linetype=\"dashed\") +\n  theme_minimal() ## group_by: one grouping variable (model)\n## mutate (grouped): new variable 'mean_beta' (double) with 2 unique values and 0% NA\n## ungroup: no grouping variables remain"},{"path":"lecture-5-the-simple-regression-model-ii.html","id":"c.-increased-sample-size-is-good","chapter":"12 Lecture 5: The Simple Regression Model II","heading":"12.3.3 c. Increased sample size is good:","text":"","code":"\n## This is my population regression a:\ndf_a <- tibble(x=rnorm(100000)) %>%\n  mutate(y = 1 + 2*x + rnorm(100000))## mutate: new variable 'y' (double) with 100,000 unique values and 0% NA\n### Let's say that I take n RANDOM samples of size N from the population:\nn_samples <- 200\nN_size <- 20\n\nbeta_coefs_a <- NULL\n\nfor(i in 1:n_samples){\n  df_sample <- df_a %>%\n    slice_sample(n=N_size)\n  model_sam <- lm(y~x,data = df_sample)\n  \n  beta_coefs_a <- rbind.data.frame(beta_coefs_a,coef(model_sam)[2])\n}## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\n## slice_sample: removed 99,980 rows (>99%), 20 rows remaining\ncolnames(beta_coefs_a) <- \"betas\"\nbeta_coefs_a$model <- \"df_a\"\n\n## This is my population regression b:\ndf_b <- tibble(x=rnorm(100000)) %>%\n  mutate(y = 1 + 2*x + rnorm(100000))## mutate: new variable 'y' (double) with 100,000 unique values and 0% NA\n### Let's say that I take n RANDOM samples of size N from the population:\nn_samples <- 200\nN_size <- 2000\n\nbeta_coefs_b <- NULL\n\nfor(i in 1:n_samples){\n  df_sample <- df_b %>%\n    slice_sample(n=N_size)\n  model_sam <- lm(y~x,data = df_sample)\n  \n  beta_coefs_b <- rbind.data.frame(beta_coefs_b,coef(model_sam)[2])\n}## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\n## slice_sample: removed 98,000 rows (98%), 2,000 rows remaining\ncolnames(beta_coefs_b) <- \"betas\"\nbeta_coefs_b$model <- \"df_b\"\n\n## Now we compare:\nbeta_coefs_comp <- rbind.data.frame(beta_coefs_a,beta_coefs_b)\n\nbeta_coefs_comp %>%\n  group_by(model) %>%\n  mutate(mean_beta=mean(betas)) %>%\n  ungroup() %>%\n  ggplot(aes(x=betas,fill=model,color=model)) +\n  geom_density(alpha=0.5) + \n  geom_vline(aes(xintercept=mean_beta,color=model),linetype=\"dashed\") +\n  theme_minimal() ## group_by: one grouping variable (model)\n## mutate (grouped): new variable 'mean_beta' (double) with 2 unique values and 0% NA\n## ungroup: no grouping variables remain\n### NOTE: beta is unbiased!!"},{"path":"tutorial-5-the-simple-regression-model-ii.html","id":"tutorial-5-the-simple-regression-model-ii","chapter":"13 Tutorial 5: The Simple Regression Model II","heading":"13 Tutorial 5: The Simple Regression Model II","text":"","code":""},{"path":"tutorial-5-the-simple-regression-model-ii.html","id":"what-is-this-tutorial-about-3","chapter":"13 Tutorial 5: The Simple Regression Model II","heading":"What is this tutorial about?","text":"can access tutorial clicking . tutorial focus fundamental theory underlying regression analysis, based knowledge presented Achen (1982) Huntington-Klein’s Effect (2024). ’ll asked theoretical foundations practical applications regression analysis interpret findings.","code":""},{"path":"homework-1-practice.html","id":"homework-1-practice","chapter":"14 Homework 1 Practice:","heading":"14 Homework 1 Practice:","text":"","code":""},{"path":"homework-1-practice.html","id":"what-is-this-practice-about","chapter":"14 Homework 1 Practice:","heading":"What is this practice about?","text":"can access practice clicking . practice cover topics included Homework 1.","code":""},{"path":"lecture-6-the-multiple-regression-model-i.html","id":"lecture-6-the-multiple-regression-model-i","chapter":"15 Lecture 6: The Multiple Regression Model I","heading":"15 Lecture 6: The Multiple Regression Model I","text":"","code":""},{"path":"lecture-6-the-multiple-regression-model-i.html","id":"slides-5","chapter":"15 Lecture 6: The Multiple Regression Model I","heading":"Slides","text":"7 Multiple Regression Model (link)","code":""},{"path":"lecture-6-the-multiple-regression-model-i.html","id":"introduction-5","chapter":"15 Lecture 6: The Multiple Regression Model I","heading":"15.1 Introduction","text":"continue studying simple regression model.\nFigure 15.1: Slides 7 Multiple Regression Model.\n","code":"## \n## Attaching package: 'ggpubr'## The following objects are masked from 'package:tidylog':\n## \n##     group_by, mutate"},{"path":"lecture-6-the-multiple-regression-model-i.html","id":"vignette-6.1","chapter":"15 Lecture 6: The Multiple Regression Model I","heading":"15.2 Vignette 6.1","text":", let’s simulate data. Maybe interested urban rural towns (70% urban) :Now can estimate effect wage expenditure income:Wait ? (Interpret log ~ level)","code":"\ndf <- tibble(urban = sample(c(0,1),500,replace=T,prob=c(.3,.7))) %>%\n  ## Urban towns spend, on average, $3 million more on wages than rural towns\n  mutate(expen_wages = 3*urban+runif(500,min=0,max=4)) %>%\n  ## Urban towns are also have greater incomes (e.g., from taxes), but these are reduced by their high wage expenditures:\n  mutate(log_income = 1 + 2*urban - .3*expen_wages + rnorm(500,mean=2)) ## <- Population Eq.\nmodel_a <- lm(log_income ~ expen_wages, data = df) \nsummary(model_a) ## \n## Call:\n## lm(formula = log_income ~ expen_wages, data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3529 -0.7474 -0.0107  0.7696  3.4804 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  2.94527    0.11899  24.752   <2e-16 ***\n## expen_wages  0.05542    0.02647   2.093   0.0368 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.086 on 498 degrees of freedom\n## Multiple R-squared:  0.008723,   Adjusted R-squared:  0.006733 \n## F-statistic: 4.382 on 1 and 498 DF,  p-value: 0.03682"},{"path":"lecture-6-the-multiple-regression-model-i.html","id":"vignette-6.2","chapter":"15 Lecture 6: The Multiple Regression Model I","heading":"15.3 Vignette 6.2","text":"Let’s see… can remove everything wages explained urban? can remove everything income explained urban?difference explained urban income/expendinture (mean) observed value income/expenditure …residual… explained urban!!Let’s plot:","code":"\ndf %>% group_by(urban) %>%\n  summarise(income_urb= mean(log_income)) ## summarise: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##   urban income_urb\n##   <dbl>      <dbl>\n## 1     0       2.54\n## 2     1       3.44\ndf %>% group_by(urban) %>% \n  summarise(expen_wages_urb = mean(expen_wages))## summarise: now 2 rows and 2 columns, ungrouped## # A tibble: 2 × 2\n##   urban expen_wages_urb\n##   <dbl>           <dbl>\n## 1     0            1.86\n## 2     1            5.04\ndf <- df %>% group_by(urban) %>%\n  mutate(log_income_residual = log_income - mean(log_income),\n         expen_wages_residual = expen_wages - mean(expen_wages)) %>%\n  ungroup()## ungroup: no grouping variables remain\nmodel_b <- lm(log_income_residual ~ expen_wages_residual, data = df) \nsummary(model_b) ### CLOSER!## \n## Call:\n## lm(formula = log_income_residual ~ expen_wages_residual, data = df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.32648 -0.63083 -0.00077  0.58448  2.83280 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           2.371e-16  4.213e-02   0.000        1    \n## expen_wages_residual -3.228e-01  3.742e-02  -8.627   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.942 on 498 degrees of freedom\n## Multiple R-squared:   0.13,  Adjusted R-squared:  0.1283 \n## F-statistic: 74.42 on 1 and 498 DF,  p-value: < 2.2e-16\nA <- ggplot(df, aes(x=expen_wages,y=log_income)) +\n  geom_point() +\n  labs(title = \"0. Relation between wages and income. Beta = 0.13\") +\n  geom_smooth(method = \"lm\") +\n  xlim(c(-3,7)) + ylim(c(-3,6))\nA## `geom_smooth()` using formula = 'y ~ x'\nB <- ggplot(df, aes(x=expen_wages,y=log_income,color = factor(urban))) +\n  geom_point() +\n  labs(title = \"1. Relation between wages and income divided by urban.\") +\n  xlim(c(-3,7)) + ylim(c(-3,6))\nB\nC <- ggplot(df, aes(x=expen_wages_residual,y=log_income,color = factor(urban))) +\n  geom_point() +\n  labs(title = \"2. We remove the difference of wages explained by urban.\")+\n  xlim(c(-3,7)) + ylim(c(-3,6))\nC\nD <- ggplot(df, aes(x=expen_wages_residual,y=log_income_residual,color = factor(urban))) +\n  geom_point() +\n  labs(title = \"3. We remove the difference of income explained by urban.\")+\n  xlim(c(-3,7)) + ylim(c(-3,6))\nD\nE <- ggplot(df, aes(expen_wages_residual,y=log_income_residual)) +\n  geom_point() +\n  labs(title = \"4. We analize what is left. Beta = -0.22\") +\n  geom_smooth(method = \"lm\")+\n  xlim(c(-3,7)) + ylim(c(-3,6))\nE## `geom_smooth()` using formula = 'y ~ x'\nggarrange(A,B,C,D,E,\n          common.legend = T,\n          ncol = 2,\n          nrow = 3)## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"tutorial-6-regression-analysis-and-interpretation.html","id":"tutorial-6-regression-analysis-and-interpretation","chapter":"16 Tutorial 6: Regression Analysis and Interpretation","heading":"16 Tutorial 6: Regression Analysis and Interpretation","text":"","code":""},{"path":"tutorial-6-regression-analysis-and-interpretation.html","id":"what-is-this-tutorial-about-4","chapter":"16 Tutorial 6: Regression Analysis and Interpretation","heading":"What is this tutorial about?","text":"can access tutorial clicking . tutorial ’ll deepen understanding regression models recall questions complete--code tasks.","code":""},{"path":"lecture-6-exercises.html","id":"lecture-6-exercises","chapter":"17 Lecture 6 Exercises","heading":"17 Lecture 6 Exercises","text":"tutorial created John Santos (minor adaptations ).","code":""},{"path":"lecture-6-exercises.html","id":"how-to-nicely-display-model-results","chapter":"17 Lecture 6 Exercises","heading":"17.1 How to (nicely) display model results","text":"want display models aesthetically-pleasing format summary() command, options.Let’s start running models.","code":"\nlibrary(wooldridge)\ndata(\"wage1\")\nwage_mod1<- lm(wage ~ educ + exper, data=wage1)\nwage_mod1a<- lm(I(wage*100) ~ educ + exper, data=wage1)\nwage_mod2 <- lm(wage ~ educ, data=wage1)"},{"path":"lecture-6-exercises.html","id":"the-boring-ugly-way-base-r","chapter":"17 Lecture 6 Exercises","heading":"17.1.1 The boring, ugly way (Base R)","text":"","code":"\nsummary(wage_mod1)## \n## Call:\n## lm(formula = wage ~ educ + exper, data = wage1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5532 -1.9801 -0.7071  1.2030 15.8370 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -3.39054    0.76657  -4.423 1.18e-05 ***\n## educ         0.64427    0.05381  11.974  < 2e-16 ***\n## exper        0.07010    0.01098   6.385 3.78e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.257 on 523 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2222 \n## F-statistic: 75.99 on 2 and 523 DF,  p-value: < 2.2e-16\nsummary(wage_mod1a)## \n## Call:\n## lm(formula = I(wage * 100) ~ educ + exper, data = wage1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -555.32 -198.01  -70.71  120.30 1583.70 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -339.054     76.657  -4.423 1.18e-05 ***\n## educ          64.427      5.381  11.974  < 2e-16 ***\n## exper          7.010      1.098   6.385 3.78e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 325.7 on 523 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2222 \n## F-statistic: 75.99 on 2 and 523 DF,  p-value: < 2.2e-16\nsummary(wage_mod2)## \n## Call:\n## lm(formula = wage ~ educ, data = wage1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.3396 -2.1501 -0.9674  1.1921 16.6085 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.90485    0.68497  -1.321    0.187    \n## educ         0.54136    0.05325  10.167   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.378 on 524 degrees of freedom\n## Multiple R-squared:  0.1648, Adjusted R-squared:  0.1632 \n## F-statistic: 103.4 on 1 and 524 DF,  p-value: < 2.2e-16"},{"path":"lecture-6-exercises.html","id":"stargazer","chapter":"17 Lecture 6 Exercises","heading":"17.1.2 {stargazer}","text":"easiest use {stargazer} package eponymously-named function. package easy enough, flexible enough, , unfortunately, longer maintained.{stargazer}, want set type= option “text” work RStudio, change “html” “latex” output final document (respectively, HTML file .tex file).","code":"\nlibrary(stargazer)## \n## Please cite as:##  Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.##  R package version 5.2.3. https://CRAN.R-project.org/package=stargazer\nstargazer(wage_mod1, wage_mod1a, wage_mod2,\n          title = \"Modeling wage versus education and experience\",\n          type = \"text\")## \n## Modeling wage versus education and experience\n## ============================================================================================\n##                                               Dependent variable:                           \n##                     ------------------------------------------------------------------------\n##                              wage                I(wage * 100)                wage          \n##                               (1)                     (2)                     (3)           \n## --------------------------------------------------------------------------------------------\n## educ                       0.644***                64.427***                0.541***        \n##                             (0.054)                 (5.381)                 (0.053)         \n##                                                                                             \n## exper                      0.070***                7.010***                                 \n##                             (0.011)                 (1.098)                                 \n##                                                                                             \n## Constant                   -3.391***              -339.054***                -0.905         \n##                             (0.767)                (76.657)                 (0.685)         \n##                                                                                             \n## --------------------------------------------------------------------------------------------\n## Observations                  526                     526                     526           \n## R2                           0.225                   0.225                   0.165          \n## Adjusted R2                  0.222                   0.222                   0.163          \n## Residual Std. Error    3.257 (df = 523)       325.704 (df = 523)        3.378 (df = 524)    \n## F Statistic         75.990*** (df = 2; 523) 75.990*** (df = 2; 523) 103.363*** (df = 1; 524)\n## ============================================================================================\n## Note:                                                            *p<0.1; **p<0.05; ***p<0.01"},{"path":"lecture-6-exercises.html","id":"modelsummary","chapter":"17 Lecture 6 Exercises","heading":"17.1.3 {modelsummary}","text":"can also use package modelsummary function msummary(). package little involved, produces flexible output, works wider variety models, actively maintained.Modeling wage versus education experience","code":"\nlibrary(modelsummary)## \n## Attaching package: 'modelsummary'## The following object is masked from 'package:psych':\n## \n##     SD\nmsummary(models = dvnames(list(wage_mod1, wage_mod1a, wage_mod2)),\n         statistic = c(\"std.error\", \"p.value\"),\n         stars = TRUE,\n         title = \"Modeling wage versus education and experience\",\n         notes = \"Cell entries are parameter estimates, standard errors, and p values\")"},{"path":"lecture-6-exercises.html","id":"functional-form","chapter":"17 Lecture 6 Exercises","heading":"17.2 Functional form","text":"","code":""},{"path":"lecture-6-exercises.html","id":"feelings-towards-the-lpc-quadratics","chapter":"17 Lecture 6 Exercises","heading":"17.2.1 Feelings towards the LPC (quadratics)","text":"Quadratics right-hand-side (RHS) regression equation useful capturing non-linear relationships, especially relationship non-monotonic.function non-monotonic, means switches direction (either positive negative negative positive).’ll demonstrate modelling feelings towards Liberal Party Canada market liberalism.effect X Y, marketlib feel_lib requires \\(\\hat{\\beta_1}\\) \\(\\hat{\\beta_2}\\), can seen .\\[\\begin{aligned}\n\\hat{feellib} &= 60.439 + 0.654marketlib - 0.216marketlib^2 \\\\\n\\Delta\\hat{feellib} &= 0.654marketlib - 0.216marketlib^2\n\\end{aligned}\\]convenient approximation \\(\\Delta{\\hat{y}} &= \\hat{\\beta_1}x + \\hat{\\beta_2}x^2\\), applied follows:\\[\\begin{aligned}\n\\Delta{\\hat{y}} &= \\hat{\\beta_1}x + \\hat{\\beta_2}x^2  \\\\\n\\Delta{\\hat{y}} &\\approx [\\hat{\\beta_1} + (2 \\times \\hat{\\beta_2}x)] \\Delta{x}  \\\\\n\\Delta\\hat{feellib} &\\approx [0.654 - (2 \\times 0.216marketlib)] \\Delta{marketlib}  \\\\\n\\Delta\\hat{feellib} &\\approx [0.654 - 0.432marketlib] \\Delta{marketlib}  \\\\\n\\Delta{marketlib} &= 1  && \\text{(looking one-unit change marketlib)}  \\\\\n\\Delta\\hat{feellib} &\\approx [0.654 - 0.432marketlib]\n\\end{aligned}\\]","code":"\nload(\"Sample_data/ces.rda\")\n\nsummary(ces$marketlib)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00    6.00    9.00    8.59   11.00   16.00   32684\nsummary(ces$feel_lib)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00   16.00   54.00   48.34   77.00  100.00    2640\nggplot(ces) +\n  aes(x = feel_lib, y = marketlib) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(method = \"lm\", \n              formula = \"y ~ x + I(x^2)\", \n              color = \"red\")## `geom_smooth()` using formula = 'y ~ x'\nfeel1 <- lm(feel_lib ~ marketlib, data = ces)\nfeel2 <- lm(feel_lib ~ marketlib + I(marketlib^2), data = ces)\n\nstargazer(feel1, feel2,\n          type = \"text\")## \n## =======================================================================\n##                                     Dependent variable:                \n##                     ---------------------------------------------------\n##                                          feel_lib                      \n##                                (1)                       (2)           \n## -----------------------------------------------------------------------\n## marketlib                   -2.802***                   0.654          \n##                              (0.130)                   (0.481)         \n##                                                                        \n## I(marketlib2)                                         -0.216***        \n##                                                        (0.029)         \n##                                                                        \n## Constant                    71.472***                 60.439***        \n##                              (1.203)                   (1.904)         \n##                                                                        \n## -----------------------------------------------------------------------\n## Observations                  4,781                     4,781          \n## R2                            0.089                     0.099          \n## Adjusted R2                   0.089                     0.099          \n## Residual Std. Error    31.808 (df = 4779)        31.628 (df = 4778)    \n## F Statistic         466.091*** (df = 1; 4779) 263.454*** (df = 2; 4778)\n## =======================================================================\n## Note:                                       *p<0.1; **p<0.05; ***p<0.01"},{"path":"lecture-6-exercises.html","id":"calculating-the-turning-point-of-a-quadratic","chapter":"17 Lecture 6 Exercises","heading":"17.2.1.1 Calculating the turning point of a quadratic","text":"point (least theoretically) quadratic function changes direction called turning point, inflection point, functional minimum/maximum. often represented \\(x\\star\\).Another way think point rate change Y across values X equals 0, formally \\(\\Delta{\\hat{y}} = 0\\).quadratic equations \\(\\hat{y} = \\hat\\beta_0 + \\hat\\beta_1x + \\hat\\beta_2x^2\\) point can calculated formula \\(x\\star = -\\hat\\beta_1 / 2\\hat\\beta_2\\).example , point \n\\[\\begin{aligned}\nx\\star &= -\\hat\\beta_1 / 2\\hat\\beta_2  \\\\\nx\\star &= -0.654 / (2 \\times 0.216)  \\\\\nx\\star &= -0.654 / 0.432  \\\\\nx\\star &= -1.513889\n\\end{aligned}\\], turning point one-unit change marketlib causes feelings towards LPC change direction -1.5. outside actual range marketlib variable, relevance us (hence turning point sometimes theoretical).’ll encounter later course, don’t worry much point.","code":""},{"path":"lecture-6-exercises.html","id":"occupational-prestige-logarithms","chapter":"17 Lecture 6 Exercises","heading":"17.2.2 Occupational prestige (logarithms)","text":"Logarithms useful right- (predictor) left-hand-side (outcome) regression equations.log \\(X\\) useful predictor large effects \\(Y\\) low levels \\(X\\) smaller effects higher levels \\(X\\).can also useful converse situation.\\[\\hat{prestige} = 2.714 + 0.002897income\\]every increase income $1, prestige increases 0.002897 points scale 14 87 points.Alternatively, every increase $1000, prestige increases 2.9 points (0.002897 * 1000 = 2.897).…, relationship actually linear?looks like logarithmic function might work better, let’s try .make coefficients easier interpret, ’ll make new variable transformation income, measured thousands.\\[\\begin{aligned}\n\\hat{prestige} &= 27.141 + 2.897income   && \\text{(eq 1: level-level)}  \\\\\n\\hat{prestige} &= 9.050 + 21.556log(income)   && \\text{(eq 2: level-log)}  \\\\\n\\hat{log(prestige)} &= 3.353 + 0.062income   && \\text{(eq 3: log-level)}  \\\\\n\\hat{log(prestige)} &= 2.901 + 0.499log(income)   && \\text{(eq 4: log-log)}\n\\end{aligned}\\]Remembering rules interpreting models, …level-level unit increase X results b unit increase Ylevel-log percent increase X results (b/100)*unit increase Ylog-level unit increase X results 100*b% increase Ylog-log percent increase X results b% increase YTherefore, interpretation equation follows:every increase income $1 (thousand), prestige increases 2.897 points.every 1% increase income (thousands), prestige increases .21556 points (.e. \\(\\hat\\beta_1/100 = 21.556/100 == 0.21556\\)).every increase income $1 (thousand), prestige increases 6.2% (.e. \\(100 * \\hat\\beta_1 = 0.062/100 == 6.2\\%\\)).every 1% increase income (thousands), prestige increases 0.499%.","code":"\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(carData)\ndata(Prestige)\n\nsummary(Prestige)##    education          income          women           prestige         census    \n##  Min.   : 6.380   Min.   :  611   Min.   : 0.000   Min.   :14.80   Min.   :1113  \n##  1st Qu.: 8.445   1st Qu.: 4106   1st Qu.: 3.592   1st Qu.:35.23   1st Qu.:3120  \n##  Median :10.540   Median : 5930   Median :13.600   Median :43.60   Median :5135  \n##  Mean   :10.738   Mean   : 6798   Mean   :28.979   Mean   :46.83   Mean   :5402  \n##  3rd Qu.:12.648   3rd Qu.: 8187   3rd Qu.:52.203   3rd Qu.:59.27   3rd Qu.:8312  \n##  Max.   :15.970   Max.   :25879   Max.   :97.510   Max.   :87.20   Max.   :9517  \n##    type   \n##  bc  :44  \n##  prof:31  \n##  wc  :23  \n##  NA's: 4  \n##           \n## \nggplot(Prestige) +\n  aes(x = income, y = prestige) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"prestige ~ income\")## `geom_smooth()` using formula = 'y ~ x'\nprest1 <- lm(prestige ~ income, data = Prestige)\n\nsummary(prest1)## \n## Call:\n## lm(formula = prestige ~ income, data = Prestige)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -33.007  -8.378  -2.378   8.432  32.084 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 2.714e+01  2.268e+00   11.97   <2e-16 ***\n## income      2.897e-03  2.833e-04   10.22   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.09 on 100 degrees of freedom\n## Multiple R-squared:  0.5111, Adjusted R-squared:  0.5062 \n## F-statistic: 104.5 on 1 and 100 DF,  p-value: < 2.2e-16\nggplot(Prestige) +\n  aes(x = income, y = prestige) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(method = \"lm\", formula = \"y ~ log(x)\", color = \"red\") +\n  labs(title = \"prestige ~ income\")## `geom_smooth()` using formula = 'y ~ x'\nPrestige$income1000 <- Prestige$income / 1000\nprest1 <- lm(prestige ~ income1000, data = Prestige)\nprest2 <- lm(prestige ~ log(income1000), data = Prestige)\nprest3 <- lm(log(prestige) ~ income1000, data = Prestige)\nprest4 <- lm(log(prestige) ~ log(income1000), data = Prestige)\n\nlibrary(stargazer)\nstargazer(prest1, prest2, prest3, prest4,\n          type = \"text\")## \n## =========================================================================\n##                                           Dependent variable:            \n##                                ------------------------------------------\n##                                      prestige           log(prestige)    \n##                                   (1)        (2)        (3)       (4)    \n## -------------------------------------------------------------------------\n## income1000                      2.897***             0.062***            \n##                                 (0.283)               (0.007)            \n##                                                                          \n## log(income1000)                           21.556***             0.499*** \n##                                            (1.953)              (0.044)  \n##                                                                          \n## Constant                       27.141***   9.050**   3.353***   2.901*** \n##                                 (2.268)    (3.611)    (0.055)   (0.081)  \n##                                                                          \n## -------------------------------------------------------------------------\n## Observations                      102        102        102       102    \n## R2                               0.511      0.549      0.452     0.566   \n## Adjusted R2                      0.506      0.545      0.447     0.562   \n## Residual Std. Error (df = 100)   12.090     11.609     0.291     0.259   \n## F Statistic (df = 1; 100)      104.537*** 121.810*** 82.642*** 130.520***\n## =========================================================================\n## Note:                                         *p<0.1; **p<0.05; ***p<0.01\np1 <- ggplot(Prestige) +\n  aes(x = income, y = prestige) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"prestige ~ income\")\n\np2 <- ggplot(Prestige) +\n  aes(x = log(income), y = prestige) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"prestige ~ log(income)\")\n\np3 <- ggplot(Prestige) +\n  aes(x = income, y = log(prestige)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"log(prestige) ~ income\")\n\np4 <- ggplot(Prestige) +\n  aes(x = log(income), y = log(prestige)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"log(prestige) ~ log(income)\")\n\nlibrary(ggpubr)\nggarrange(p1, p2, p3, p4)## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'\n## `geom_smooth()` using formula = 'y ~ x'"},{"path":"lecture-6-exercises.html","id":"automobile-performance-logarithms","chapter":"17 Lecture 6 Exercises","heading":"17.2.3 Automobile performance (logarithms)","text":"another exercise using mtcars dataset carData package. ’ll model quarter-mile time (qsec) function horsepower (hp).","code":"\nlibrary(tidyverse)\nlibrary(ggplot2)\ndata(mtcars)\n\nsummary(mtcars)##       mpg             cyl             disp             hp             drat      \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0   Min.   :2.760  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5   1st Qu.:3.080  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0   Median :3.695  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7   Mean   :3.597  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0   3rd Qu.:3.920  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0   Max.   :4.930  \n##        wt             qsec             vs               am              gear      \n##  Min.   :1.513   Min.   :14.50   Min.   :0.0000   Min.   :0.0000   Min.   :3.000  \n##  1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:3.000  \n##  Median :3.325   Median :17.71   Median :0.0000   Median :0.0000   Median :4.000  \n##  Mean   :3.217   Mean   :17.85   Mean   :0.4375   Mean   :0.4062   Mean   :3.688  \n##  3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:4.000  \n##  Max.   :5.424   Max.   :22.90   Max.   :1.0000   Max.   :1.0000   Max.   :5.000  \n##       carb      \n##  Min.   :1.000  \n##  1st Qu.:2.000  \n##  Median :2.000  \n##  Mean   :2.812  \n##  3rd Qu.:4.000  \n##  Max.   :8.000\nhpqsec1 <- lm(qsec ~ hp, data = mtcars)\nhpqsec2 <- lm(qsec ~ log(hp), data = mtcars)\nhpqsec3 <- lm(log(qsec) ~ hp, data = mtcars)\nhpqsec4 <- lm(log(qsec) ~ log(hp), data = mtcars)\n\nlibrary(stargazer)\nstargazer(hpqsec1, hpqsec2, hpqsec3, hpqsec4,\n          type = \"text\")## \n## =====================================================================\n##                                         Dependent variable:          \n##                               ---------------------------------------\n##                                      qsec              log(qsec)     \n##                                  (1)       (2)       (3)       (4)   \n## ---------------------------------------------------------------------\n## hp                            -0.018***           -0.001***          \n##                                (0.003)            (0.0002)           \n##                                                                      \n## log(hp)                                 -2.576***           -0.146***\n##                                          (0.500)             (0.027) \n##                                                                      \n## Constant                      20.556*** 30.422*** 3.032***  3.591*** \n##                                (0.542)   (2.453)   (0.029)   (0.134) \n##                                                                      \n## ---------------------------------------------------------------------\n## Observations                     32        32        32        32    \n## R2                              0.502     0.469     0.530     0.488  \n## Adjusted R2                     0.485     0.451     0.515     0.471  \n## Residual Std. Error (df = 30)   1.282     1.323     0.069     0.072  \n## F Statistic (df = 1; 30)      30.190*** 26.514*** 33.882*** 28.575***\n## =====================================================================\n## Note:                                     *p<0.1; **p<0.05; ***p<0.01\nggplot(mtcars) +\n  aes(x = hp, y = qsec) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(method = \"lm\", formula = y ~ log(x), color = \"red\") +\n  labs(title = \"qsec ~ hp\")## `geom_smooth()` using formula = 'y ~ x'\nggplot(mtcars) +\n  aes(x = hp, y = log(qsec)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(title = \"log(qsec) ~ hp\")\nggplot(mtcars) +\n  aes(x = log(hp), y = log(qsec)) +\n  geom_point(position = \"jitter\") +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(title = \"log(qsec) ~ log(hp)\")"},{"path":"lecture-6-exercises.html","id":"practice-exercises-1","chapter":"17 Lecture 6 Exercises","heading":"17.3 Practice exercises","text":"lecture:Using ‘fertil2’ dataset ‘wooldridge’ women living Republic Botswana 1988…Estimate regression equation number children education, age mother, electricity.Interpret \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_{education}\\), \\(\\hat{\\beta}_{age}\\), \\(\\hat{\\beta}_{electric}\\).Verify sample covariance predicted values residuals = 0.Verify sample covariance education residuals = 0.Re-estimate regression equation number children education, age mother, electricity also include square age mother. now interpret effect age number children?Using ‘wage1’ datadest ‘woolridge’ US workers 1976…Estimate regression equation hourly wage (wage) dollars education (educ) experience (exper).coefficients change units wages cents instead dollars?Compare estimated coefficient education one obtained regressing hourly wage education. Interpret.Additional exercises tutorial:’s old saying (often attributed Winston Churchill) goes, “liberal young, heart. conservative old, brain.” , yet activist left-wing groups comprised seniors, like Raging Grannies.Using sample CES dataset, explore relationship Canadians’ support free market principles (\\(marketlib\\)) age (\\(age\\))? relationship? , linear? ’re going run two regression models. Model 1 regress \\(marketlib\\) \\(age\\), controlling \\(gender\\). Model 2 Model 1, except also include quadratic term \\(age\\) (.e. \\(age^2\\)).","code":""},{"path":"lecture-6-exercises.html","id":"stop-4","chapter":"17 Lecture 6 Exercises","heading":"17.4 STOP!!","text":"continue, try solving exercises . ’s way learn. , come back page see well .","code":""},{"path":"lecture-6-exercises.html","id":"continue-4","chapter":"17 Lecture 6 Exercises","heading":"17.5 Continue","text":"","code":""},{"path":"lecture-6-exercises.html","id":"lecture-exercises","chapter":"17 Lecture 6 Exercises","heading":"17.6 Lecture exercises","text":"","code":""},{"path":"lecture-6-exercises.html","id":"q1.-estimate-the-regression-equation-of-the-number-of-children-on-education-age-of-the-mother-and-electricity.","chapter":"17 Lecture 6 Exercises","heading":"17.6.1 Q1. Estimate the regression equation of the number of children on education, age of the mother, and electricity.","text":"begin, ’m going make data frame contains variables need listiwise delete NAs. make easier run diagnostic tests.population model :\\[children = \\beta_0 + \\beta_1educ + \\beta_2age + \\beta_3electric + u\\]model returns regression equation :\\[\\hat{children} = -2.071 - 0.079educ + 0.177age - 0.362electric\\]\\[n = 4,358 \\\\\nR^2 = 0.562\\]","code":"\nlibrary(wooldridge)\nlibrary(tidyverse)\ndata(\"fertil2\")\ndat <- fertil2 %>%\n  select(c(children, educ, age, electric)) %>%\n  na.omit()## select: dropped 23 variables (mnthborn, yearborn, radio, tv, bicycle, …)\nlibrary(wooldridge)\nfertil2 <- get(data('fertil2'))\nlibrary(dplyr)\n\nfmod1 <- lm(children ~ educ + age + electric, data = dat)\n\nsummary(fmod1)## \n## Call:\n## lm(formula = children ~ educ + age + electric, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.6019 -0.6814 -0.0619  0.7476  7.1061 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -2.071091   0.094741 -21.861  < 2e-16 ***\n## educ        -0.078751   0.006319 -12.462  < 2e-16 ***\n## age          0.176999   0.002729  64.855  < 2e-16 ***\n## electric    -0.361758   0.068032  -5.317  1.1e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.471 on 4354 degrees of freedom\n## Multiple R-squared:  0.5621, Adjusted R-squared:  0.5618 \n## F-statistic:  1863 on 3 and 4354 DF,  p-value: < 2.2e-16"},{"path":"lecture-6-exercises.html","id":"q2.-interpret-hatbeta_0-hatbeta_education-hatbeta_age-and-hatbeta_electric","chapter":"17 Lecture 6 Exercises","heading":"17.6.2 Q2. Interpret \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_{education}\\), \\(\\hat{\\beta}_{age}\\), and \\(\\hat{\\beta}_{electric}\\)","text":"\\(\\hat{\\beta_0}\\): intercept, average predicted number children three predictors set zero. -2.07 theoretically meaningful (negative children).\\(\\hat{\\beta}_{education}\\): every additional year education woman , predict , average, -0.08 fewer children, holding things constant.\\(\\hat{\\beta}_{age}\\): every additional year age, predict woman , average, 0.18 children, ceteris paribus.\\(\\hat{\\beta}_{electric}\\): Women electricity , average, 0.36 fewer children women without electricity, ceteris paribus.","code":""},{"path":"lecture-6-exercises.html","id":"q3.-verify-that-sample-covariance-between-predicted-values-and-residuals-0.","chapter":"17 Lecture 6 Exercises","heading":"17.6.3 Q3. Verify that sample covariance between predicted values and residuals = 0.","text":"two ways . Mathematically, can use cov() command fitted values residuals inputs function.covariance exactly 0, number small, basically zero.Note, finding correlation fitted values residuals gets us answer correlation coefficient simply mathematical transformation covariance bounds result [-1,1].might remember earlier stats classes \\[correlation = \\frac{covariance(x,y)}{\\sigma_x \\sigma_y}\\], result calculating equation manually equal using cor() commandWe can also confirm relationship fitted values residuals plotting .(called “residual versus fitted/predicted plot” useful tool diagnosing models potential problems like non-linearity, heteroskedasticity, outliers. )","code":"\ncov(fmod1$fitted.values, fmod1$residuals)## [1] -3.381377e-16\ncov(fitted(fmod1), resid(fmod1))## [1] -3.381377e-16\ncor.test(fmod1$fitted.values, fmod1$residuals)## \n##  Pearson's product-moment correlation\n## \n## data:  fmod1$fitted.values and fmod1$residuals\n## t = -9.1115e-15, df = 4356, p-value = 1\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.0296911  0.0296911\n## sample estimates:\n##           cor \n## -1.380524e-16\ncov(fmod1$fitted.values, fmod1$residuals) / (sd(fmod1$fitted.values) * sd(fmod1$residuals))## [1] -1.380524e-16\nlibrary(ggplot2)\nggplot() + \n  aes(x = fmod1$fitted.values, y = fmod1$residuals) +\n  geom_point(position = \"jitter\") + \n  labs(x = \"Predicted number of children\", \n       y = \"Residual\",\n       title = \"Predicted versus residual plot\") + \n  geom_smooth(method = \"lm\", se = FALSE)## `geom_smooth()` using formula = 'y ~ x'"},{"path":"lecture-6-exercises.html","id":"q4.-verify-that-sample-covariance-between-education-and-residuals-0.","chapter":"17 Lecture 6 Exercises","heading":"17.6.4 Q4. Verify that sample covariance between education and residuals = 0.","text":"’ll fitted versus residuals. First, ’ll use R calculate covariance., number exactly zero, basically zero.can confirm visual inspection.","code":"\ncov(dat$educ, resid(fmod1))## [1] 1.216786e-15\nggplot() + \n  aes(x = dat$educ, y = fmod1$residuals) +\n  geom_point(position = \"jitter\") + \n  labs(x = \"Education\", \n       y = \"Residual\",\n       title = \"Education versus residual plot\") + \n  geom_smooth(method = \"lm\", se = FALSE)## `geom_smooth()` using formula = 'y ~ x'"},{"path":"lecture-6-exercises.html","id":"q5.-re-estimate-the-regression-equation-of-the-number-of-children-on-education-age-of-the-mother-and-electricity-but-also-include-the-square-of-the-age-of-the-mother.-how-do-you-now-interpret-the-effect-of-age-on-the-number-of-children","chapter":"17 Lecture 6 Exercises","heading":"17.6.5 Q5. Re-estimate the regression equation of the number of children on education, age of the mother, and electricity but also include the square of the age of the mother. How do you now interpret the effect of age on the number of children?","text":"population model :\\[children = \\beta_0 + \\beta_1educ + \\beta_2age + \\beta_3age^2 + \\beta_4electric + u\\]include \\(age^2\\) model, can enclose parentheses preface capital “” (.e. (age^2)). tells lm() command calculate whatever inside ().model returns regression equation :\\[\\hat{children} = -4.248 - 0.079educ + 0.337age - 0.0027age^2 - 0.362electric\\]\n\\[n = 4,358 \\\\\nR^2 = 0.572\\]Adding quadratic (.e. squared) term age model returns positive coefficient \\(age\\) negative coefficient \\(age^2\\). means positive relationship age predicted number children point, relationship becomes negative.can check result using graphing calculator app like Desmos (https://www.desmos.com/calculator) entering function includes terms \\(age\\) \\(age^2\\) (.e. enter \\(y = 0.337x - 0.0027x^2\\) app).Using ‘wage1’ datadest ‘woolridge’ US workers 1976…","code":"\nfmod2 <- lm(children ~ educ + age + I(age^2) + electric, data = dat)\nsummary(fmod2)## \n## Call:\n## lm(formula = children ~ educ + age + I(age^2) + electric, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8576 -0.7153 -0.0005  0.7186  7.4868 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -4.2475679  0.2406003 -17.654  < 2e-16 ***\n## educ        -0.0788944  0.0062513 -12.620  < 2e-16 ***\n## age          0.3370397  0.0165166  20.406  < 2e-16 ***\n## I(age^2)    -0.0026696  0.0002718  -9.822  < 2e-16 ***\n## electric    -0.3777901  0.0673176  -5.612 2.12e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.455 on 4353 degrees of freedom\n## Multiple R-squared:  0.5716, Adjusted R-squared:  0.5712 \n## F-statistic:  1452 on 4 and 4353 DF,  p-value: < 2.2e-16"},{"path":"lecture-6-exercises.html","id":"q6.-estimate-the-regression-equation-of-the-hourly-wage-wage-in-dollars-on-education-educ-and-experience-exper.","chapter":"17 Lecture 6 Exercises","heading":"17.6.6 Q6. Estimate the regression equation of the hourly wage (\\(wage\\)) in dollars on education (\\(educ\\)) and experience (\\(exper\\)).","text":"population model :\\[wage = \\beta_0 + \\beta_1educ + \\beta_2exper + u\\]model returns regression equation :\\[\\hat{wage} = -3.391 - 0.644educ + 0.070exper\\]\n\\[n = 526 \\\\\nR^2 = 0.225\\]terms :\\(\\hat{\\beta}_{0}\\) : intercept, predicted wage education experience zero. three predictors set zero. -3.39 theoretically meaningful (one typically pay work).\\(\\hat{\\beta}_{educ}\\) : every additional year education worker , predict make, average, additional $0.64 per hour, ceteris paribus.\\(\\hat{\\beta}_{exper}\\) : every additional year experience worker , predict make, average, additional $0.07 per hour, ceteris paribus.","code":"\nlibrary(wooldridge)\ndata(\"wage1\")\nwage_mod1<- lm(wage ~ educ + exper, data=wage1)\nsummary(wage_mod1)## \n## Call:\n## lm(formula = wage ~ educ + exper, data = wage1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5532 -1.9801 -0.7071  1.2030 15.8370 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -3.39054    0.76657  -4.423 1.18e-05 ***\n## educ         0.64427    0.05381  11.974  < 2e-16 ***\n## exper        0.07010    0.01098   6.385 3.78e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.257 on 523 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2222 \n## F-statistic: 75.99 on 2 and 523 DF,  p-value: < 2.2e-16"},{"path":"lecture-6-exercises.html","id":"q7.-how-do-the-coefficients-change-when-the-units-of-wages-are-in-cents-instead-of-dollars","chapter":"17 Lecture 6 Exercises","heading":"17.6.7 Q7. How do the coefficients change when the units of wages are in cents instead of dollars?","text":"coefficients multiplied 100 (.e. \\(Intercept = -339.05\\), \\(educ = 64.43\\), \\(exper = 7.01\\)). 100 cents every dollar, multiplying left-hand side regression equation (.e. side containing \\(\\hat{wage}\\)) require use multiply entire right hand side equation 100 (.e. side contains terms \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_{educ}}\\), \\(\\hat{\\beta_{exper}}\\). equivalent multiplying every single term right-hand side 100 (.e. factoring 100).\\[\\begin{aligned}\n\\hat{wage_{dollars}} &= -3.391 - 0.644educ + 0.070exper \\\\\n\\hat{wage_{cents}} &= \\hat{wage_{dollars}} \\times 100 \\\\\n100 \\times \\hat{wage_{dollars}} &= [-3.391 - 0.644educ + 0.070exper] \\times 100 \\\\\n100 \\times \\hat{wage_{dollars}} &= [-3.391 \\times 100] - [0.644educ \\times 100] + [0.070exper \\times 100]  \\\\\n\\hat{wage_{cents}} &= -339.1 - 64.4educ + 7.0exper \\\\\n\\end{aligned}\\]confirm, let’s convert \\(wage\\) variable cents run model .","code":"\nwage_mod1a<- lm(I(wage*100) ~ educ + exper, data=wage1)\nsummary(wage_mod1a)## \n## Call:\n## lm(formula = I(wage * 100) ~ educ + exper, data = wage1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -555.32 -198.01  -70.71  120.30 1583.70 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -339.054     76.657  -4.423 1.18e-05 ***\n## educ          64.427      5.381  11.974  < 2e-16 ***\n## exper          7.010      1.098   6.385 3.78e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 325.7 on 523 degrees of freedom\n## Multiple R-squared:  0.2252, Adjusted R-squared:  0.2222 \n## F-statistic: 75.99 on 2 and 523 DF,  p-value: < 2.2e-16"},{"path":"lecture-6-exercises.html","id":"q8.-compare-the-estimated-coefficient-for-education-to-the-one-obtained-by-regressing-only-the-hourly-wage-on-educaiton.-interpret.","chapter":"17 Lecture 6 Exercises","heading":"17.6.8 Q8. Compare the estimated coefficient for education to the one obtained by regressing only the hourly wage on educaiton. Interpret.","text":"population model :\\[wage = \\beta_0 + \\beta_1educ + u\\]returns regression equation :\\[\\hat{wage} = -0.905 - 0.541educ\\]\n\\[n = 526 \\\\\nR^2 = 0.165\\]simple bivariate model \\(wage\\) regressed \\(educ\\), get coefficient \\(\\hat{\\beta}_{educ} = 0.541\\). lower coefficient multiple regression model, effect age \\(\\hat{\\beta}_{educ} = 0.644\\). suggests omitted variable bias accounting experience attenuates estimated relationship age experience.shows omitted variable bias always result overestimation coefficients, can also cause underestimated.Looking scatterplot \\(wage\\) versus \\(educ\\), colour-coded \\(exper\\) gives hints see pattern. lower left-hand corner (.e. low education low wage), see greatest concentration high high experience (green yellow) versus parts map. older folks lower levels (formal) education work lower-paying “lower-skill” jobs.see clump points low levels experience (purple) lower left-hand corner, purple point tend simple bivariate regression line, means outperforming expect expect model account experience.","code":"\nwage_mod2 <- lm(wage ~ educ, data=wage1)\nsummary(wage_mod2)## \n## Call:\n## lm(formula = wage ~ educ, data = wage1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.3396 -2.1501 -0.9674  1.1921 16.6085 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.90485    0.68497  -1.321    0.187    \n## educ         0.54136    0.05325  10.167   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.378 on 524 degrees of freedom\n## Multiple R-squared:  0.1648, Adjusted R-squared:  0.1632 \n## F-statistic: 103.4 on 1 and 524 DF,  p-value: < 2.2e-16\nggplot(wage1) + \n  aes(x=wage, y=educ, color=exper) +\n  geom_point(position=\"jitter\") + \n  scale_color_viridis_c() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x=\"Education\", \n       y=\"Wage\",\n       color=\"Exper\",\n       title=\"Wage by education\") ## `geom_smooth()` using formula = 'y ~ x'"},{"path":"lecture-6-exercises.html","id":"additional-practice-questions","chapter":"17 Lecture 6 Exercises","heading":"17.7 Additional practice questions","text":"’s old saying (often attributed Winston Churchill) says, “liberal young, heart. conservative old, brain.” , yet activist left-wing groups comprised seniors, like Raging Grannies.Using sample CES dataset, explore relationship Canadians’ support free market principles (\\(marketlib\\)) age (\\(age\\))? relationship? , linear?’re going run two regression models. Model 1 regress \\(marketlib\\) \\(age\\), controlling \\(gender\\). Model 2 Model 1, except also include quadratic term \\(age\\) (.e. \\(age^2\\)).begin, ’s useful look univariate distributions variables. Besides letting us know missing values, also helps interpreting substantive significance relationships find.Market liberalism variable ranges 0 16, mean 8.59 (SD = 3.52). (Note: simple additive index, summated rating scale, four Likert-type items commonly used Canadian political science research.)Age ranges 18 99, mean 49.0 (SD = 16.61).Gender categorical variable, 58.6% women 41.4% men.","code":"\nload(\"Sample_data/ces.rda\")\nsummary(ces$marketlib)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    0.00    6.00    9.00    8.59   11.00   16.00   32684\nsd(ces$marketlib, na.rm = TRUE)## [1] 3.519915\nsummary(ces$age)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    18.0    35.0    49.0    48.7    62.0    99.0\nsd(ces$age, na.rm = TRUE)## [1] 16.60883\nsummary(ces$gender)##   Man Woman  NA's \n## 15517 21928   288\ntable(ces$gender) %>% prop.table()## \n##       Man     Woman \n## 0.4143944 0.5856056"},{"path":"lecture-6-exercises.html","id":"for-both-model-1-and-model-2-write-the-equation-of-the-population-models.","chapter":"17 Lecture 6 Exercises","heading":"17.7.1 1. For both Model 1 and Model 2, write the equation of the population models.","text":"population models :\\[\\begin{aligned}\nmarketlib &= \\beta_0 + \\beta_1gender + \\beta_2age + \\mu && \\text{(Model 1)} \\\\\nmarketlib &= \\beta_0 + \\beta_1gender + \\beta_2age + \\beta_3age^2 + \\mu && \\text{(Model 2)} \\\\\n\\end{aligned}\\]","code":""},{"path":"lecture-6-exercises.html","id":"run-the-regressions-and-write-out-the-equations-for-the-estimated-regression-models.","chapter":"17 Lecture 6 Exercises","heading":"17.7.2 2. Run the regressions and write out the equations for the estimated regression models.","text":"regression equation Model 1 :\\[\\hat{marketlib} = 8.309 - 0.855gender + 0.014age\\]\n\\[n = 5025 \\\\ R^2 = 0.021 \\]regression equation Model 2 :\\[\\hat{marketlib} = 6.261 - 0.856gender + 0.100age -0.000824age^2\\]\n\\[n = 5025 \\\\ R^2 = 0.025 \\]","code":"\nmarketmod1 <- lm(marketlib ~ gender + age, data = ces)\nsummary(marketmod1)## \n## Call:\n## lm(formula = marketlib ~ gender + age, data = ces)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.3775 -2.2386  0.2442  2.5586  8.2885 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  8.309741   0.195029  42.608  < 2e-16 ***\n## genderWoman -0.855020   0.099390  -8.603  < 2e-16 ***\n## age          0.013516   0.003192   4.234 2.34e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.474 on 5022 degrees of freedom\n##   (32708 observations deleted due to missingness)\n## Multiple R-squared:  0.02078,    Adjusted R-squared:  0.02039 \n## F-statistic: 53.28 on 2 and 5022 DF,  p-value: < 2.2e-16\nmarketmod2 <- lm(marketlib ~ gender + age + I(age^2), data = ces)\nsummary(marketmod2)## \n## Call:\n## lm(formula = marketlib ~ gender + age + I(age^2), data = ces)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.2733 -2.2700  0.4905  2.5909  8.9988 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  6.2610833  0.5002976  12.515  < 2e-16 ***\n## genderWoman -0.8560525  0.0992055  -8.629  < 2e-16 ***\n## age          0.0996610  0.0196397   5.074 4.03e-07 ***\n## I(age^2)    -0.0008238  0.0001853  -4.445 8.97e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.467 on 5021 degrees of freedom\n##   (32708 observations deleted due to missingness)\n## Multiple R-squared:  0.02461,    Adjusted R-squared:  0.02403 \n## F-statistic: 42.24 on 3 and 5021 DF,  p-value: < 2.2e-16"},{"path":"lecture-6-exercises.html","id":"explain-what-the-coefficients-mean-in-both-models.-how-does-the-effect-of-age-differ-between-model-1-and-model-2","chapter":"17 Lecture 6 Exercises","heading":"17.7.3 3. Explain what the coefficients mean in both models. How does the effect of age differ between Model 1 and Model 2?","text":"Model 1, coefficients :\\(\\hat{\\beta_0}\\): intercept, average predicted market liberalism score men 0 years old. value 7.966, value theoretically meaningful.\\(\\hat{\\beta}_{age}\\): every additional year age, predict Canadian, average, 0.022 supportive free market principles, holding things constant. continuous variables like age, ’s often useful convert units–example, represent relationship saying , average, every increase 10 years age associated increase 0.220 market liberalism scale. (also makes theoretical sense variable like age wouldn’t really expect huge change year--year, might expect change 10 years.)\\(\\hat{\\beta}_{gender}\\): Ceteris paribus, women score average 0.826 points lower men market liberalism scale.Model 2, coefficients :\\(\\hat{\\beta_0}\\): intercept, average predicted market liberalism score men 0 years old. value 6.782, value theoretically meaningful.\\(\\hat{\\beta}_{age}\\) \\(\\hat{\\beta}_{age^2}\\): understand effect age, just look coefficients \\(age\\) \\(age^2\\) individually; look together. , effect age market liberalism \\(0.77age -0.000569age^2\\). non-linear, meaning rate change across possible changes across values age. also might non-monotonic, meaning positive changes across values fo age negative others. (want see graph —don’t know Assignment #1—’s Q4.)\\(\\hat{\\beta}_{gender}\\): Ceteris paribus, women score average 0.827 points lower men market liberalism scale.","code":""},{"path":"lecture-6-exercises.html","id":"which-of-the-models-better-explains-the-relationship-between-marketlib-and-age","chapter":"17 Lecture 6 Exercises","heading":"17.7.4 4. Which of the models better explains the relationship between \\(marketlib\\) and \\(age\\)?","text":"\\(R^2\\) Model 1 0.074 Model 2, 0.079. suggests Model 2 fits data better Model 1. bit comparing model fit, ’ll get weeks time, starting point.Also, empirical performance isn’t everything. Fortunately, also theoretical reasons suspect effect age might clear-cut linear relationship suggested old adage. Older folks typically lower levels income tend rely upon welfare state younger folks (average, things equal). , life-cycle effects might cause interests change, extension, orientations society run.also issue comparing across ages one point time separate life-cycle effects generational effects, neither models able disentangle.","code":""},{"path":"lecture-7-the-multiple-regression-model-ii.html","id":"lecture-7-the-multiple-regression-model-ii","chapter":"18 Lecture 7: The Multiple Regression Model II","heading":"18 Lecture 7: The Multiple Regression Model II","text":"","code":""},{"path":"lecture-7-the-multiple-regression-model-ii.html","id":"slides-6","chapter":"18 Lecture 7: The Multiple Regression Model II","heading":"Slides","text":"8 Multiple Regression Model II (link)fancy code Lecture :( …\nFigure 18.1: Slides 8 Multiple Regression Model II.\n","code":""},{"path":"tutorial-7-multiple-regression-analysis.html","id":"tutorial-7-multiple-regression-analysis","chapter":"19 Tutorial 7: Multiple Regression Analysis","heading":"19 Tutorial 7: Multiple Regression Analysis","text":"","code":""},{"path":"tutorial-7-multiple-regression-analysis.html","id":"what-is-this-tutorial-about-5","chapter":"19 Tutorial 7: Multiple Regression Analysis","heading":"What is this tutorial about?","text":"can access tutorial clicking . tutorial reinforce understanding fundamental logic assumptions underlying multiple regression models. also go couple selected exercises Wooldridge Chapter 3.","code":""},{"path":"lecture-8-the-multiple-regression-model-interpretation.html","id":"lecture-8-the-multiple-regression-model-interpretation","chapter":"20 Lecture 8: The Multiple Regression Model: Interpretation","heading":"20 Lecture 8: The Multiple Regression Model: Interpretation","text":"","code":""},{"path":"lecture-8-the-multiple-regression-model-interpretation.html","id":"slides-7","chapter":"20 Lecture 8: The Multiple Regression Model: Interpretation","heading":"Slides","text":"9 Multiple Regression Model: Interpretation (link)fancy code Lecture :( …\nFigure 20.1: Slides 9 Multiple Regression Model: Interpretation.\n","code":""},{"path":"lecture-9-exercises.html","id":"lecture-9-exercises","chapter":"21 Lecture 9 Exercises","heading":"21 Lecture 9 Exercises","text":"tutorial created John Santos (minor adaptations ). tutorial covers carrying hypothesis testing R.","code":"\nlibrary(tidyverse)\nlibrary(car)\nload(\"Sample_data/ces.rda\")\nmod <- lm(feel_cpc ~ woman + age + univ + \n            marketlib + moraltrad, data = ces)\nsummary(mod)## \n## Call:\n## lm(formula = feel_cpc ~ woman + age + univ + marketlib + moraltrad, \n##     data = ces)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -79.594 -22.510  -0.289  22.511  86.068 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  5.67469    3.00978   1.885 0.059496 .  \n## woman       -4.53724    1.24012  -3.659 0.000259 ***\n## age         -0.02066    0.03947  -0.524 0.600618    \n## univ         0.50494    1.26471   0.399 0.689739    \n## marketlib    2.71180    0.18729  14.479  < 2e-16 ***\n## moraltrad    2.83293    0.18193  15.572  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 29.49 on 2375 degrees of freedom\n##   (35352 observations deleted due to missingness)\n## Multiple R-squared:  0.2596, Adjusted R-squared:  0.2581 \n## F-statistic: 166.6 on 5 and 2375 DF,  p-value: < 2.2e-16"},{"path":"lecture-9-exercises.html","id":"the-car-package-and-the-functions-linearhypothesis-and-anova","chapter":"21 Lecture 9 Exercises","heading":"21.1 The {car} package and the functions linearHypothesis() and Anova()","text":"(confused Base R’s anova())learn conduct hypothesis testing manually using re-parameterized models, practice, often use pre-programmed functions, linearHypothesis() function car package.purposes, takes two arguments:object = model object (case, mod)hypothesis hypotheses tested quotes (following case, \"marketlib = 0\")note: multiple hypotheses can tested enclosed separate quoted equations combined c() command (e.g. c(\"age = 0\", \"univ = 0\"))","code":""},{"path":"lecture-9-exercises.html","id":"a-basic-test-of-one-terms-statistical-significance-e.g.-is-marketlib-0","chapter":"21 Lecture 9 Exercises","heading":"21.1.1 A basic test of one term’s statistical significance, e.g. is marketlib != 0?","text":"never use just look summary table, work.also useful show F-tests t-tests sort thing, different ways. tests model summary table t-tests term 0, return t-statistics. linearHypothesis() uses F-test, returns F-statistics., can see F-statistics t-statistics return p-values.","code":"\nlibrary(car)\n\nlinearHypothesis(mod, \"marketlib=0\") ## \n## Linear hypothesis test:\n## marketlib = 0\n## \n## Model 1: restricted model\n## Model 2: feel_cpc ~ woman + age + univ + marketlib + moraltrad\n## \n##   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n## 1   2376 2247929                                  \n## 2   2375 2065588  1    182341 209.65 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlinearHypothesis(mod, \"marketlib=0\") %>% as_tibble()## # A tibble: 2 × 6\n##   Res.Df      RSS    Df `Sum of Sq`     F  `Pr(>F)`\n##    <dbl>    <dbl> <dbl>       <dbl> <dbl>     <dbl>\n## 1   2376 2247929.    NA         NA    NA  NA       \n## 2   2375 2065588.     1     182341.  210.  1.35e-45\nsummary(mod)$coefficients ##                Estimate Std. Error    t value     Pr(>|t|)\n## (Intercept)  5.67469015 3.00978465  1.8854140 5.949593e-02\n## woman       -4.53724017 1.24012170 -3.6587056 2.590110e-04\n## age         -0.02066458 0.03946767 -0.5235825 6.006178e-01\n## univ         0.50494416 1.26470847  0.3992574 6.897395e-01\n## marketlib    2.71180033 0.18728635 14.4794339 1.349400e-45\n## moraltrad    2.83292849 0.18192678 15.5718055 3.914799e-52"},{"path":"lecture-9-exercises.html","id":"testing-whether-one-term-is-different-from-another-e.g.-is-marketlib-moraltrad","chapter":"21 Lecture 9 Exercises","heading":"21.1.2 Testing whether one term is different from another, e.g. is marketlib != moraltrad?","text":"effect moraltrad 0.1 larger marketlib, difference statistically significant?Notice linearHypothesis() function reformulates “marketlib != moraltrad” “marketlib - moraltrad == 0” (just like t-test).","code":"\nlinearHypothesis(mod, \"marketlib = moraltrad\")## \n## Linear hypothesis test:\n## marketlib - moraltrad = 0\n## \n## Model 1: restricted model\n## Model 2: feel_cpc ~ woman + age + univ + marketlib + moraltrad\n## \n##   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n## 1   2376 2065722                           \n## 2   2375 2065588  1    133.82 0.1539 0.6949\nlinearHypothesis(mod, \"marketlib - moraltrad = 0\")## \n## Linear hypothesis test:\n## marketlib - moraltrad = 0\n## \n## Model 1: restricted model\n## Model 2: feel_cpc ~ woman + age + univ + marketlib + moraltrad\n## \n##   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n## 1   2376 2065722                           \n## 2   2375 2065588  1    133.82 0.1539 0.6949"},{"path":"lecture-9-exercises.html","id":"testing-the-joint-significance-of-multiple-variables-e.g.-age-and-univ-are-jointly-0","chapter":"21 Lecture 9 Exercises","heading":"21.1.3 Testing the joint significance of multiple variables, e.g. “age and univ are jointly != 0”:","text":"","code":"\nlinearHypothesis(mod, c(\"age = 0\", \"univ = 0\"))## \n## Linear hypothesis test:\n## age = 0\n## univ = 0\n## \n## Model 1: restricted model\n## Model 2: feel_cpc ~ woman + age + univ + marketlib + moraltrad\n## \n##   Res.Df     RSS Df Sum of Sq     F Pr(>F)\n## 1   2377 2066009                          \n## 2   2375 2065588  2    420.89 0.242 0.7851"},{"path":"lecture-9-exercises.html","id":"testing-if-at-least-one-term-in-the-model-is-significant","chapter":"21 Lecture 9 Exercises","heading":"21.1.4 Testing if at least one term in the model is significant","text":"test asks least one woman, age, univ, marketlib, moraltrad different zero.Note, F-test overall significance regression, reported model summary table.","code":"\nlinearHypothesis(mod, c(\"woman = 0\",\n                        \"age = 0\",\n                        \"univ = 0\",\n                        \"marketlib = 0\",\n                        \"moraltrad = 0\"))## \n## Linear hypothesis test:\n## woman = 0\n## age = 0\n## univ = 0\n## marketlib = 0\n## moraltrad = 0\n## \n## Model 1: restricted model\n## Model 2: feel_cpc ~ woman + age + univ + marketlib + moraltrad\n## \n##   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n## 1   2380 2789889                                  \n## 2   2375 2065588  5    724301 166.56 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(mod)## \n## Call:\n## lm(formula = feel_cpc ~ woman + age + univ + marketlib + moraltrad, \n##     data = ces)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -79.594 -22.510  -0.289  22.511  86.068 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  5.67469    3.00978   1.885 0.059496 .  \n## woman       -4.53724    1.24012  -3.659 0.000259 ***\n## age         -0.02066    0.03947  -0.524 0.600618    \n## univ         0.50494    1.26471   0.399 0.689739    \n## marketlib    2.71180    0.18729  14.479  < 2e-16 ***\n## moraltrad    2.83293    0.18193  15.572  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 29.49 on 2375 degrees of freedom\n##   (35352 observations deleted due to missingness)\n## Multiple R-squared:  0.2596, Adjusted R-squared:  0.2581 \n## F-statistic: 166.6 on 5 and 2375 DF,  p-value: < 2.2e-16"},{"path":"lecture-9-exercises.html","id":"calculating-p-values-of-t-statistics-using-r","chapter":"21 Lecture 9 Exercises","heading":"21.2 Calculating p values of t statistics using R","text":"can also use R calculate p values t test using function pt().Recall, summary table model. extract manually printout summary() command rounds p values 2e-16.one-tailed test marketlib >= 0…one-tailed test marketlib <= 0…two-tailed test marketlib = 0…","code":"\nsummary(mod)$coefficients##                Estimate Std. Error    t value     Pr(>|t|)\n## (Intercept)  5.67469015 3.00978465  1.8854140 5.949593e-02\n## woman       -4.53724017 1.24012170 -3.6587056 2.590110e-04\n## age         -0.02066458 0.03946767 -0.5235825 6.006178e-01\n## univ         0.50494416 1.26470847  0.3992574 6.897395e-01\n## marketlib    2.71180033 0.18728635 14.4794339 1.349400e-45\n## moraltrad    2.83292849 0.18192678 15.5718055 3.914799e-52\npt(-summary(mod)$coefficients[5,3], mod$df, lower = FALSE)## [1] 1\npt(-summary(mod)$coefficients[5,3], mod$df, lower = TRUE)## [1] 6.747001e-46\n2*pt(abs(summary(mod)$coefficients[5,3]), mod$df, lower = FALSE)## [1] 1.3494e-45"},{"path":"lecture-9-exercises.html","id":"confidence-intervals","chapter":"21 Lecture 9 Exercises","heading":"21.3 Confidence intervals","text":"Recall, formula constructing confidence interval \\[CI_{\\beta_j} = \\hat{\\beta}_j \\pm (c \\times se_{\\hat{\\beta}_j})\\],\\(c\\) percentile \\(t_{n-k-1}\\) distribution corresponds confidence interval. 90%, 95%, 99% confidence intervals, corresponds , respectively, 1.64, 1.96, 2.57., wanted build CI around coefficient marketlib, first pull table coefficientsand note coefficient marketlib [5,1] SE [5,2]use formula calculate lower boundand upper bound.can vectorized:B + ( c(-1,1) * (1.96 * SE) )also use tidyverse conventions calculate confidence intervals:’re really lazy, can also {stargazer} .","code":"\nsummary(mod)$coefficients##                Estimate Std. Error    t value     Pr(>|t|)\n## (Intercept)  5.67469015 3.00978465  1.8854140 5.949593e-02\n## woman       -4.53724017 1.24012170 -3.6587056 2.590110e-04\n## age         -0.02066458 0.03946767 -0.5235825 6.006178e-01\n## univ         0.50494416 1.26470847  0.3992574 6.897395e-01\n## marketlib    2.71180033 0.18728635 14.4794339 1.349400e-45\n## moraltrad    2.83292849 0.18192678 15.5718055 3.914799e-52\nsummary(mod)$coefficients[5,1]## [1] 2.7118\nsummary(mod)$coefficients[5,2]## [1] 0.1872864\nsummary(mod)$coefficients[5,1] - (1.96 * summary(mod)$coefficients[5,2])## [1] 2.344719\nsummary(mod)$coefficients[5,1] + (1.96 * summary(mod)$coefficients[5,2])## [1] 3.078882\nsummary(mod)$coefficients[5,1] + (c(-1,1) * (1.96 * summary(mod)$coefficients[5,2]))## [1] 2.344719 3.078882\nsummary(mod)$coefficients %>%\n  as.data.frame() %>%\n  mutate(lower90 = Estimate - (`Std. Error` * 1.64),\n         upper90 = Estimate + (`Std. Error` * 1.64),\n         lower95 = Estimate - (`Std. Error` * 1.96),\n         upper95 = Estimate + (`Std. Error` * 1.96),\n         lower99 = Estimate - (`Std. Error` * 2.57),\n         upper99 = Estimate + (`Std. Error` * 2.57)) ##                Estimate Std. Error    t value     Pr(>|t|)     lower90     upper90\n## (Intercept)  5.67469015 3.00978465  1.8854140 5.949593e-02  0.73864332 10.61073697\n## woman       -4.53724017 1.24012170 -3.6587056 2.590110e-04 -6.57103977 -2.50344058\n## age         -0.02066458 0.03946767 -0.5235825 6.006178e-01 -0.08539156  0.04406239\n## univ         0.50494416 1.26470847  0.3992574 6.897395e-01 -1.56917773  2.57906605\n## marketlib    2.71180033 0.18728635 14.4794339 1.349400e-45  2.40465072  3.01894995\n## moraltrad    2.83292849 0.18192678 15.5718055 3.914799e-52  2.53456857  3.13128842\n##                 lower95     upper95    lower99     upper99\n## (Intercept) -0.22448777 11.57386806 -2.0604564 13.40983669\n## woman       -6.96787871 -2.10660163 -7.7243530 -1.35012739\n## age         -0.09802121  0.05669205 -0.1220965  0.08076733\n## univ        -1.97388444  2.98377276 -2.7453566  3.75524492\n## marketlib    2.34471908  3.07888158  2.2304744  3.19312625\n## moraltrad    2.47635199  3.18950499  2.3653767  3.30048033\nlibrary(stargazer)\nstargazer(mod,\n          ci = TRUE,\n          ci.level = 0.95, \n          type = \"html\")## \n## <table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\"><\/td><td><em>Dependent variable:<\/em><\/td><\/tr>\n## <tr><td><\/td><td colspan=\"1\" style=\"border-bottom: 1px solid black\"><\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td>feel_cpc<\/td><\/tr>\n## <tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\">woman<\/td><td>-4.537<sup>***<\/sup><\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td>(-6.968, -2.107)<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td><\/td><\/tr>\n## <tr><td style=\"text-align:left\">age<\/td><td>-0.021<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td>(-0.098, 0.057)<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td><\/td><\/tr>\n## <tr><td style=\"text-align:left\">univ<\/td><td>0.505<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td>(-1.974, 2.984)<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td><\/td><\/tr>\n## <tr><td style=\"text-align:left\">marketlib<\/td><td>2.712<sup>***<\/sup><\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td>(2.345, 3.079)<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td><\/td><\/tr>\n## <tr><td style=\"text-align:left\">moraltrad<\/td><td>2.833<sup>***<\/sup><\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td>(2.476, 3.189)<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td><\/td><\/tr>\n## <tr><td style=\"text-align:left\">Constant<\/td><td>5.675<sup>*<\/sup><\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td>(-0.224, 11.574)<\/td><\/tr>\n## <tr><td style=\"text-align:left\"><\/td><td><\/td><\/tr>\n## <tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\">Observations<\/td><td>2,381<\/td><\/tr>\n## <tr><td style=\"text-align:left\">R<sup>2<\/sup><\/td><td>0.260<\/td><\/tr>\n## <tr><td style=\"text-align:left\">Adjusted R<sup>2<\/sup><\/td><td>0.258<\/td><\/tr>\n## <tr><td style=\"text-align:left\">Residual Std. Error<\/td><td>29.491 (df = 2375)<\/td><\/tr>\n## <tr><td style=\"text-align:left\">F Statistic<\/td><td>166.559<sup>***<\/sup> (df = 5; 2375)<\/td><\/tr>\n## <tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\"><em>Note:<\/em><\/td><td style=\"text-align:right\"><sup>*<\/sup>p<0.1; <sup>**<\/sup>p<0.05; <sup>***<\/sup>p<0.01<\/td><\/tr>\n## <\/table>"},{"path":"lecture-9-exercises.html","id":"manually-calculating-se-t-and-p","chapter":"21 Lecture 9 Exercises","heading":"21.4 Manually calculating SE, t, and p","text":"","code":"\nsummary(mod)$coefficients ##                Estimate Std. Error    t value     Pr(>|t|)\n## (Intercept)  5.67469015 3.00978465  1.8854140 5.949593e-02\n## woman       -4.53724017 1.24012170 -3.6587056 2.590110e-04\n## age         -0.02066458 0.03946767 -0.5235825 6.006178e-01\n## univ         0.50494416 1.26470847  0.3992574 6.897395e-01\n## marketlib    2.71180033 0.18728635 14.4794339 1.349400e-45\n## moraltrad    2.83292849 0.18192678 15.5718055 3.914799e-52\n(b <- mod$coefficients)## (Intercept)       woman         age        univ   marketlib   moraltrad \n##  5.67469015 -4.53724017 -0.02066458  0.50494416  2.71180033  2.83292849\n(se <- sqrt(diag(vcov(mod))))## (Intercept)       woman         age        univ   marketlib   moraltrad \n##  3.00978465  1.24012170  0.03946767  1.26470847  0.18728635  0.18192678\n(t <- mod$coefficients / se)## (Intercept)       woman         age        univ   marketlib   moraltrad \n##   1.8854140  -3.6587056  -0.5235825   0.3992574  14.4794339  15.5718055\n(p <- 2*pt(abs(t), mod$df, lower = FALSE))##  (Intercept)        woman          age         univ    marketlib    moraltrad \n## 5.949593e-02 2.590110e-04 6.006178e-01 6.897395e-01 1.349400e-45 3.914799e-52\ntibble(\n  term = names(mod$coefficients),\n  estimate = b,\n  se = se,\n  t = t,\n  p = p)## # A tibble: 6 × 5\n##   term        estimate     se      t        p\n##   <chr>          <dbl>  <dbl>  <dbl>    <dbl>\n## 1 (Intercept)   5.67   3.01    1.89  5.95e- 2\n## 2 woman        -4.54   1.24   -3.66  2.59e- 4\n## 3 age          -0.0207 0.0395 -0.524 6.01e- 1\n## 4 univ          0.505  1.26    0.399 6.90e- 1\n## 5 marketlib     2.71   0.187  14.5   1.35e-45\n## 6 moraltrad     2.83   0.182  15.6   3.91e-52"},{"path":"lecture-9-exercises.html","id":"hypothesis-testing-exercises-cis-testing-coefficients-f-test","chapter":"21 Lecture 9 Exercises","heading":"21.5 Hypothesis Testing Exercises: CIs, testing coefficients, F-test","text":"Using ‘fertil2’ dataset ‘wooldridge’ women living Republic Botswana 1988, estimate regression equation number children education (educ), age mother (age) square, electricity (electric), husband’s education (heduc), whether women radio (radio) /TV (tv ) home.Construct 90% 95% confidence interval electric. Interpret.","code":"\nlibrary(wooldridge)\ndata(\"fertil2\")\n\nfertil2.mod <- lm(children ~ educ + age + I(age^2) + electric + heduc + radio + tv, data = fertil2)\n\nsummary(fertil2.mod)## \n## Call:\n## lm(formula = children ~ educ + age + I(age^2) + electric + heduc + \n##     radio + tv, data = fertil2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8086 -0.9290  0.0464  1.0729  7.2573 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -7.1070688  0.6531876 -10.881  < 2e-16 ***\n## educ        -0.0624920  0.0131587  -4.749 2.19e-06 ***\n## age          0.5326561  0.0401685  13.261  < 2e-16 ***\n## I(age^2)    -0.0054912  0.0005966  -9.205  < 2e-16 ***\n## electric    -0.2453435  0.1377525  -1.781   0.0751 .  \n## heduc       -0.0449532  0.0112074  -4.011 6.27e-05 ***\n## radio        0.1457302  0.0914020   1.594   0.1110    \n## tv          -0.3849668  0.1605246  -2.398   0.0166 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.734 on 1945 degrees of freedom\n##   (2408 observations deleted due to missingness)\n## Multiple R-squared:  0.4302, Adjusted R-squared:  0.4281 \n## F-statistic: 209.8 on 7 and 1945 DF,  p-value: < 2.2e-16\nlibrary(stargazer)\nstargazer(fertil2.mod, type = \"html\", single.row=TRUE)"},{"path":"lecture-9-exercises.html","id":"constructing-confidence-intervals","chapter":"21 Lecture 9 Exercises","heading":"21.5.1 Constructing confidence intervals","text":"two options : manually use {stargazer}.","code":""},{"path":"lecture-9-exercises.html","id":"manual-calculation-using-rounded-numbers","chapter":"21 Lecture 9 Exercises","heading":"21.5.1.1 Manual calculation using rounded numbers","text":"slightly used rounded numbers instead estimates extracted model.","code":"\n# Recall: B_elec = -0.245 (se = 0.138)\n\n# CI bounds = B +/- (SE * [crit value])\n\n# Lower 90%\n-0.245 - (0.138 * 1.645)## [1] -0.47201\n# Upper 90%\n-0.245 + (0.138 * 1.645)## [1] -0.01799\n# Lower and upper 95%, vectorized\n-0.245 + ( c(-1,1) * (0.138 * 1.960) )## [1] -0.51548  0.02548"},{"path":"lecture-9-exercises.html","id":"manual-calculation-using-actual-values","chapter":"21 Lecture 9 Exercises","heading":"21.5.1.2 Manual calculation using actual values","text":"’s better extract vectors betas SEs perform vectorized calculations terms model. (useful want create coefficient plot.)\nRegression results 90% CIs\n\nRegression results 95% CIs\n","code":"\n# Recall, the coefficients and their statistics can be found in the summary table\nsummary(fertil2.mod)$coefficients##                 Estimate  Std. Error    t value     Pr(>|t|)\n## (Intercept) -7.107068766 0.653187623 -10.880593 8.298592e-27\n## educ        -0.062491970 0.013158660  -4.749113 2.192456e-06\n## age          0.532656067 0.040168495  13.260543 1.740455e-38\n## I(age^2)    -0.005491249 0.000596579  -9.204563 8.590393e-20\n## electric    -0.245343458 0.137752499  -1.781045 7.506105e-02\n## heduc       -0.044953170 0.011207360  -4.011040 6.273806e-05\n## radio        0.145730153 0.091402002   1.594387 1.110119e-01\n## tv          -0.384966792 0.160524586  -2.398180 1.657049e-02\ncoef(summary(fertil2.mod))##                 Estimate  Std. Error    t value     Pr(>|t|)\n## (Intercept) -7.107068766 0.653187623 -10.880593 8.298592e-27\n## educ        -0.062491970 0.013158660  -4.749113 2.192456e-06\n## age          0.532656067 0.040168495  13.260543 1.740455e-38\n## I(age^2)    -0.005491249 0.000596579  -9.204563 8.590393e-20\n## electric    -0.245343458 0.137752499  -1.781045 7.506105e-02\n## heduc       -0.044953170 0.011207360  -4.011040 6.273806e-05\n## radio        0.145730153 0.091402002   1.594387 1.110119e-01\n## tv          -0.384966792 0.160524586  -2.398180 1.657049e-02\nlibrary(dplyr)\n\nci_table <- data.frame(\n  coef = coef(fertil2.mod),\n  lower90 = coef(fertil2.mod) - (coef(summary(fertil2.mod))[, 2] * 1.645),\n  upper90 = coef(fertil2.mod) + (coef(summary(fertil2.mod))[, 2] * 1.645),\n  lower95 = coef(fertil2.mod) - (coef(summary(fertil2.mod))[, 2] * 1.960),\n  upper95 = coef(fertil2.mod) + (coef(summary(fertil2.mod))[, 2] * 1.960)\n  )\n\nci_table##                     coef      lower90      upper90      lower95      upper95\n## (Intercept) -7.107068766 -8.181562406 -6.032575125 -8.387316507 -5.826821024\n## educ        -0.062491970 -0.084137965 -0.040845974 -0.088282943 -0.036700996\n## age          0.532656067  0.466578892  0.598733241  0.453925816  0.611386317\n## I(age^2)    -0.005491249 -0.006472622 -0.004509877 -0.006660544 -0.004321954\n## electric    -0.245343458 -0.471946320 -0.018740597 -0.515338357  0.024651440\n## heduc       -0.044953170 -0.063389277 -0.026517063 -0.066919595 -0.022986745\n## radio        0.145730153 -0.004626141  0.296086447 -0.033417771  0.324878077\n## tv          -0.384966792 -0.649029736 -0.120903848 -0.699594981 -0.070338603\nlibrary(stargazer)\nstargazer(fertil2.mod, type = \"html\", no.space=TRUE,\n          ci = TRUE, ci.level = 0.90, single.row=TRUE,\n          title = \"Regression results with 90% CIs\")\nstargazer(fertil2.mod, type = \"html\", no.space=TRUE,\n          ci = TRUE, ci.level = 0.95, single.row=TRUE,\n          title = \"Regression results with 95% CIs\")"},{"path":"lecture-9-exercises.html","id":"evaluate-the-following-hypotheses-at-the-5-and-1-levels-of-significance.-interpret.","chapter":"21 Lecture 9 Exercises","heading":"21.5.2 Evaluate the following hypotheses at the 5% and 1% levels of significance. Interpret.","text":"","code":""},{"path":"lecture-9-exercises.html","id":"section","chapter":"21 Lecture 9 Exercises","heading":"21.5.2.1 1.","text":"\\(H0 : B_{educ} = B{heduc}\\)\\(H1 : B_{educ} < B_{heduc}\\)p-value = 0.402 F-test \\(B_{educ} = B_{heduc}\\). Therefore reject null hypothesis either 10% 5% significance level sufficient evidence suggests father’s level education predictive fertility mother’s level education.","code":"\nlibrary(car)\nlinearHypothesis(fertil2.mod, \"educ = heduc\")## \n## Linear hypothesis test:\n## educ - heduc = 0\n## \n## Model 1: restricted model\n## Model 2: children ~ educ + age + I(age^2) + electric + heduc + radio + \n##     tv\n## \n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1   1946 5852.2                           \n## 2   1945 5850.1  1    2.1133 0.7026  0.402"},{"path":"lecture-9-exercises.html","id":"section-1","chapter":"21 Lecture 9 Exercises","heading":"21.5.2.2 2","text":"\\(H0 : B_{radio} = 0; B_{tv} = 0\\)\\(H1 : H0\\) trueThis hypothesis asks radio tv improve model fit, five variables accounted –.e. model without two terms statistically distinguishable model include ?F-test testing whether \\(B_{radio}\\) \\(B_{TV} = 0\\) returns p-value 0.01779. means can reject null hypothesis equal 0 0.05 significance level 0.01 significance level., mixed evidence can safely excluded model.","code":"\nlinearHypothesis(fertil2.mod, c(\"radio = 0\", \"tv = 0\"))## \n## Linear hypothesis test:\n## radio = 0\n## tv = 0\n## \n## Model 1: restricted model\n## Model 2: children ~ educ + age + I(age^2) + electric + heduc + radio + \n##     tv\n## \n##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n## 1   1947 5874.3                              \n## 2   1945 5850.1  2    24.289 4.0377 0.01779 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lecture-9-exercises.html","id":"a.-using-caranova-for-a-restricted-models-f-test","chapter":"21 Lecture 9 Exercises","heading":"21.5.2.3 2a. Using car::anova for a restricted models F-test","text":"code functionally equivalent code , assuming number cases used estimate .","code":"\nfertil2.mod.restricted <- lm(children ~ educ + age + I(age^2) + electric + heduc, data = fertil2)\n\nanova(fertil2.mod, fertil2.mod.restricted)## Analysis of Variance Table\n## \n## Model 1: children ~ educ + age + I(age^2) + electric + heduc + radio + \n##     tv\n## Model 2: children ~ educ + age + I(age^2) + electric + heduc\n##   Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n## 1   1945 5850.1                              \n## 2   1947 5874.3 -2   -24.289 4.0377 0.01779 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lecture-9-exercises.html","id":"b.-using-r2-form-of-f-test","chapter":"21 Lecture 9 Exercises","heading":"21.5.2.4 2b. Using R^2 form of F test","text":"F-value test 4.022923, greater critical value 95% (3.00) 99% (4.61).","code":"\nsummary(fertil2.mod)## \n## Call:\n## lm(formula = children ~ educ + age + I(age^2) + electric + heduc + \n##     radio + tv, data = fertil2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8086 -0.9290  0.0464  1.0729  7.2573 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -7.1070688  0.6531876 -10.881  < 2e-16 ***\n## educ        -0.0624920  0.0131587  -4.749 2.19e-06 ***\n## age          0.5326561  0.0401685  13.261  < 2e-16 ***\n## I(age^2)    -0.0054912  0.0005966  -9.205  < 2e-16 ***\n## electric    -0.2453435  0.1377525  -1.781   0.0751 .  \n## heduc       -0.0449532  0.0112074  -4.011 6.27e-05 ***\n## radio        0.1457302  0.0914020   1.594   0.1110    \n## tv          -0.3849668  0.1605246  -2.398   0.0166 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.734 on 1945 degrees of freedom\n##   (2408 observations deleted due to missingness)\n## Multiple R-squared:  0.4302, Adjusted R-squared:  0.4281 \n## F-statistic: 209.8 on 7 and 1945 DF,  p-value: < 2.2e-16\nsummary(fertil2.mod.restricted)## \n## Call:\n## lm(formula = children ~ educ + age + I(age^2) + electric + heduc, \n##     data = fertil2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8892 -0.9183  0.0350  1.0893  7.1715 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -6.9725803  0.6515291 -10.702  < 2e-16 ***\n## educ        -0.0647434  0.0127459  -5.080 4.14e-07 ***\n## age          0.5317886  0.0400575  13.276  < 2e-16 ***\n## I(age^2)    -0.0054960  0.0005956  -9.227  < 2e-16 ***\n## electric    -0.3880371  0.1238214  -3.134  0.00175 ** \n## heduc       -0.0468908  0.0110680  -4.237 2.38e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.737 on 1947 degrees of freedom\n##   (2408 observations deleted due to missingness)\n## Multiple R-squared:  0.4278, Adjusted R-squared:  0.4263 \n## F-statistic: 291.1 on 5 and 1947 DF,  p-value: < 2.2e-16\n# R^2 forumula \n((0.4302 - 0.4278) / 2) / ((1-0.4302) / (1953 - 7 - 1))## [1] 4.096174\n# SSR formula\n((5874.3 - 5850.1) / 2) / (5850.1 / (1953 - 7 - 1))## [1] 4.022923"},{"path":"lecture-9-exercises.html","id":"section-2","chapter":"21 Lecture 9 Exercises","heading":"21.5.2.5 3.","text":"\\(H0 : B_{educ} = B_{age} = B_{electric} = B_{heduc} = B_{radio} = B_{tv} = 0\\)\\(H1 : H0\\) trueThis F-test overall significance regression (.e. whether slope coefficients different zero, model terms performs better model just intercept).p-value F-test small (16 zeros decimal place). Therefore can reject null hypothesis coefficients model equal zero, means least one non-zero.","code":"\nlinearHypothesis(fertil2.mod, c( \"educ = 0\", \n                                 \"age = 0\", \n                                 \"I(age^2) = 0\", \n                                 \"electric = 0\", \n                                 \"heduc = 0\", \n                                 \"radio = 0\", \n                                 \"tv = 0\")) ## \n## Linear hypothesis test:\n## educ = 0\n## age = 0\n## I(age^2) = 0\n## electric = 0\n## heduc = 0\n## radio = 0\n## tv = 0\n## \n## Model 1: restricted model\n## Model 2: children ~ educ + age + I(age^2) + electric + heduc + radio + \n##     tv\n## \n##   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n## 1   1952 10266.4                                  \n## 2   1945  5850.1  7    4416.4 209.76 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lecture-9-exercises.html","id":"turning-points-and-heteroskedasticity","chapter":"21 Lecture 9 Exercises","heading":"21.6 Turning points and Heteroskedasticity","text":"Using ‘fertil2’ dataset ‘wooldridge’ women living Republic Botswana 1988, estimate regression equation number children (children) education (educ), age mother (age) square, electricity (electric), husband’s education (heduc), whether women radio (radio) /TV (tv) home.population model :\\[children = \\beta_0 + \\beta_1educ + \\beta_2age + \\beta_3age^2 + \\beta_4electric + \\beta_5heduc + \\beta_6radio + \\beta_7tv + u\\](Note: start, create new dataframe just variables need listwise delete observations missing values. make easier run diagnostics later.)regression model gives us equation \\[\\hat{children} = -7.107 - 0.062educ + 0.533age - 0.0055age^2 - 0.245electric - 0.045heduc + 0.146radio  - 0.385tv\\]\n\\[n = 1953, R^2 = 0.430\\]","code":"\nlibrary(tidyverse)\nlibrary(wooldridge)\nfertil2 <- get(data(\"fertil2\"))\ndat <- fertil2 %>%\n  select(c(children, educ, age, electric, heduc, radio, tv)) %>%\n  na.omit()\n  \nmod1 <- lm(children ~ educ + age + I(age^2) + electric + heduc + radio + tv, data = dat)\nsummary(mod1)## \n## Call:\n## lm(formula = children ~ educ + age + I(age^2) + electric + heduc + \n##     radio + tv, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.8086 -0.9290  0.0464  1.0729  7.2573 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -7.1070688  0.6531876 -10.881  < 2e-16 ***\n## educ        -0.0624920  0.0131587  -4.749 2.19e-06 ***\n## age          0.5326561  0.0401685  13.261  < 2e-16 ***\n## I(age^2)    -0.0054912  0.0005966  -9.205  < 2e-16 ***\n## electric    -0.2453435  0.1377525  -1.781   0.0751 .  \n## heduc       -0.0449532  0.0112074  -4.011 6.27e-05 ***\n## radio        0.1457302  0.0914020   1.594   0.1110    \n## tv          -0.3849668  0.1605246  -2.398   0.0166 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.734 on 1945 degrees of freedom\n## Multiple R-squared:  0.4302, Adjusted R-squared:  0.4281 \n## F-statistic: 209.8 on 7 and 1945 DF,  p-value: < 2.2e-16\nlibrary(stargazer)\nstargazer(mod1, \n          digits = 4,\n          header = FALSE,\n          no.space = TRUE,\n          align = TRUE,\n          type = \"html\")"},{"path":"lecture-9-exercises.html","id":"interpret-the-effect-of-age-on-children-and-find-the-turning-point.","chapter":"21 Lecture 9 Exercises","heading":"21.6.1 1. Interpret the effect of age on children and find the turning point.","text":"Even without calculations, can know something relationship age number children just looking table.negative sign \\(age^2\\) tells us effect starts positive diminishes strength reaches certain point (maximum function). maximum, effect negative.","code":""},{"path":"lecture-9-exercises.html","id":"the-effect-of-age","chapter":"21 Lecture 9 Exercises","heading":"21.6.1.1 The effect of age","text":"calculate (approximate) effect \\(age\\) \\(children\\), can apply power rule quadratic term (.e. exponent \\(n=2\\)):\\[\\begin{aligned}\n\\frac{d}{dx}[x^n] &= nx^{n-1}   \\\\\nn &= 2   \\\\\n\\frac{d}{dx}[x^2] &= 2x^{2-1}   \\\\\n\\frac{d}{dx}[x^2] &= 2x\n\\end{aligned}\\]Applying example gives approximation effect \\(age\\) \\(children\\) (average, holding variables constant):\\[\\begin{aligned}\n\\Delta\\hat{children} &= \\hat{\\beta}_2\\Delta{age} + \\hat{\\beta}_3\\Delta{age^2} \\\\\n\\Delta\\hat{children} &\\approx [(\\hat{\\beta}_2 + 2\\hat{\\beta}_3{age}]\\Delta{age} \\\\\n\\Delta\\hat{children} &\\approx [0.533 + 2(-0.0055 \\times age)]\\Delta{age} \\\\\n\\Delta\\hat{children} &\\approx [0.533 - 0.011age]\\Delta{age} \\\\\n\\end{aligned}\\]","code":""},{"path":"lecture-9-exercises.html","id":"calculating-the-turning-point","chapter":"21 Lecture 9 Exercises","heading":"21.6.1.2 Calculating the turning point","text":"calculate turning point (also known inflection point), use formula:\\[\\begin{aligned}\nage\\star &= - \\hat{\\beta}_2 / (2 * \\hat{\\beta}_3) \\\\\nage\\star &= - 0.533 / (2 * -0.0055) \\\\\nage\\star &= - 0.533 / (-0.011) \\\\\nage\\star &=  48.5\n\\end{aligned}\\]Thus, point \\(\\hat{children}\\) stops increasing starts decreasing –average holding factors model constant–48.5 years.","code":"\n- mod1$coefficients[3] / (2 * mod1$coefficients[4])##      age \n## 48.50045"},{"path":"lecture-9-exercises.html","id":"replace-the-quadratic-functional-form-of-age-for-a-logarithmic-form-instead-that-is-replace-age-and-its-square-with-just-logage.-what-do-you-conclude","chapter":"21 Lecture 9 Exercises","heading":"21.6.2 2. Replace the quadratic functional form of \\(age\\) for a logarithmic form instead, that is, replace \\(age\\) and its square with just \\(log(age)\\). What do you conclude?","text":"Note, \\(\\beta_2\\), 5.402 predicted increase \\(\\hat{children}\\) one-unit increase \\(log(age)\\), ceteris paribus. facilitate interpretation (following approximations Wooldridge p.44), can divide \\(\\beta_2\\) 100 find much \\(\\hat{children}\\) increases (units) 1% increase \\(age\\).\\[\\begin{aligned}\n\\Delta \\hat{children} &\\approx (\\beta_2 /100)(\\text{%} \\Delta{age})  \\\\\n\\Delta \\hat{children} &\\approx (5.40185 /100)(\\text{%} \\Delta{age} ) \\\\\n\\Delta \\hat{children} &\\approx 0.054(\\text{%} \\Delta{age})  \\\\\n\\end{aligned}\\]Therefore, every 1% increase woman’s age, model predicts increase 0.054 children average, holding factors constant.model logarithmic term fit model well model quadratic term (adjusted R^2 = 0.420 versus 0.428, respectively; SER = 1.734 versus 1.747). basis, one might pick Model 1 Model 2.","code":"\nmod2 <- lm(children ~ educ + log(age) + electric + heduc + radio + tv, data = fertil2)\nstargazer(mod1, mod2, \n          no.space = TRUE,\n          header = FALSE,\n          align = TRUE,\n          type = \"html\")"},{"path":"lecture-9-exercises.html","id":"comparing-models-visually","chapter":"21 Lecture 9 Exercises","heading":"21.6.2.1 Comparing models visually","text":"","code":"\nlibrary(ggeffects)\nplot.sqmod <- ggpredict(mod1, \"age\") %>% plot() + ggtitle(\"age + age^2\")\nplot.logmod <- ggpredict(mod2, \"age\") %>% plot() + ggtitle(\"log(age)\")\nlibrary(ggpubr)\nggarrange(plot.sqmod, plot.logmod)"},{"path":"lecture-9-exercises.html","id":"test-the-regression-model-for-heteroskedasticity-by-using-the-three-steps-presented-above-and-comparing-it-with-the-function-provided-in-r-for-the-breusch-pagan-test.","chapter":"21 Lecture 9 Exercises","heading":"21.6.3 3. Test the regression model for heteroskedasticity by using the three steps presented above and comparing it with the function provided in R for the Breusch-Pagan test.","text":"B-P test consists regression squared residuals constituent terms evaluating resulting F-test overall significance regression.","code":""},{"path":"lecture-9-exercises.html","id":"manual-procedure","chapter":"21 Lecture 9 Exercises","heading":"21.6.3.1 Manual procedure","text":"’ll start running test models Model 1 (quadratic) Model 2 (logarithmic).automatically get F-statistic associated p-value model outputs, see get positive test result quadratic logarithmic form models.thoroughness, can calculate F-statistics manually calculate p-values.Calculate F statistic BP Test Model 1:\\[\\begin{aligned}\nF &= \\frac{ R^2_{\\hat{u}^2} / k }{ (1 - R^2_{\\hat{u}^2}) / (n - k - 1) }   \\\\\nF &= \\frac{ (0.1681318 / 7) }  { ((1-0.1681318) / (1953-7-1)) }   \\\\\nF &= 56.1587   \\\\\n\\end{aligned}\\]Calculate F statistic BP Test Model 2:\\[\\begin{aligned}\nF &= \\frac{ R^2_{\\hat{u}^2} / k }{ (1 - R^2_{\\hat{u}^2}) / (n - k - 1) }   \\\\\nF &= \\frac{ (0.1585037 / 6) }  { ((1-0.1585037) / (1953-6-1)) }   \\\\\nF &= 61.09122   \\\\\n\\end{aligned}\\]","code":"\n# Mod 1 (quad)\ndat$m1_resid <- resid(mod1)\ndat$m1_residsq <- dat$m1_resid ^2\nbptest_m1 <- lm(m1_residsq ~ educ + age + I(age^2) + electric + heduc + radio + tv, data = dat)\n# Mod 2 (log)\ndat$m2_resid <- resid(mod2)\ndat$m2_resid2sq <- dat$m2_resid ^2\nbptest_m2 <- lm(m2_resid2sq ~ educ + log(age) + electric + heduc + radio + tv, data = dat)\nstargazer(bptest_m1, bptest_m2,\n          header = FALSE,\n          no.space = FALSE,\n          align = TRUE,\n          type = \"html\")\n(summary(bptest_m1)$r.squared / 7) / \n  ((1 - summary(bptest_m1)$r.squared) / (bptest_m1$df.residual))## [1] 56.1587\npf(56.1587, 7, 1945, lower.tail = FALSE)## [1] 1.873915e-73\n(summary(bptest_m2)$r.squared / 6) / \n  ((1 - summary(bptest_m2)$r.squared) / (bptest_m2$df.residual))## [1] 61.09122\npf(61.09122, 6, 1946, lower.tail = FALSE)## [1] 1.435968e-69"},{"path":"lecture-9-exercises.html","id":"calculating-the-lm-statistic","chapter":"21 Lecture 9 Exercises","heading":"21.6.3.2 Calculating the LM statistic","text":"calculate LM statistic, ’ll use formula provided:\\[\\begin{aligned}\nLM &= n \\times R^2_{\\hat{u}^2}   \\\\\nLM &= 1953 \\times 0.1681318   \\\\\nLM &= 328.3614\n\\end{aligned}\\]manual B-P test (.e. regression squared residuals constituent terms) Model 1 (quadratic) returns result significant 0.001 level. confirmed using lmtest::bptest().Calculate LM statistic Model 2:\\[\\begin{aligned}\nLM &= n \\times R^2_{\\hat{u}^2}   \\\\\nLM &= 1953 \\times 0.1585037   \\\\\nLM &= 309.5577\n\\end{aligned}\\]","code":"\n1953 * (summary(bptest_m1)$r.squared)## [1] 328.3615\npchisq(328.3615, 7, lower.tail = FALSE)## [1] 5.254828e-67\nlibrary(lmtest)\nbp1 <- bptest(mod1)\nbp1 ## \n##  studentized Breusch-Pagan test\n## \n## data:  mod1\n## BP = 328.36, df = 7, p-value < 2.2e-16\n1953 * (summary(bptest_m2)$r.squared)## [1] 309.5577\npchisq(309.5577, 6, lower.tail = FALSE)## [1] 7.318202e-64"},{"path":"lecture-9-exercises.html","id":"using-the-bptest-function-from-lmtest","chapter":"21 Lecture 9 Exercises","heading":"21.6.3.3 Using the bptest() function from {lmtest}","text":"B-P test also returns significant result 0.001 level \\(log(age)\\) model.","code":"\nlibrary(lmtest)\nbp2 <- bptest(mod2)\nbp2## \n##  studentized Breusch-Pagan test\n## \n## data:  mod2\n## BP = 309.56, df = 6, p-value < 2.2e-16"},{"path":"lecture-9-exercises.html","id":"dealing-with-heteroskedasticity-calculating-robust-ses","chapter":"21 Lecture 9 Exercises","heading":"21.7 Dealing with heteroskedasticity: Calculating robust SEs","text":"sandwich lmtest packages facilitate calculation heteroskedastic-corrected SEs using re-test coefficients.First, use sandwich::vcovHC() extract model’s variance-covariance matrix adjust account heteroskedasticity.test coefficients, can use lmtest::coeftest() specify adjusted variance-covariance matrix.","code":"\nlibrary(sandwich)\nmod1.hcvcov <- vcovHC(mod1)\nmod2.hcvcov <- vcovHC(mod2)\nlibrary(lmtest)\ncoeftest(mod1, vcov = mod1.hcvcov)## \n## t test of coefficients:\n## \n##                Estimate  Std. Error  t value  Pr(>|t|)    \n## (Intercept) -7.10706877  0.62615271 -11.3504 < 2.2e-16 ***\n## educ        -0.06249197  0.01242771  -5.0284 5.399e-07 ***\n## age          0.53265607  0.04183773  12.7315 < 2.2e-16 ***\n## I(age^2)    -0.00549125  0.00066975  -8.1989 4.352e-16 ***\n## electric    -0.24534346  0.13398537  -1.8311  0.067235 .  \n## heduc       -0.04495317  0.01053909  -4.2654 2.091e-05 ***\n## radio        0.14573015  0.09499743   1.5340  0.125182    \n## tv          -0.38496679  0.14793897  -2.6022  0.009333 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ncoeftest(mod2, vcov = mod2.hcvcov)  ## \n## t test of coefficients:\n## \n##               Estimate Std. Error  t value  Pr(>|t|)    \n## (Intercept) -14.599084   0.580554 -25.1468 < 2.2e-16 ***\n## educ         -0.064415   0.012578  -5.1214 3.332e-07 ***\n## log(age)      5.401854   0.174558  30.9459 < 2.2e-16 ***\n## electric     -0.233198   0.135629  -1.7194   0.08570 .  \n## heduc        -0.043244   0.010679  -4.0492 5.340e-05 ***\n## radio         0.170769   0.095975   1.7793   0.07534 .  \n## tv           -0.360619   0.149204  -2.4170   0.01574 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"lecture-9-exercises.html","id":"using-robust-ses-in-a-stargazer-table","chapter":"21 Lecture 9 Exercises","heading":"21.7.0.1 Using robust SEs in a stargazer table","text":"Instead using coeftest() function, ’s usually practical replace unadjusted SEs robust SEs model reporting function., first calculate heteroskedastic-robust SEs taking square-root diagonal adjusted variance-covariance matrix.vectors can used place standard SEs stargazer table using se = list() argument.\nComparing normal vs robust SEs Model 1 Model 2\n","code":"\nmod1.robustse <- sqrt(diag(mod1.hcvcov))\nmod2.robustse <- sqrt(diag(mod2.hcvcov))\nstargazer(mod1, mod1, mod2, mod2, \n          se = list(NULL, mod1.robustse, NULL, mod2.robustse), \n          column.labels = c(\"M1 Default\",\"M1 Robust\", \"M2 Default\",\"M2 Robust\"), \n          no.space = FALSE,\n          align = TRUE,\n          header = FALSE,\n          title = \"Comparing normal vs robust SEs in Model 1 and Model 2\",\n          type = \"html\")"}]
